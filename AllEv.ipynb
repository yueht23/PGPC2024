{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load Packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a64019f32cf3667"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, explained_variance_score\n",
    "\n",
    "from srcs.utils import load_data, print_scores, get_scores, calculate_metrics\n",
    "import time\n",
    "import tracemalloc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# use to store the scores of each model\n",
    "results = []\n",
    "\n",
    "hyperparameters = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:22.759594500Z",
     "start_time": "2024-02-26T21:44:22.621052500Z"
    }
   },
   "id": "44ab88ea82f2bb1f",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Statistical Model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "469bbdd8d64bffad"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "496d5e6ef25244aeabf414125b3c5f33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE and Mean Poisson Deviance are not well calculated because of the negative values in the prediction\n",
      "PDE and Mean Poisson Deviance are not well calculated because of the negative values in the prediction\n",
      "PDE and Mean Poisson Deviance are not well calculated because of the negative values in the prediction\n",
      "PDE and Mean Poisson Deviance are not well calculated because of the negative values in the prediction\n",
      "PDE and Mean Poisson Deviance are not well calculated because of the negative values in the prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric    Mean    Std\n1   Zero   test                  MAE  0.0515 0.0008\n5   Zero   test             MaxError  3.0000 0.6325\n7   Zero   test  MeanPoissonDeviance  1.3280 0.0215\n9   Zero   test                  PDE -3.2368 0.0181\n11  Zero   test                   R2 -0.0484 0.0008\n3   Zero   test                 RMSE  0.2397 0.0037\n15  Zero   test               memory  0.0008 0.0000\n13  Zero   test                 time  0.0000 0.0000\n0   Zero  train                  MAE  0.0515 0.0002\n4   Zero  train             MaxError  3.8000 0.4000\n6   Zero  train  MeanPoissonDeviance  1.3280 0.0054\n8   Zero  train                  PDE -3.2367 0.0045\n10  Zero  train                   R2 -0.0484 0.0002\n2   Zero  train                 RMSE  0.2397 0.0009\n14  Zero  train               memory  0.0000 0.0000\n12  Zero  train                 time  0.0000 0.0000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0515</td>\n      <td>0.0008</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>3.0000</td>\n      <td>0.6325</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>1.3280</td>\n      <td>0.0215</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>-3.2368</td>\n      <td>0.0181</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>-0.0484</td>\n      <td>0.0008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2397</td>\n      <td>0.0037</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0008</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Zero</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0515</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.8000</td>\n      <td>0.4000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>1.3280</td>\n      <td>0.0054</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>-3.2367</td>\n      <td>0.0045</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>-0.0484</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2397</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Zero</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "# End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"Zero\")\n",
    "# Noting to specify, as it is an intercept only model\n",
    "# End of model specification\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    # Noting to specify, as it is an intercept only model,no need to train\n",
    "    # End of model training\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = 0 * np.ones_like(y_train)\n",
    "    y_pred_test = 0 * np.ones_like(y_test)\n",
    "\n",
    "    # End of model testing\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:22.906145900Z",
     "start_time": "2024-02-26T21:44:22.765591400Z"
    }
   },
   "id": "debf88cf77904344",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.09851990317004865"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs((y_train-y_test.mean())).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:22.917691600Z",
     "start_time": "2024-02-26T21:44:22.903144900Z"
    }
   },
   "id": "73d2d512e634970e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Intercept Only Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7808a3d902ec9c6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d40ea0c238db4621a3447e2f7aeaae0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric   Mean    Std\n1    INT   test                  MAE 0.0980 0.0015\n5    INT   test             MaxError 2.9485 0.6323\n7    INT   test  MeanPoissonDeviance 0.3134 0.0043\n9    INT   test                  PDE 0.0000 0.0000\n11   INT   test                   R2 0.0000 0.0000\n3    INT   test                 RMSE 0.2341 0.0036\n15   INT   test               memory 0.0009 0.0000\n13   INT   test                 time 0.0002 0.0004\n0    INT  train                  MAE 0.0980 0.0004\n4    INT  train             MaxError 3.7485 0.3999\n6    INT  train  MeanPoissonDeviance 0.3135 0.0011\n8    INT  train                  PDE 0.0000 0.0000\n10   INT  train                   R2 0.0000 0.0000\n2    INT  train                 RMSE 0.2341 0.0009\n14   INT  train               memory 0.0000 0.0000\n12   INT  train                 time 0.0000 0.0000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0980</td>\n      <td>0.0015</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9485</td>\n      <td>0.6323</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3134</td>\n      <td>0.0043</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2341</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0009</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>INT</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0002</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0980</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7485</td>\n      <td>0.3999</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3135</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2341</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>INT</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "# End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"INT\")\n",
    "# Noting to specify, as it is an intercept only model\n",
    "# End of model specification\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    # Noting to specify, as it is an intercept only model,no need to train\n",
    "    # End of model training\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = y_train.mean() * np.ones_like(y_train)\n",
    "    y_pred_test = y_test.mean() * np.ones_like(y_test)\n",
    "\n",
    "    # End of model testing\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:23.072244600Z",
     "start_time": "2024-02-26T21:44:22.917691600Z"
    }
   },
   "id": "61359982bd69c866",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Poisson Regression -- Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9969da89c404643"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c07b94a2ffa24b8ba6f399c167418058"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric    Mean    Std\n1   GLM-N   test                  MAE  0.1077 0.0005\n5   GLM-N   test             MaxError  2.9378 0.6326\n7   GLM-N   test  MeanPoissonDeviance  0.3154 0.0039\n9   GLM-N   test                  PDE -0.0064 0.0015\n11  GLM-N   test                   R2 -0.0021 0.0005\n3   GLM-N   test                 RMSE  0.2343 0.0036\n15  GLM-N   test               memory  0.0388 0.0000\n13  GLM-N   test                 time  0.0256 0.0017\n0   GLM-N  train                  MAE  0.1077 0.0005\n4   GLM-N  train             MaxError  3.7378 0.3999\n6   GLM-N  train  MeanPoissonDeviance  0.3154 0.0011\n8   GLM-N  train                  PDE -0.0063 0.0003\n10  GLM-N  train                   R2 -0.0021 0.0001\n2   GLM-N  train                 RMSE  0.2344 0.0009\n14  GLM-N  train               memory  0.0388 0.0000\n12  GLM-N  train                 time  0.0543 0.0451",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.1077</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9378</td>\n      <td>0.6326</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3154</td>\n      <td>0.0039</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>-0.0064</td>\n      <td>0.0015</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>-0.0021</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2343</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0388</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>GLM-N</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0256</td>\n      <td>0.0017</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.1077</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7378</td>\n      <td>0.3999</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3154</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>-0.0063</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>-0.0021</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2344</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0388</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>GLM-N</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.0543</td>\n      <td>0.0451</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "# End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"GLM-N\")\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "naive_poisson = PoissonRegressor()\n",
    "# End of model specification\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = naive_poisson.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "    # End of model training\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    # End of model testing\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:23.726835300Z",
     "start_time": "2024-02-26T21:44:23.050245400Z"
    }
   },
   "id": "4ed40bd0a504cf20",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Poisson Regression -- With Feature Engineering + Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8898ae7f528a535"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-26 21:44:24,975] A new study created in memory with name: no-name-4bc70948-59c6-46f2-981d-27f1db1924d1\n",
      "[I 2024-02-26 21:44:25,441] Trial 13 finished with value: 0.31834821060412427 and parameters: {'alpha': 77590.03790918298, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,460] Trial 3 finished with value: 0.31834821060412427 and parameters: {'alpha': 41681.52969976433, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,680] Trial 8 finished with value: 0.31834821060412427 and parameters: {'alpha': 35444.97302594358, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,844] Trial 5 finished with value: 0.31834821060412427 and parameters: {'alpha': 89901.52816636732, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,937] Trial 7 finished with value: 0.31834821060412427 and parameters: {'alpha': 40898.99465314161, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,973] Trial 12 finished with value: 0.31834821060412427 and parameters: {'alpha': 30430.920935199516, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:25,997] Trial 16 finished with value: 0.31834821060412427 and parameters: {'alpha': 99159.76446163826, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:26,009] Trial 11 finished with value: 0.31834821060412427 and parameters: {'alpha': 71306.420880571, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:26,030] Trial 17 finished with value: 0.31834821060412427 and parameters: {'alpha': 76141.30920495497, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.31834821060412427.\n",
      "[I 2024-02-26 21:44:26,080] Trial 4 finished with value: 0.3181116377834615 and parameters: {'alpha': 87620.30673723055, 'solver': 'newton-cholesky'}. Best is trial 4 with value: 0.3181116377834615.\n",
      "[I 2024-02-26 21:44:26,181] Trial 9 finished with value: 0.3181141897823576 and parameters: {'alpha': 83583.1488668788, 'solver': 'newton-cholesky'}. Best is trial 4 with value: 0.3181116377834615.\n",
      "[I 2024-02-26 21:44:26,205] Trial 2 finished with value: 0.31810788162632464 and parameters: {'alpha': 94856.63205265216, 'solver': 'newton-cholesky'}. Best is trial 2 with value: 0.31810788162632464.\n",
      "[I 2024-02-26 21:44:26,208] Trial 0 finished with value: 0.3181077250849615 and parameters: {'alpha': 95204.6705332781, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,210] Trial 1 finished with value: 0.31812159001208506 and parameters: {'alpha': 74225.91170733153, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,228] Trial 6 finished with value: 0.31814241130837484 and parameters: {'alpha': 56886.49199355005, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,255] Trial 14 finished with value: 0.3181197102247409 and parameters: {'alpha': 76350.74464636014, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,260] Trial 19 finished with value: 0.31812851723848884 and parameters: {'alpha': 67414.69765257354, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,281] Trial 15 finished with value: 0.3182742059309161 and parameters: {'alpha': 10660.40550472737, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,304] Trial 18 finished with value: 0.3182195704433714 and parameters: {'alpha': 25172.901866405322, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,307] Trial 10 finished with value: 0.31828289427829726 and parameters: {'alpha': 6980.448312802341, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,504] Trial 20 finished with value: 0.31834821060412427 and parameters: {'alpha': 57600.51087616345, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,520] Trial 21 finished with value: 0.31834821060412427 and parameters: {'alpha': 54933.97701788449, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,609] Trial 23 finished with value: 0.31834821060412427 and parameters: {'alpha': 64061.46106015303, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,675] Trial 25 finished with value: 0.31834821060412427 and parameters: {'alpha': 70279.04926415553, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:26,859] Trial 22 finished with value: 0.3181871296131872 and parameters: {'alpha': 35519.14603436701, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:27,209] Trial 24 finished with value: 0.3181332500952786 and parameters: {'alpha': 63452.96342907481, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:27,529] Trial 26 finished with value: 0.3181558825197098 and parameters: {'alpha': 49034.01823948515, 'solver': 'newton-cholesky'}. Best is trial 0 with value: 0.3181077250849615.\n",
      "[I 2024-02-26 21:44:27,610] Trial 34 finished with value: 0.3181060317228484 and parameters: {'alpha': 99307.49918585453, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,712] Trial 29 finished with value: 0.3182749269433045 and parameters: {'alpha': 3488.366269841681, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,722] Trial 30 finished with value: 0.3182056407329893 and parameters: {'alpha': 964.5978914865627, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,726] Trial 31 finished with value: 0.31823153671557053 and parameters: {'alpha': 1346.9193631953021, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,728] Trial 28 finished with value: 0.31823841169600176 and parameters: {'alpha': 1500.5461038520298, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,739] Trial 27 finished with value: 0.31827538324872295 and parameters: {'alpha': 3547.807793409498, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,747] Trial 32 finished with value: 0.3182831664308557 and parameters: {'alpha': 6636.93437163296, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,808] Trial 33 finished with value: 0.31826228169440335 and parameters: {'alpha': 2411.5034294571306, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,814] Trial 38 finished with value: 0.3181064380370732 and parameters: {'alpha': 98260.21137929267, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,834] Trial 36 finished with value: 0.3181072382007373 and parameters: {'alpha': 96317.98071785676, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,841] Trial 37 finished with value: 0.31813603338354557 and parameters: {'alpha': 61322.37304904796, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,864] Trial 35 finished with value: 0.3181374210468285 and parameters: {'alpha': 60307.63928168779, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:27,920] Trial 39 finished with value: 0.3181062592067245 and parameters: {'alpha': 98715.67584150043, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,109] Trial 40 finished with value: 0.3181069352791224 and parameters: {'alpha': 97035.8158320865, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,122] Trial 41 finished with value: 0.31810732813231984 and parameters: {'alpha': 96108.69305053678, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,202] Trial 42 finished with value: 0.31810695179967013 and parameters: {'alpha': 96996.14046183636, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,410] Trial 44 finished with value: 0.3181066561564203 and parameters: {'alpha': 97715.74813814765, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,430] Trial 43 finished with value: 0.3181072863295902 and parameters: {'alpha': 96205.76365741796, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,644] Trial 45 finished with value: 0.3181064232103381 and parameters: {'alpha': 98297.65697948214, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,771] Trial 47 finished with value: 0.3181061521265894 and parameters: {'alpha': 98992.47824996964, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:28,913] Trial 46 finished with value: 0.31810666332393717 and parameters: {'alpha': 97698.05651284529, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:29,046] Trial 50 finished with value: 0.31810608877451585 and parameters: {'alpha': 99157.7250933093, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:29,083] Trial 49 finished with value: 0.31810649415344905 and parameters: {'alpha': 98118.99550196141, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:29,086] Trial 48 finished with value: 0.31810622322097004 and parameters: {'alpha': 98808.35205287451, 'solver': 'newton-cholesky'}. Best is trial 34 with value: 0.3181060317228484.\n",
      "[I 2024-02-26 21:44:29,120] Trial 51 finished with value: 0.31810598546124336 and parameters: {'alpha': 99429.62531212707, 'solver': 'newton-cholesky'}. Best is trial 51 with value: 0.31810598546124336.\n",
      "[I 2024-02-26 21:44:29,185] Trial 52 finished with value: 0.3181058025948714 and parameters: {'alpha': 99918.47516635977, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,191] Trial 53 finished with value: 0.31810926913894844 and parameters: {'alpha': 91956.50476740998, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,196] Trial 56 finished with value: 0.31810929088118967 and parameters: {'alpha': 91913.45731938757, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,202] Trial 54 finished with value: 0.31810893705386856 and parameters: {'alpha': 92622.61690968217, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,231] Trial 55 finished with value: 0.31810967908359455 and parameters: {'alpha': 91156.04493070068, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,322] Trial 58 finished with value: 0.3181096326656735 and parameters: {'alpha': 91245.51677609596, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,351] Trial 57 finished with value: 0.31810919467088516 and parameters: {'alpha': 92104.46167866558, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,366] Trial 59 finished with value: 0.31810853478779916 and parameters: {'alpha': 93452.15303272968, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,496] Trial 61 finished with value: 0.3181105979520346 and parameters: {'alpha': 89441.87751495585, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,572] Trial 62 finished with value: 0.31811046549896876 and parameters: {'alpha': 89682.56661631445, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,577] Trial 60 finished with value: 0.3181098162924673 and parameters: {'alpha': 90893.25725268078, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:29,818] Trial 63 finished with value: 0.3181103789187475 and parameters: {'alpha': 89841.02143888902, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,037] Trial 64 finished with value: 0.31810969228723696 and parameters: {'alpha': 91130.6475829559, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,062] Trial 65 finished with value: 0.3181105499363964 and parameters: {'alpha': 89528.89157231862, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,349] Trial 66 finished with value: 0.3181113454535977 and parameters: {'alpha': 88120.73105542781, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,369] Trial 69 finished with value: 0.31811042110209004 and parameters: {'alpha': 89763.70755721163, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,377] Trial 68 finished with value: 0.3181103612012833 and parameters: {'alpha': 89873.55793160872, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,488] Trial 67 finished with value: 0.3181110280195828 and parameters: {'alpha': 88674.26099042779, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,494] Trial 70 finished with value: 0.31810989476422763 and parameters: {'alpha': 90744.08192489376, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,503] Trial 71 finished with value: 0.3181099328329795 and parameters: {'alpha': 90672.00175353842, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,570] Trial 73 finished with value: 0.31811245592776227 and parameters: {'alpha': 86264.33485633659, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,589] Trial 72 finished with value: 0.3181132283565685 and parameters: {'alpha': 85039.66312569114, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,612] Trial 74 finished with value: 0.3181130393221965 and parameters: {'alpha': 85334.67843681881, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,659] Trial 79 finished with value: 0.31811235886728906 and parameters: {'alpha': 86421.91901556215, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,671] Trial 76 finished with value: 0.31811352186412845 and parameters: {'alpha': 84587.37535539611, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,678] Trial 75 finished with value: 0.31811398591057216 and parameters: {'alpha': 83886.09490752337, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,680] Trial 77 finished with value: 0.3181131867777273 and parameters: {'alpha': 85104.29960623977, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,791] Trial 78 finished with value: 0.3181132794123378 and parameters: {'alpha': 84960.48776104854, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,851] Trial 80 finished with value: 0.31811384560319106 and parameters: {'alpha': 84096.39054962015, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:30,970] Trial 81 finished with value: 0.31811248682025783 and parameters: {'alpha': 86214.35739949495, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,024] Trial 82 finished with value: 0.31811430366749216 and parameters: {'alpha': 83415.24833077233, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,076] Trial 86 finished with value: 0.31834821060412427 and parameters: {'alpha': 84933.654360013, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,095] Trial 83 finished with value: 0.3181147531205088 and parameters: {'alpha': 82761.6555454891, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,102] Trial 87 finished with value: 0.31834821060412427 and parameters: {'alpha': 83184.14638521257, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,129] Trial 88 finished with value: 0.31834821060412427 and parameters: {'alpha': 81580.29876379986, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,201] Trial 90 finished with value: 0.31834821060412427 and parameters: {'alpha': 84210.73927533248, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,207] Trial 89 finished with value: 0.31834821060412427 and parameters: {'alpha': 80738.75465870096, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,230] Trial 85 finished with value: 0.3181140288140588 and parameters: {'alpha': 83822.08569703036, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,239] Trial 91 finished with value: 0.31834821060412427 and parameters: {'alpha': 83797.35143298496, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,249] Trial 93 finished with value: 0.31834821060412427 and parameters: {'alpha': 81065.06300222571, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,254] Trial 92 finished with value: 0.31834821060412427 and parameters: {'alpha': 80837.9692387604, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,264] Trial 94 finished with value: 0.31834821060412427 and parameters: {'alpha': 81307.51171827689, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,270] Trial 84 finished with value: 0.31811413871691196 and parameters: {'alpha': 83658.74198834332, 'solver': 'newton-cholesky'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,273] Trial 95 finished with value: 0.31834821060412427 and parameters: {'alpha': 99711.89137867978, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,284] Trial 97 finished with value: 0.31834821060412427 and parameters: {'alpha': 99919.15894525967, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,288] Trial 96 finished with value: 0.31834821060412427 and parameters: {'alpha': 99806.95275378317, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,291] Trial 98 finished with value: 0.31834821060412427 and parameters: {'alpha': 81130.4284456109, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n",
      "[I 2024-02-26 21:44:31,299] Trial 99 finished with value: 0.31834821060412427 and parameters: {'alpha': 79135.89851735385, 'solver': 'lbfgs'}. Best is trial 52 with value: 0.3181058025948714.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef28e6e0a4a841568c4cf0b3cfd63c36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "       Model   Type               Metric    Mean    Std\n1   GLM-N-FH   test                  MAE  0.1078 0.0005\n5   GLM-N-FH   test             MaxError  2.9419 0.6280\n7   GLM-N-FH   test  MeanPoissonDeviance  0.3149 0.0040\n9   GLM-N-FH   test                  PDE -0.0048 0.0013\n11  GLM-N-FH   test                   R2 -0.0017 0.0004\n3   GLM-N-FH   test                 RMSE  0.2343 0.0036\n15  GLM-N-FH   test               memory  0.0428 0.0000\n13  GLM-N-FH   test                 time  0.0521 0.0387\n0   GLM-N-FH  train                  MAE  0.1078 0.0005\n4   GLM-N-FH  train             MaxError  3.7388 0.3976\n6   GLM-N-FH  train  MeanPoissonDeviance  0.3149 0.0011\n8   GLM-N-FH  train                  PDE -0.0045 0.0004\n10  GLM-N-FH  train                   R2 -0.0016 0.0001\n2   GLM-N-FH  train                 RMSE  0.2343 0.0009\n14  GLM-N-FH  train               memory  0.0446 0.0000\n12  GLM-N-FH  train                 time  0.1441 0.1072",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.1078</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9419</td>\n      <td>0.6280</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3149</td>\n      <td>0.0040</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>-0.0048</td>\n      <td>0.0013</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>-0.0017</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2343</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0428</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>GLM-N-FH</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0521</td>\n      <td>0.0387</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.1078</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7388</td>\n      <td>0.3976</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3149</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>-0.0045</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>-0.0016</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2343</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0446</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>GLM-N-FH</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.1441</td>\n      <td>0.1072</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"GLM-N-FH\")\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 1e5),\n",
    "        'solver': trial.suggest_categorical('solver', [\"lbfgs\", \"newton-cholesky\"]),\n",
    "    }\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = PoissonRegressor(**param)\n",
    "    model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import mean_poisson_deviance\n",
    "    score = mean_poisson_deviance(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "naive_poisson2 = PoissonRegressor(**study.best_params)\n",
    "\n",
    "# End of model specification\n",
    "\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = naive_poisson2.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:44:32.439182900Z",
     "start_time": "2024-02-26T21:44:23.732839300Z"
    }
   },
   "id": "49b81d7c9e29252",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## zero-inflated Poisson Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ccc1399ebb1e6dd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85d0e580754c4cc9afa5dca7027ae943"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1992272568476434\n",
      "            Iterations: 86\n",
      "            Function evaluations: 157\n",
      "            Gradient evaluations: 86\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20116052214080302\n",
      "            Iterations: 44\n",
      "            Function evaluations: 125\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19993822003171902\n",
      "            Iterations: 39\n",
      "            Function evaluations: 119\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20107709146227806\n",
      "            Iterations: 66\n",
      "            Function evaluations: 146\n",
      "            Gradient evaluations: 66\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19939757288294813\n",
      "            Iterations: 46\n",
      "            Function evaluations: 120\n",
      "            Gradient evaluations: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric    Mean    Std\n1   GLM-Z   test                  MAE  0.0965 0.0005\n5   GLM-Z   test             MaxError  2.9539 0.6035\n7   GLM-Z   test  MeanPoissonDeviance  0.3030 0.0058\n9   GLM-Z   test                  PDE  0.0335 0.0089\n11  GLM-Z   test                   R2  0.0128 0.0039\n3   GLM-Z   test                 RMSE  0.2326 0.0037\n15  GLM-Z   test               memory  0.0857 0.0000\n13  GLM-Z   test                 time  0.0432 0.0039\n0   GLM-Z  train                  MAE  0.0964 0.0004\n4   GLM-Z  train             MaxError  3.7140 0.3709\n6   GLM-Z  train  MeanPoissonDeviance  0.3012 0.0014\n8   GLM-Z  train                  PDE  0.0390 0.0022\n10  GLM-Z  train                   R2  0.0150 0.0011\n2   GLM-Z  train                 RMSE  0.2324 0.0009\n14  GLM-Z  train               memory  0.2142 0.0000\n12  GLM-Z  train                 time 12.8646 1.9937",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0965</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9539</td>\n      <td>0.6035</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3030</td>\n      <td>0.0058</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0335</td>\n      <td>0.0089</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0128</td>\n      <td>0.0039</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2326</td>\n      <td>0.0037</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0857</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>GLM-Z</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0432</td>\n      <td>0.0039</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0964</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7140</td>\n      <td>0.3709</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3012</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.0390</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0150</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2324</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.2142</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>GLM-Z</td>\n      <td>train</td>\n      <td>time</td>\n      <td>12.8646</td>\n      <td>1.9937</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"GLM-Z\")\n",
    "\n",
    "from statsmodels.discrete.count_model import ZeroInflatedPoisson\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = zero_inflated_poisson = ZeroInflatedPoisson(endog=y_train,\n",
    "                                                        exog=X_train.astype(float),\n",
    "                                                        exog_infl=X_train.astype(float),\n",
    "                                                        exposure=X_train[\"Exposure\"]).fit_regularized()\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train.astype(float),\n",
    "                                 exog_infl=X_train.astype(float),\n",
    "                                 exposure=X_train[\"Exposure\"])\n",
    "    y_pred_test = model.predict(X_test.astype(float),\n",
    "                                exog_infl=X_test.astype(float),\n",
    "                                exposure=X_test[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:45:37.228156800Z",
     "start_time": "2024-02-26T21:44:32.449182800Z"
    }
   },
   "id": "4dec0eacc0b7245a",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hurdle Poisson Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51254bb63d20170"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ceaf87cac2af47e79eb67c6ad12ac79f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric    Mean    Std\n1   GLM-H   test                  MAE  0.1230 0.0004\n5   GLM-H   test             MaxError  2.9290 0.5946\n7   GLM-H   test  MeanPoissonDeviance  0.3162 0.0043\n9   GLM-H   test                  PDE -0.0088 0.0085\n11  GLM-H   test                   R2 -0.0161 0.0060\n3   GLM-H   test                 RMSE  0.2360 0.0036\n15  GLM-H   test               memory  0.0445 0.0000\n13  GLM-H   test                 time  0.0180 0.0009\n0   GLM-H  train                  MAE  0.1229 0.0005\n4   GLM-H  train             MaxError  3.6730 0.3619\n6   GLM-H  train  MeanPoissonDeviance  0.3144 0.0014\n8   GLM-H  train                  PDE -0.0031 0.0022\n10  GLM-H  train                   R2 -0.0128 0.0010\n2   GLM-H  train                 RMSE  0.2356 0.0010\n14  GLM-H  train               memory  0.1057 0.0000\n12  GLM-H  train                 time 22.3274 0.5442",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.1230</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9290</td>\n      <td>0.5946</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3162</td>\n      <td>0.0043</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>-0.0088</td>\n      <td>0.0085</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>-0.0161</td>\n      <td>0.0060</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2360</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0445</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>GLM-H</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0180</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.1229</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.6730</td>\n      <td>0.3619</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3144</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>-0.0031</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>-0.0128</td>\n      <td>0.0010</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2356</td>\n      <td>0.0010</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.1057</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>GLM-H</td>\n      <td>train</td>\n      <td>time</td>\n      <td>22.3274</td>\n      <td>0.5442</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"GLM-H\")\n",
    "\n",
    "from statsmodels.discrete.truncated_model import HurdleCountModel\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = HurdleCountModel(endog=y_train.astype(float),\n",
    "                             exog=X_train.astype(float),\n",
    "                             dist='poisson',\n",
    "                             zerodist='poisson',\n",
    "                             ).fit(disp=False)\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train.astype(float))\n",
    "    y_pred_test = model.predict(X_test.astype(float))\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:47:29.120033Z",
     "start_time": "2024-02-26T21:45:37.231140900Z"
    }
   },
   "id": "205bc9c9cfd1c1ee",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Model\n",
    "## XGBoost Poisson Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c32dd4e0a6c0597"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5ad4c69ac2b4838955cbddef6958678"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric   Mean    Std\n1    XGB   test                  MAE 0.0939 0.0003\n5    XGB   test             MaxError 2.9513 0.6199\n7    XGB   test  MeanPoissonDeviance 0.2956 0.0044\n9    XGB   test                  PDE 0.0570 0.0076\n11   XGB   test                   R2 0.0205 0.0026\n3    XGB   test                 RMSE 0.2317 0.0033\n15   XGB   test               memory 0.0540 0.0000\n13   XGB   test                 time 0.7932 0.0076\n0    XGB  train                  MAE 0.0871 0.0005\n4    XGB  train             MaxError 3.5343 0.2996\n6    XGB  train  MeanPoissonDeviance 0.2340 0.0023\n8    XGB  train                  PDE 0.2534 0.0071\n10   XGB  train                   R2 0.1740 0.0075\n2    XGB  train                 RMSE 0.2128 0.0011\n14   XGB  train               memory 0.0540 0.0000\n12   XGB  train                 time 1.1207 0.2632",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0939</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9513</td>\n      <td>0.6199</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2956</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0570</td>\n      <td>0.0076</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0205</td>\n      <td>0.0026</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2317</td>\n      <td>0.0033</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.7932</td>\n      <td>0.0076</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0871</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.5343</td>\n      <td>0.2996</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2340</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.2534</td>\n      <td>0.0071</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.1740</td>\n      <td>0.0075</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2128</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>time</td>\n      <td>1.1207</td>\n      <td>0.2632</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"XGB\")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_poisson_model = XGBRegressor(objective=\"count:poisson\")\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = xgboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:47:38.959293500Z",
     "start_time": "2024-02-26T21:47:29.127033700Z"
    }
   },
   "id": "4364f02aa85df3f2",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Regression -- Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd82626cb8a3f883"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5ee9395f5654029aab596a291956772"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric   Mean    Std\n1    XGB   test                  MAE 0.0939 0.0003\n5    XGB   test             MaxError 2.9513 0.6199\n7    XGB   test  MeanPoissonDeviance 0.2956 0.0044\n9    XGB   test                  PDE 0.0570 0.0076\n11   XGB   test                   R2 0.0205 0.0026\n3    XGB   test                 RMSE 0.2317 0.0033\n15   XGB   test               memory 0.0540 0.0000\n13   XGB   test                 time 0.7918 0.0140\n0    XGB  train                  MAE 0.0871 0.0005\n4    XGB  train             MaxError 3.5343 0.2996\n6    XGB  train  MeanPoissonDeviance 0.2340 0.0023\n8    XGB  train                  PDE 0.2534 0.0071\n10   XGB  train                   R2 0.1740 0.0075\n2    XGB  train                 RMSE 0.2128 0.0011\n14   XGB  train               memory 0.0540 0.0000\n12   XGB  train                 time 1.1213 0.2488",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0939</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9513</td>\n      <td>0.6199</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2956</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0570</td>\n      <td>0.0076</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0205</td>\n      <td>0.0026</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2317</td>\n      <td>0.0033</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGB</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.7918</td>\n      <td>0.0140</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0871</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.5343</td>\n      <td>0.2996</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2340</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.2534</td>\n      <td>0.0071</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.1740</td>\n      <td>0.0075</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2128</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGB</td>\n      <td>train</td>\n      <td>time</td>\n      <td>1.1213</td>\n      <td>0.2488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"XGB\")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_poisson_model = XGBRegressor(objective=\"count:poisson\")\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = xgboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "hyperparameters[\"XGB\"] = study.best_params\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:47:48.742915400Z",
     "start_time": "2024-02-26T21:47:38.960291800Z"
    }
   },
   "id": "b64f88613d8389f2",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-26 21:47:48,862] A new study created in memory with name: no-name-b73895ad-96b0-4502-9b1f-c5f26ff814c5\n",
      "[I 2024-02-26 21:47:54,533] Trial 7 finished with value: 0.2969230405828778 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 101, 'learning_rate': 0.07666949414512937, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.8092751021230236, 'colsample_bytree': 0.6336440749680982, 'reg_alpha': 0.11936017185735659, 'reg_lambda': 0.017551708837032498, 'gamma': 0.7927251261401786}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:47:55,175] Trial 15 finished with value: 0.2984598039412756 and parameters: {'objective': 'count:poisson', 'n_estimators': 162, 'learning_rate': 0.10638990525189393, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.5951531624814536, 'colsample_bytree': 0.7034888927428176, 'reg_alpha': 0.2776825679181779, 'reg_lambda': 0.7551341453867674, 'gamma': 0.006849294732071853}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:47:56,979] Trial 17 finished with value: 0.29979678809497884 and parameters: {'objective': 'count:poisson', 'n_estimators': 174, 'learning_rate': 0.23409788219659222, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.8874580028439323, 'colsample_bytree': 0.6079544461994029, 'reg_alpha': 0.2911876562278417, 'reg_lambda': 0.3436440724936619, 'gamma': 0.5432874829212717}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:47:57,282] Trial 16 finished with value: 0.3659926304046429 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 105, 'learning_rate': 0.3074091363556765, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.9327943988270437, 'colsample_bytree': 0.8438171774845753, 'reg_alpha': 0.864780610236456, 'reg_lambda': 0.4293847874816794, 'gamma': 0.385914884540311}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:47:57,855] Trial 11 finished with value: 0.3005894615113392 and parameters: {'objective': 'count:poisson', 'n_estimators': 167, 'learning_rate': 0.2327075391094547, 'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.540670989458857, 'colsample_bytree': 0.6520502355893827, 'reg_alpha': 0.7686568048710096, 'reg_lambda': 0.8644113446303441, 'gamma': 0.8115388304018634}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:47:58,194] Trial 18 finished with value: 0.29846877621296586 and parameters: {'objective': 'count:poisson', 'n_estimators': 326, 'learning_rate': 0.3513130152383824, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.906925918929073, 'colsample_bytree': 0.7164287450000855, 'reg_alpha': 0.4603313740315864, 'reg_lambda': 0.11068908558738022, 'gamma': 0.4429301511840825}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:00,226] Trial 1 finished with value: 0.29843990348697474 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 527, 'learning_rate': 0.09102996558342218, 'max_depth': 2, 'min_child_weight': 5, 'subsample': 0.5049038027484383, 'colsample_bytree': 0.6616522667552769, 'reg_alpha': 0.011521116733843817, 'reg_lambda': 0.8152153603892447, 'gamma': 0.2420218434316167}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:00,626] Trial 13 finished with value: 0.2974336803707675 and parameters: {'objective': 'count:poisson', 'n_estimators': 534, 'learning_rate': 0.05158674430635159, 'max_depth': 2, 'min_child_weight': 3, 'subsample': 0.9331372814623828, 'colsample_bytree': 0.8432482118357811, 'reg_alpha': 0.8279040429375084, 'reg_lambda': 0.49576432166755413, 'gamma': 0.13498087355874122}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:02,293] Trial 2 finished with value: 0.3019519994339823 and parameters: {'objective': 'count:poisson', 'n_estimators': 736, 'learning_rate': 0.34490037465047774, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.9590488731782749, 'colsample_bytree': 0.6446904020828916, 'reg_alpha': 0.25710507848759634, 'reg_lambda': 0.31003522705313813, 'gamma': 0.28402051725597777}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:02,884] Trial 6 finished with value: 0.3640435250330466 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 348, 'learning_rate': 0.32572160725364385, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.9610505956061624, 'colsample_bytree': 0.5797567947060538, 'reg_alpha': 0.40381749191369953, 'reg_lambda': 0.8616609141413284, 'gamma': 0.2719883409971887}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:03,417] Trial 12 finished with value: 0.30761091518576417 and parameters: {'objective': 'count:poisson', 'n_estimators': 388, 'learning_rate': 0.2761590755587241, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.659244971614714, 'colsample_bytree': 0.9034661569284486, 'reg_alpha': 0.12137928126425312, 'reg_lambda': 0.5435959460623783, 'gamma': 0.6354952930041174}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:03,953] Trial 10 finished with value: 0.30932196835765424 and parameters: {'objective': 'count:poisson', 'n_estimators': 393, 'learning_rate': 0.28991970261332684, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.6299816242158442, 'colsample_bytree': 0.7746126838247049, 'reg_alpha': 0.5699371730478403, 'reg_lambda': 0.08230972479730692, 'gamma': 0.7047337089795702}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:04,808] Trial 0 finished with value: 0.3005855030692051 and parameters: {'objective': 'count:poisson', 'n_estimators': 784, 'learning_rate': 0.40353473281894886, 'max_depth': 2, 'min_child_weight': 10, 'subsample': 0.8093063373092421, 'colsample_bytree': 0.6278637166488323, 'reg_alpha': 0.2668524069284248, 'reg_lambda': 0.29612807370016103, 'gamma': 0.15818388562336494}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:05,369] Trial 22 finished with value: 0.29826102776705005 and parameters: {'objective': 'count:poisson', 'n_estimators': 222, 'learning_rate': 0.13335855826115153, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.7897915786847322, 'colsample_bytree': 0.9103057541133386, 'reg_alpha': 0.9807262354613151, 'reg_lambda': 0.8307939271682683, 'gamma': 0.7483233950161511}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:07,999] Trial 20 finished with value: 0.503499708155614 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 517, 'learning_rate': 0.3904903119168982, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.879290200437725, 'colsample_bytree': 0.6801348755347566, 'reg_alpha': 0.4351894937848254, 'reg_lambda': 0.1854411957189267, 'gamma': 0.9137331829331544}. Best is trial 7 with value: 0.2969230405828778.\n",
      "[I 2024-02-26 21:48:08,757] Trial 26 finished with value: 0.29633297492709787 and parameters: {'objective': 'count:poisson', 'n_estimators': 738, 'learning_rate': 0.3450833300008969, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.9411962384408791, 'colsample_bytree': 0.8511617566011751, 'reg_alpha': 0.9788341746395615, 'reg_lambda': 0.572598483594748, 'gamma': 0.8085065910243617}. Best is trial 26 with value: 0.29633297492709787.\n",
      "[I 2024-02-26 21:48:09,331] Trial 5 finished with value: 0.31601082928504415 and parameters: {'objective': 'count:poisson', 'n_estimators': 627, 'learning_rate': 0.40536649454010537, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.5699257314708526, 'colsample_bytree': 0.9979851777136133, 'reg_alpha': 0.7559477769359189, 'reg_lambda': 0.6012292523679454, 'gamma': 0.7826556003433397}. Best is trial 26 with value: 0.29633297492709787.\n",
      "[I 2024-02-26 21:48:09,811] Trial 9 finished with value: 0.3864979029972101 and parameters: {'objective': 'count:poisson', 'n_estimators': 319, 'learning_rate': 0.4793975706772805, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.6483824733955786, 'colsample_bytree': 0.5518091704113095, 'reg_alpha': 0.34366737493626875, 'reg_lambda': 0.6643796472246444, 'gamma': 0.03716565775776537}. Best is trial 26 with value: 0.29633297492709787.\n",
      "[I 2024-02-26 21:48:10,252] Trial 14 finished with value: 0.348108073066695 and parameters: {'objective': 'count:poisson', 'n_estimators': 598, 'learning_rate': 0.4803639842342304, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.6490122509981342, 'colsample_bytree': 0.8398357370279059, 'reg_alpha': 0.02063180040120305, 'reg_lambda': 0.5204269811120669, 'gamma': 0.14767699059533101}. Best is trial 26 with value: 0.29633297492709787.\n",
      "[I 2024-02-26 21:48:12,025] Trial 28 finished with value: 0.47245268858122863 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 220, 'learning_rate': 0.3043421028544743, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.6466475810716625, 'colsample_bytree': 0.999097682600806, 'reg_alpha': 0.2408587079688147, 'reg_lambda': 0.8271244952644828, 'gamma': 0.18111544159684945}. Best is trial 26 with value: 0.29633297492709787.\n",
      "[I 2024-02-26 21:48:14,208] Trial 24 finished with value: 0.29488311751199936 and parameters: {'objective': 'count:poisson', 'n_estimators': 569, 'learning_rate': 0.038559709658625554, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.7810693101758337, 'colsample_bytree': 0.5794648227470574, 'reg_alpha': 0.7774639422926753, 'reg_lambda': 0.09813415205432219, 'gamma': 0.9959264569275461}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:14,654] Trial 21 finished with value: 0.3198766859772704 and parameters: {'objective': 'count:poisson', 'n_estimators': 831, 'learning_rate': 0.3402950351602743, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.8162372844225645, 'colsample_bytree': 0.7703717919342281, 'reg_alpha': 0.6182400169804442, 'reg_lambda': 0.8561081808216444, 'gamma': 0.3295617704778627}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:15,492] Trial 27 finished with value: 0.295884442092167 and parameters: {'objective': 'count:poisson', 'n_estimators': 788, 'learning_rate': 0.03169353768474714, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9724669541409665, 'colsample_bytree': 0.8797401824076825, 'reg_alpha': 0.5962732105784323, 'reg_lambda': 0.720318995171719, 'gamma': 0.9819357881003551}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:17,200] Trial 23 finished with value: 0.3164482671990193 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 801, 'learning_rate': 0.06898115533768928, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7029473225087972, 'colsample_bytree': 0.5150201373450205, 'reg_alpha': 0.055711319050700925, 'reg_lambda': 0.4940193591290103, 'gamma': 0.6142332357120416}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:18,506] Trial 3 finished with value: 0.30920423019166327 and parameters: {'objective': 'count:poisson', 'n_estimators': 955, 'learning_rate': 0.1658879460211334, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6625190643900238, 'colsample_bytree': 0.6130381647823022, 'reg_alpha': 0.8330646208449737, 'reg_lambda': 0.5238311821544751, 'gamma': 0.45574885085109096}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:20,165] Trial 8 finished with value: 0.31166203424748284 and parameters: {'objective': 'count:poisson', 'n_estimators': 968, 'learning_rate': 0.19614768281607314, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.7678820467757288, 'colsample_bytree': 0.5524979101853806, 'reg_alpha': 0.0551626247831144, 'reg_lambda': 0.7289427074482255, 'gamma': 0.3343923314373284}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:23,546] Trial 30 finished with value: 0.2976986307795068 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 998, 'learning_rate': 0.023157623073144278, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.7915092112017789, 'colsample_bytree': 0.8033963543078382, 'reg_alpha': 0.7327883166470031, 'reg_lambda': 0.08334042529015016, 'gamma': 0.9065937929769267}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:24,310] Trial 4 finished with value: 0.3389694149781801 and parameters: {'objective': 'count:poisson', 'n_estimators': 700, 'learning_rate': 0.12026158265026085, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.9443703059712604, 'colsample_bytree': 0.5364620882636575, 'reg_alpha': 0.1787902516985378, 'reg_lambda': 0.6804670465516163, 'gamma': 0.031620467852417566}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:25,767] Trial 42 finished with value: 0.2950357308990252 and parameters: {'objective': 'count:poisson', 'n_estimators': 997, 'learning_rate': 0.16957630172724283, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9897526680067755, 'colsample_bytree': 0.5210797837901879, 'reg_alpha': 0.9967860680131454, 'reg_lambda': 0.9898226193110158, 'gamma': 0.9884019517675149}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:27,132] Trial 19 finished with value: 0.33419660259970313 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 727, 'learning_rate': 0.016714641134297433, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.7089929812454892, 'colsample_bytree': 0.5596999825203478, 'reg_alpha': 0.6994231810223208, 'reg_lambda': 0.5299714242827696, 'gamma': 0.14638063604019325}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:27,853] Trial 33 finished with value: 0.29694315124633297 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 907, 'learning_rate': 0.013623003645183013, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7194085611360371, 'colsample_bytree': 0.5041540156316562, 'reg_alpha': 0.6472240783411565, 'reg_lambda': 0.6140368137347578, 'gamma': 0.910150610862621}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:28,051] Trial 29 finished with value: 0.35216016217615803 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 962, 'learning_rate': 0.15808893931782755, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.756341386285663, 'colsample_bytree': 0.503856422445272, 'reg_alpha': 0.036363591056952166, 'reg_lambda': 0.035984923819439185, 'gamma': 0.9912520560073694}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:29,660] Trial 32 finished with value: 0.3003885949453787 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 991, 'learning_rate': 0.02167092744086676, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7777114106544742, 'colsample_bytree': 0.9996192886616392, 'reg_alpha': 0.6656249756350896, 'reg_lambda': 0.004175343563131588, 'gamma': 0.9850476811566176}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:30,124] Trial 31 finished with value: 0.3076101846723003 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 999, 'learning_rate': 0.037377224807169374, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.8056075064002158, 'colsample_bytree': 0.9733078208539492, 'reg_alpha': 0.6697475580410367, 'reg_lambda': 0.005089016590105522, 'gamma': 0.006910591644048392}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:32,525] Trial 37 finished with value: 0.36184186791445944 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 875, 'learning_rate': 0.16661773273328018, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7100365884026797, 'colsample_bytree': 0.509460433281192, 'reg_alpha': 0.6184233583884329, 'reg_lambda': 0.01301228522548303, 'gamma': 0.9852099180186117}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:32,735] Trial 38 finished with value: 0.36423328447601877 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 909, 'learning_rate': 0.17098890349219234, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7482020337861505, 'colsample_bytree': 0.764639120234825, 'reg_alpha': 0.6140226726302147, 'reg_lambda': 0.701461403760961, 'gamma': 0.9457406874855939}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:34,206] Trial 35 finished with value: 0.4940293334430422 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 991, 'learning_rate': 0.4904397285742237, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7216332478081857, 'colsample_bytree': 0.5165421313323215, 'reg_alpha': 0.631622710628441, 'reg_lambda': 0.6190868595215725, 'gamma': 0.8957947452565911}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:34,636] Trial 34 finished with value: 0.30668819833289823 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 991, 'learning_rate': 0.04326179980387951, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7205793235402695, 'colsample_bytree': 0.5096316549538029, 'reg_alpha': 0.645687381515631, 'reg_lambda': 0.6399176406100944, 'gamma': 0.9870723915432436}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:35,727] Trial 36 finished with value: 0.35205428425712504 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 977, 'learning_rate': 0.14986557271125286, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7037892822276237, 'colsample_bytree': 0.5299981731230696, 'reg_alpha': 0.6272997237397029, 'reg_lambda': 0.6531835539560304, 'gamma': 0.9554705061207008}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:36,891] Trial 39 finished with value: 0.3625373008404257 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 949, 'learning_rate': 0.17818337741161883, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7180423308570807, 'colsample_bytree': 0.7839821316313628, 'reg_alpha': 0.5792583356576891, 'reg_lambda': 0.9819799723992767, 'gamma': 0.9619758504689031}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:37,344] Trial 44 finished with value: 0.2967895686909502 and parameters: {'objective': 'count:poisson', 'n_estimators': 687, 'learning_rate': 0.016130525871624217, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.99971484596167, 'colsample_bytree': 0.9137721219399475, 'reg_alpha': 0.9985483122018147, 'reg_lambda': 0.9598082249900406, 'gamma': 0.9945589914080915}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:38,770] Trial 49 finished with value: 0.29533393616060855 and parameters: {'objective': 'count:poisson', 'n_estimators': 882, 'learning_rate': 0.16648524658781283, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.993382593777586, 'colsample_bytree': 0.5013562753089934, 'reg_alpha': 0.9173458958312255, 'reg_lambda': 0.962854609918265, 'gamma': 0.995521341607265}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:39,172] Trial 50 finished with value: 0.2956423774059432 and parameters: {'objective': 'count:poisson', 'n_estimators': 857, 'learning_rate': 0.19882560788649642, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9894177613652253, 'colsample_bytree': 0.8901881332969328, 'reg_alpha': 0.9832496573688574, 'reg_lambda': 0.9156643311095607, 'gamma': 0.9958771833132448}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:39,411] Trial 25 finished with value: 0.7433173649497493 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 931, 'learning_rate': 0.36543115996385495, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.7857224202891946, 'colsample_bytree': 0.7291686523148322, 'reg_alpha': 0.3743996480277756, 'reg_lambda': 0.9360293334564151, 'gamma': 0.0437078699439597}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:39,424] Trial 43 finished with value: 0.29573728386719367 and parameters: {'objective': 'count:poisson', 'n_estimators': 941, 'learning_rate': 0.020755459708255587, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9951505406671912, 'colsample_bytree': 0.91318070542017, 'reg_alpha': 0.9899975689860256, 'reg_lambda': 0.6738240787331895, 'gamma': 0.9720100975936526}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:39,572] Trial 40 finished with value: 0.3009347272834838 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 938, 'learning_rate': 0.02647492871842925, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7470447988468989, 'colsample_bytree': 0.5159923393863091, 'reg_alpha': 0.6339722075542679, 'reg_lambda': 0.01216521126092214, 'gamma': 0.9862772329270423}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:40,609] Trial 45 finished with value: 0.2964306226552518 and parameters: {'objective': 'count:poisson', 'n_estimators': 687, 'learning_rate': 0.01693428958061145, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9932077427771441, 'colsample_bytree': 0.9331714352305387, 'reg_alpha': 0.9870757360883377, 'reg_lambda': 0.9691135001649119, 'gamma': 0.9933656955283917}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:41,860] Trial 41 finished with value: 0.37198985131985257 and parameters: {'objective': 'reg:tweedie', 'n_estimators': 1000, 'learning_rate': 0.17515239253436765, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7100317131599015, 'colsample_bytree': 0.5061317926348129, 'reg_alpha': 0.9901045797217192, 'reg_lambda': 0.0009299636489940137, 'gamma': 0.972466898226078}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:42,792] Trial 52 finished with value: 0.296301934781554 and parameters: {'objective': 'count:poisson', 'n_estimators': 867, 'learning_rate': 0.18178086611921135, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9838100474861057, 'colsample_bytree': 0.9212472529002127, 'reg_alpha': 0.9890201217798942, 'reg_lambda': 0.9932464446990574, 'gamma': 0.8615440770896463}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:44,041] Trial 53 finished with value: 0.2963520420784803 and parameters: {'objective': 'count:poisson', 'n_estimators': 883, 'learning_rate': 0.17889167105603038, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9989197120585556, 'colsample_bytree': 0.9192807033835177, 'reg_alpha': 0.9720138864151844, 'reg_lambda': 0.9686430101384078, 'gamma': 0.852836176312007}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:45,249] Trial 51 finished with value: 0.29577902489479646 and parameters: {'objective': 'count:poisson', 'n_estimators': 866, 'learning_rate': 0.0579391894097408, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9979349831358046, 'colsample_bytree': 0.9115431211129907, 'reg_alpha': 0.9948340179973156, 'reg_lambda': 0.966403131335818, 'gamma': 0.9971086490046531}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:45,888] Trial 54 finished with value: 0.29672651161224534 and parameters: {'objective': 'count:poisson', 'n_estimators': 679, 'learning_rate': 0.20004575027306593, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9886067644231301, 'colsample_bytree': 0.9300564145997048, 'reg_alpha': 0.9987655034660159, 'reg_lambda': 0.977951692609619, 'gamma': 0.856317540167494}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:46,684] Trial 55 finished with value: 0.29574452215658636 and parameters: {'objective': 'count:poisson', 'n_estimators': 673, 'learning_rate': 0.10356337351798317, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9956611584292442, 'colsample_bytree': 0.9037902724341537, 'reg_alpha': 0.9885890674199052, 'reg_lambda': 0.9687254284844274, 'gamma': 0.8479579354886067}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:47,995] Trial 56 finished with value: 0.2960053800961849 and parameters: {'objective': 'count:poisson', 'n_estimators': 643, 'learning_rate': 0.08778026710544631, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9928522705194813, 'colsample_bytree': 0.9134520368994581, 'reg_alpha': 0.9897658670219437, 'reg_lambda': 0.9843704540585646, 'gamma': 0.8466800170209907}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:49,661] Trial 57 finished with value: 0.29639353495980625 and parameters: {'objective': 'count:poisson', 'n_estimators': 639, 'learning_rate': 0.08776952674659944, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9873817336381076, 'colsample_bytree': 0.8967576935564747, 'reg_alpha': 0.9630896171520289, 'reg_lambda': 0.991086361105719, 'gamma': 0.8495338470526749}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:50,434] Trial 59 finished with value: 0.29544919311284235 and parameters: {'objective': 'count:poisson', 'n_estimators': 666, 'learning_rate': 0.09046720723782184, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.996133759299146, 'colsample_bytree': 0.901193102873491, 'reg_alpha': 0.9921531658237206, 'reg_lambda': 0.9203374167703353, 'gamma': 0.8300246102437212}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:50,920] Trial 58 finished with value: 0.29606894756253543 and parameters: {'objective': 'count:poisson', 'n_estimators': 696, 'learning_rate': 0.10743000579581397, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.9844018074192974, 'colsample_bytree': 0.903310655999336, 'reg_alpha': 0.9875175225830952, 'reg_lambda': 0.9792478234489176, 'gamma': 0.8678673416216908}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:51,614] Trial 60 finished with value: 0.2959449825022939 and parameters: {'objective': 'count:poisson', 'n_estimators': 673, 'learning_rate': 0.08445813751432887, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.9926367554207062, 'colsample_bytree': 0.9318825088842555, 'reg_alpha': 0.9871700057232673, 'reg_lambda': 0.9568987201277089, 'gamma': 0.8603840932183555}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:53,571] Trial 46 finished with value: 0.30715828768337156 and parameters: {'objective': 'count:poisson', 'n_estimators': 667, 'learning_rate': 0.010249066355556117, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.991968048210864, 'colsample_bytree': 0.9434383589067441, 'reg_alpha': 0.9535206676897395, 'reg_lambda': 0.9976487544438303, 'gamma': 0.9760388997933166}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:53,811] Trial 64 finished with value: 0.2955703995563146 and parameters: {'objective': 'count:poisson', 'n_estimators': 850, 'learning_rate': 0.20039034609557577, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9933934585632205, 'colsample_bytree': 0.5969363912999384, 'reg_alpha': 0.9171574191516606, 'reg_lambda': 0.9180748987124474, 'gamma': 0.8491533662900954}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:53,893] Trial 61 finished with value: 0.2954302166227433 and parameters: {'objective': 'count:poisson', 'n_estimators': 770, 'learning_rate': 0.09794551674892304, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9956602106157996, 'colsample_bytree': 0.6029509378487911, 'reg_alpha': 0.9356210318167573, 'reg_lambda': 0.929098390549274, 'gamma': 0.8475167288252561}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:53,991] Trial 62 finished with value: 0.2951935669223409 and parameters: {'objective': 'count:poisson', 'n_estimators': 871, 'learning_rate': 0.21452425408409942, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.9892118936305889, 'colsample_bytree': 0.5883216772988327, 'reg_alpha': 0.9159731434052052, 'reg_lambda': 0.9519553676289965, 'gamma': 0.8644030052372136}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:54,209] Trial 65 finished with value: 0.29541882320435986 and parameters: {'objective': 'count:poisson', 'n_estimators': 848, 'learning_rate': 0.20704459457234758, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9912345479936179, 'colsample_bytree': 0.5889767602078544, 'reg_alpha': 0.9173363763171513, 'reg_lambda': 0.9124806711187136, 'gamma': 0.8459751949556146}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:54,464] Trial 48 finished with value: 0.29589270705345355 and parameters: {'objective': 'count:poisson', 'n_estimators': 899, 'learning_rate': 0.015172266525178751, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9978489018425922, 'colsample_bytree': 0.5886358312232214, 'reg_alpha': 0.6215070434049593, 'reg_lambda': 0.9743420835184268, 'gamma': 0.9958392800281942}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:54,565] Trial 69 finished with value: 0.2952308203537132 and parameters: {'objective': 'count:poisson', 'n_estimators': 477, 'learning_rate': 0.20781149497915508, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9612351387227563, 'colsample_bytree': 0.5925903875658337, 'reg_alpha': 0.9234094541630281, 'reg_lambda': 0.7738597255583917, 'gamma': 0.8558314581015072}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:56,880] Trial 67 finished with value: 0.2954393012239793 and parameters: {'objective': 'count:poisson', 'n_estimators': 855, 'learning_rate': 0.20874723299653847, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9639369216713831, 'colsample_bytree': 0.582908555517788, 'reg_alpha': 0.8823366946671484, 'reg_lambda': 0.9018629075834196, 'gamma': 0.8660794421939251}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:57,362] Trial 63 finished with value: 0.2965608422519468 and parameters: {'objective': 'count:poisson', 'n_estimators': 834, 'learning_rate': 0.22091886292635862, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8469243096216591, 'colsample_bytree': 0.5959081366466696, 'reg_alpha': 0.9135635216784168, 'reg_lambda': 0.9307606369316916, 'gamma': 0.8510635540346502}. Best is trial 24 with value: 0.29488311751199936.\n",
      "[I 2024-02-26 21:48:58,770] Trial 72 finished with value: 0.29473281079995745 and parameters: {'objective': 'count:poisson', 'n_estimators': 482, 'learning_rate': 0.09904375364403797, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9530309822643652, 'colsample_bytree': 0.5904001707914204, 'reg_alpha': 0.9121783423987584, 'reg_lambda': 0.9200103091772601, 'gamma': 0.9233674312767562}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:48:59,380] Trial 66 finished with value: 0.29592877944842017 and parameters: {'objective': 'count:poisson', 'n_estimators': 843, 'learning_rate': 0.20247108543349068, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8447294378053168, 'colsample_bytree': 0.5841280409878822, 'reg_alpha': 0.9229371507338667, 'reg_lambda': 0.9227448283352923, 'gamma': 0.854133448571137}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:48:59,484] Trial 73 finished with value: 0.29529309246586033 and parameters: {'objective': 'count:poisson', 'n_estimators': 481, 'learning_rate': 0.11136254208881578, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9612866670286595, 'colsample_bytree': 0.5913378646104949, 'reg_alpha': 0.9206043345673136, 'reg_lambda': 0.7757294723555322, 'gamma': 0.930542083398606}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:00,437] Trial 68 finished with value: 0.29663770539148737 and parameters: {'objective': 'count:poisson', 'n_estimators': 849, 'learning_rate': 0.21188547357101226, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8561947835536459, 'colsample_bytree': 0.5796968028162173, 'reg_alpha': 0.9139077967176801, 'reg_lambda': 0.915567225282054, 'gamma': 0.8430034255411155}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:00,559] Trial 75 finished with value: 0.29499224229426585 and parameters: {'objective': 'count:poisson', 'n_estimators': 468, 'learning_rate': 0.22193236794218496, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9582187784636378, 'colsample_bytree': 0.5857321000779154, 'reg_alpha': 0.912337829254612, 'reg_lambda': 0.9132014783697852, 'gamma': 0.7296994827903835}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:01,620] Trial 77 finished with value: 0.29547672141959624 and parameters: {'objective': 'count:poisson', 'n_estimators': 484, 'learning_rate': 0.21019893701470624, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.960631725627642, 'colsample_bytree': 0.575220076781421, 'reg_alpha': 0.9139002754078649, 'reg_lambda': 0.9004493247327447, 'gamma': 0.7525714449533063}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:02,149] Trial 47 finished with value: 0.29554066063693096 and parameters: {'objective': 'count:poisson', 'n_estimators': 863, 'learning_rate': 0.014725156024800262, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.8508810752313593, 'colsample_bytree': 0.89879267383859, 'reg_alpha': 0.9200081650360341, 'reg_lambda': 0.9317157285402351, 'gamma': 0.9986317335596417}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:02,745] Trial 70 finished with value: 0.2964374362370492 and parameters: {'objective': 'count:poisson', 'n_estimators': 834, 'learning_rate': 0.2105815980773888, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9629960358540036, 'colsample_bytree': 0.9495235823141973, 'reg_alpha': 0.9081853248963504, 'reg_lambda': 0.9066942060896015, 'gamma': 0.8400149901496654}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:04,051] Trial 78 finished with value: 0.2951940436068851 and parameters: {'objective': 'count:poisson', 'n_estimators': 460, 'learning_rate': 0.21961415670554363, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9596720938485729, 'colsample_bytree': 0.5799725737486339, 'reg_alpha': 0.9122701728760523, 'reg_lambda': 0.9114101190387147, 'gamma': 0.9207016300260231}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:04,856] Trial 76 finished with value: 0.29544534334958844 and parameters: {'objective': 'count:poisson', 'n_estimators': 571, 'learning_rate': 0.21643680572565918, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.9069582469062121, 'colsample_bytree': 0.5924121832040218, 'reg_alpha': 0.9125668052087821, 'reg_lambda': 0.9051761368556394, 'gamma': 0.7575290273375889}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:05,678] Trial 79 finished with value: 0.29539608281867413 and parameters: {'objective': 'count:poisson', 'n_estimators': 477, 'learning_rate': 0.21828949668476133, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9567011547680496, 'colsample_bytree': 0.5989945118133224, 'reg_alpha': 0.9138937021225693, 'reg_lambda': 0.9119224064763697, 'gamma': 0.7305505137806323}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:06,282] Trial 71 finished with value: 0.2955703730724243 and parameters: {'objective': 'count:poisson', 'n_estimators': 832, 'learning_rate': 0.09357215351141245, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9593377263393472, 'colsample_bytree': 0.9589507959756124, 'reg_alpha': 0.9034461547074344, 'reg_lambda': 0.8957577820084589, 'gamma': 0.923203780047093}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:07,634] Trial 81 finished with value: 0.29524076361091495 and parameters: {'objective': 'count:poisson', 'n_estimators': 576, 'learning_rate': 0.22613328524635634, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9556519996566596, 'colsample_bytree': 0.5874062035790723, 'reg_alpha': 0.9098589774424347, 'reg_lambda': 0.9024348329256566, 'gamma': 0.785122598526988}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:07,848] Trial 82 finished with value: 0.29522734599861244 and parameters: {'objective': 'count:poisson', 'n_estimators': 484, 'learning_rate': 0.13243037302169425, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.9583046537720668, 'colsample_bytree': 0.5733368181485913, 'reg_alpha': 0.8639793557577172, 'reg_lambda': 0.7698465968952659, 'gamma': 0.7524205571728233}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:08,141] Trial 83 finished with value: 0.2957489828036595 and parameters: {'objective': 'count:poisson', 'n_estimators': 589, 'learning_rate': 0.22495153226941972, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.9610069698110507, 'colsample_bytree': 0.569594994196518, 'reg_alpha': 0.8740745764013323, 'reg_lambda': 0.877797998860005, 'gamma': 0.7548165085676634}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:08,411] Trial 87 finished with value: 0.2971318739078329 and parameters: {'objective': 'count:poisson', 'n_estimators': 441, 'learning_rate': 0.26687366057530326, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9247451309973441, 'colsample_bytree': 0.6304196258713013, 'reg_alpha': 0.8136437784161483, 'reg_lambda': 0.7542857777735592, 'gamma': 0.7319457310715217}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:08,741] Trial 84 finished with value: 0.2952981265704548 and parameters: {'objective': 'count:poisson', 'n_estimators': 573, 'learning_rate': 0.23472426126401882, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.9593513020367662, 'colsample_bytree': 0.5668363916539175, 'reg_alpha': 0.8725210038872789, 'reg_lambda': 0.7695949157460517, 'gamma': 0.7587031735420628}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:08,837] Trial 74 finished with value: 0.29528995239062933 and parameters: {'objective': 'count:poisson', 'n_estimators': 825, 'learning_rate': 0.11454256493230312, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9549634690833424, 'colsample_bytree': 0.607278127479066, 'reg_alpha': 0.9172638178941028, 'reg_lambda': 0.8989761272826038, 'gamma': 0.9347380991051576}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:09,075] Trial 85 finished with value: 0.2976686024253251 and parameters: {'objective': 'count:poisson', 'n_estimators': 573, 'learning_rate': 0.25288500048679535, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9223215329064183, 'colsample_bytree': 0.6278452679117421, 'reg_alpha': 0.7978448788176943, 'reg_lambda': 0.7866296992202991, 'gamma': 0.7314490907642875}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:09,297] Trial 86 finished with value: 0.2979236917625998 and parameters: {'objective': 'count:poisson', 'n_estimators': 570, 'learning_rate': 0.258084892425369, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9200434558119772, 'colsample_bytree': 0.6317029832154667, 'reg_alpha': 0.8033343628027467, 'reg_lambda': 0.7854632286664182, 'gamma': 0.7634272564339067}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:09,665] Trial 89 finished with value: 0.2967278954898785 and parameters: {'objective': 'count:poisson', 'n_estimators': 454, 'learning_rate': 0.24745582799479246, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.917599402445284, 'colsample_bytree': 0.6267164072909801, 'reg_alpha': 0.8130190344484282, 'reg_lambda': 0.7760003782930445, 'gamma': 0.740092347181173}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,159] Trial 91 finished with value: 0.2959895450242118 and parameters: {'objective': 'count:poisson', 'n_estimators': 483, 'learning_rate': 0.2495160916957379, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9148250632205741, 'colsample_bytree': 0.63488337162182, 'reg_alpha': 0.8331523792169007, 'reg_lambda': 0.7815939885212537, 'gamma': 0.9353027883481253}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,314] Trial 88 finished with value: 0.2955563408385087 and parameters: {'objective': 'count:poisson', 'n_estimators': 477, 'learning_rate': 0.23675651541851034, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9518290670451692, 'colsample_bytree': 0.5634488753456427, 'reg_alpha': 0.8471933314771272, 'reg_lambda': 0.8772666420192768, 'gamma': 0.766139411685137}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,390] Trial 90 finished with value: 0.2969746868262265 and parameters: {'objective': 'count:poisson', 'n_estimators': 459, 'learning_rate': 0.25763850832811425, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.920151968545885, 'colsample_bytree': 0.558302734615796, 'reg_alpha': 0.7891933372511869, 'reg_lambda': 0.7664886119513644, 'gamma': 0.7526074418698114}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,453] Trial 92 finished with value: 0.2960782956521275 and parameters: {'objective': 'count:poisson', 'n_estimators': 447, 'learning_rate': 0.25432271318075383, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.916590901366335, 'colsample_bytree': 0.6238499387542383, 'reg_alpha': 0.8339976567798804, 'reg_lambda': 0.8069077438700699, 'gamma': 0.9314111928769405}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,573] Trial 80 finished with value: 0.2952351669128011 and parameters: {'objective': 'count:poisson', 'n_estimators': 823, 'learning_rate': 0.21299709479338808, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9541788489779431, 'colsample_bytree': 0.5895235535417039, 'reg_alpha': 0.9165125363676573, 'reg_lambda': 0.9011010643622683, 'gamma': 0.7405097126841981}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:10,757] Trial 93 finished with value: 0.2963267411749505 and parameters: {'objective': 'count:poisson', 'n_estimators': 467, 'learning_rate': 0.25465376651411686, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9197281575067884, 'colsample_bytree': 0.6365826607167301, 'reg_alpha': 0.8471501231345979, 'reg_lambda': 0.7944984573630098, 'gamma': 0.9291248025845118}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:11,287] Trial 94 finished with value: 0.29636834389236294 and parameters: {'objective': 'count:poisson', 'n_estimators': 458, 'learning_rate': 0.2462735527160464, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9122953891528923, 'colsample_bytree': 0.6249613763111073, 'reg_alpha': 0.8092888084941625, 'reg_lambda': 0.7857515278720099, 'gamma': 0.9349516085807416}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:11,326] Trial 95 finished with value: 0.29572993803645625 and parameters: {'objective': 'count:poisson', 'n_estimators': 449, 'learning_rate': 0.2444402675561982, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9216443782816548, 'colsample_bytree': 0.6216523983095523, 'reg_alpha': 0.809950614831985, 'reg_lambda': 0.7796226323540798, 'gamma': 0.9326276763235609}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:11,621] Trial 97 finished with value: 0.2959852500385707 and parameters: {'objective': 'count:poisson', 'n_estimators': 421, 'learning_rate': 0.25846750243357636, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9182100384091377, 'colsample_bytree': 0.6271468405044026, 'reg_alpha': 0.827021398954088, 'reg_lambda': 0.7895000630842879, 'gamma': 0.9334353381050624}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:11,699] Trial 96 finished with value: 0.29638142483283964 and parameters: {'objective': 'count:poisson', 'n_estimators': 446, 'learning_rate': 0.2573959020789332, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9206896895406549, 'colsample_bytree': 0.6395900489177926, 'reg_alpha': 0.8041971323216355, 'reg_lambda': 0.7969137024888511, 'gamma': 0.932130615308935}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:11,895] Trial 98 finished with value: 0.29609075537273133 and parameters: {'objective': 'count:poisson', 'n_estimators': 417, 'learning_rate': 0.2620116192482319, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9217829319657561, 'colsample_bytree': 0.6385213699259341, 'reg_alpha': 0.8165181415917571, 'reg_lambda': 0.8058889015671571, 'gamma': 0.9362749678975696}. Best is trial 72 with value: 0.29473281079995745.\n",
      "[I 2024-02-26 21:49:12,034] Trial 99 finished with value: 0.29789527752903816 and parameters: {'objective': 'count:poisson', 'n_estimators': 407, 'learning_rate': 0.25554757972154307, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.9224478858206608, 'colsample_bytree': 0.6392538852130382, 'reg_alpha': 0.8337824755659656, 'reg_lambda': 0.8505240651919007, 'gamma': 0.674843648352105}. Best is trial 72 with value: 0.29473281079995745.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13790a08eeed4cc6a4af1636e56796d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric   Mean    Std\n1   XGB-H   test                  MAE 0.0961 0.0005\n5   XGB-H   test             MaxError 2.9477 0.6137\n7   XGB-H   test  MeanPoissonDeviance 0.2906 0.0041\n9   XGB-H   test                  PDE 0.0727 0.0056\n11  XGB-H   test                   R2 0.0308 0.0020\n3   XGB-H   test                 RMSE 0.2305 0.0034\n15  XGB-H   test               memory 0.0540 0.0000\n13  XGB-H   test                 time 0.8289 0.0132\n0   XGB-H  train                  MAE 0.0947 0.0003\n4   XGB-H  train             MaxError 3.7288 0.3825\n6   XGB-H  train  MeanPoissonDeviance 0.2763 0.0010\n8   XGB-H  train                  PDE 0.1184 0.0022\n10  XGB-H  train                   R2 0.0642 0.0022\n2   XGB-H  train                 RMSE 0.2265 0.0007\n14  XGB-H  train               memory 0.0540 0.0000\n12  XGB-H  train                 time 1.7754 0.0236",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0961</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9477</td>\n      <td>0.6137</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2906</td>\n      <td>0.0041</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0727</td>\n      <td>0.0056</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0308</td>\n      <td>0.0020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2305</td>\n      <td>0.0034</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGB-H</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.8289</td>\n      <td>0.0132</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0947</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7288</td>\n      <td>0.3825</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2763</td>\n      <td>0.0010</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.1184</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0642</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2265</td>\n      <td>0.0007</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0540</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>XGB-H</td>\n      <td>train</td>\n      <td>time</td>\n      <td>1.7754</td>\n      <td>0.0236</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"XGB-H\")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': trial.suggest_categorical('objective', ['count:poisson', \"reg:tweedie\"]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "    }\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import mean_poisson_deviance\n",
    "    score = mean_poisson_deviance(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "xgboost_poisson_model = XGBRegressor(**study.best_params)\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = xgboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:49:25.211253900Z",
     "start_time": "2024-02-26T21:47:48.750916400Z"
    }
   },
   "id": "9a73cbbe9042d65e",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightBoost Poisson Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da5c8e6392fce11"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c183ffc1c6b4aebb4389f69e1b5bb16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 994\n",
      "[LightGBM] [Info] Number of data points in the train set: 54240, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.774594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 996\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.765472\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.786055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 996\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.777346\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.780441\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric   Mean    Std\n1    LGB   test                  MAE 0.0957 0.0004\n5    LGB   test             MaxError 2.9495 0.6182\n7    LGB   test  MeanPoissonDeviance 0.2926 0.0046\n9    LGB   test                  PDE 0.0666 0.0058\n11   LGB   test                   R2 0.0271 0.0019\n3    LGB   test                 RMSE 0.2309 0.0035\n15   LGB   test               memory 0.0481 0.0000\n13   LGB   test                 time 0.0680 0.0097\n0    LGB  train                  MAE 0.0923 0.0004\n4    LGB  train             MaxError 3.7177 0.3775\n6    LGB  train  MeanPoissonDeviance 0.2603 0.0013\n8    LGB  train                  PDE 0.1695 0.0023\n10   LGB  train                   R2 0.1042 0.0017\n2    LGB  train                 RMSE 0.2216 0.0009\n14   LGB  train               memory 0.0477 0.0000\n12   LGB  train                 time 0.2225 0.0340",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0957</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9495</td>\n      <td>0.6182</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2926</td>\n      <td>0.0046</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0666</td>\n      <td>0.0058</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0271</td>\n      <td>0.0019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2309</td>\n      <td>0.0035</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0481</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LGB</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0680</td>\n      <td>0.0097</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0923</td>\n      <td>0.0004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7177</td>\n      <td>0.3775</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2603</td>\n      <td>0.0013</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.1695</td>\n      <td>0.0023</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.1042</td>\n      <td>0.0017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2216</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0477</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>LGB</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.2225</td>\n      <td>0.0340</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"LGB\")\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lightboost_poisson_model = LGBMRegressor(objective=\"poisson\")\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = lightboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:49:27.020045500Z",
     "start_time": "2024-02-26T21:49:25.214253700Z"
    }
   },
   "id": "243c5fed8991388a",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightBoost Poisson Regression -- Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1cbc2ca4835daa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-26 21:49:27,132] A new study created in memory with name: no-name-06c31105-3c12-4721-9203-2c6c8d996382\n",
      "[I 2024-02-26 21:49:29,400] Trial 10 finished with value: 0.2989252007648753 and parameters: {'objective': 'tweedie', 'n_estimators': 147, 'learning_rate': 0.30745797825826443, 'max_depth': 2, 'reg_alpha': 0.20428343575803265, 'reg_lambda': 0.6149262018359368}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:29,954] Trial 0 finished with value: 0.2990535124040635 and parameters: {'objective': 'poisson', 'n_estimators': 298, 'learning_rate': 0.03912419037385754, 'max_depth': 2, 'reg_alpha': 0.8960120091463705, 'reg_lambda': 0.6056567270512023}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:30,542] Trial 16 finished with value: 0.31415968600567085 and parameters: {'objective': 'tweedie', 'n_estimators': 232, 'learning_rate': 0.4101707415172727, 'max_depth': 3, 'reg_alpha': 0.3017882184342092, 'reg_lambda': 0.5130212774649437}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:31,292] Trial 6 finished with value: 0.30093701773143106 and parameters: {'objective': 'poisson', 'n_estimators': 367, 'learning_rate': 0.32756040169326, 'max_depth': 3, 'reg_alpha': 0.6925145884052422, 'reg_lambda': 0.7432062036332023}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:32,930] Trial 9 finished with value: 0.3047957693601744 and parameters: {'objective': 'poisson', 'n_estimators': 341, 'learning_rate': 0.3501970960251593, 'max_depth': 4, 'reg_alpha': 0.9864885082082205, 'reg_lambda': 0.7088499375151689}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:33,275] Trial 15 finished with value: 0.300555258370674 and parameters: {'objective': 'tweedie', 'n_estimators': 368, 'learning_rate': 0.047072748624126845, 'max_depth': 4, 'reg_alpha': 0.8204807323662957, 'reg_lambda': 0.3332063111139054}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:34,984] Trial 12 finished with value: 0.3409741824997778 and parameters: {'objective': 'tweedie', 'n_estimators': 235, 'learning_rate': 0.17253399978984252, 'max_depth': 6, 'reg_alpha': 0.5115344883484583, 'reg_lambda': 0.3156775461097142}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:37,215] Trial 22 finished with value: 0.36296242135896184 and parameters: {'objective': 'tweedie', 'n_estimators': 226, 'learning_rate': 0.22988625435668109, 'max_depth': 10, 'reg_alpha': 0.4627839908318394, 'reg_lambda': 0.6019845541494421}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:37,447] Trial 18 finished with value: 0.4410956945644517 and parameters: {'objective': 'tweedie', 'n_estimators': 662, 'learning_rate': 0.4441187538595688, 'max_depth': 4, 'reg_alpha': 0.9145370822572662, 'reg_lambda': 0.17604348218325672}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:40,153] Trial 5 finished with value: 0.3026190600820241 and parameters: {'objective': 'poisson', 'n_estimators': 470, 'learning_rate': 0.10662142441894829, 'max_depth': 6, 'reg_alpha': 0.9056421897773091, 'reg_lambda': 0.08580377692374319}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:40,394] Trial 7 finished with value: 0.5466142224308579 and parameters: {'objective': 'tweedie', 'n_estimators': 452, 'learning_rate': 0.3831257124399536, 'max_depth': 8, 'reg_alpha': 0.016076863218904447, 'reg_lambda': 0.8539299225883543}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:41,338] Trial 2 finished with value: 0.3100749314002108 and parameters: {'objective': 'poisson', 'n_estimators': 645, 'learning_rate': 0.13997566441166456, 'max_depth': 5, 'reg_alpha': 0.09578870378398285, 'reg_lambda': 0.3850056518188667}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:41,514] Trial 28 finished with value: 0.4010015909383638 and parameters: {'objective': 'tweedie', 'n_estimators': 135, 'learning_rate': 0.4575436619832178, 'max_depth': 8, 'reg_alpha': 0.3780234257038251, 'reg_lambda': 0.17016184442495264}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:41,643] Trial 23 finished with value: 0.322733109322807 and parameters: {'objective': 'poisson', 'n_estimators': 835, 'learning_rate': 0.4322907157761889, 'max_depth': 4, 'reg_alpha': 0.9586544638657836, 'reg_lambda': 0.9134504969879059}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:42,104] Trial 4 finished with value: 0.39026936315378247 and parameters: {'objective': 'tweedie', 'n_estimators': 631, 'learning_rate': 0.18126627984134353, 'max_depth': 5, 'reg_alpha': 0.7504526138418304, 'reg_lambda': 0.7506533391614186}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:42,433] Trial 25 finished with value: 0.30715729129519087 and parameters: {'objective': 'tweedie', 'n_estimators': 363, 'learning_rate': 0.041914519003464, 'max_depth': 6, 'reg_alpha': 0.9192571556151048, 'reg_lambda': 0.07008480416230001}. Best is trial 10 with value: 0.2989252007648753.\n",
      "[I 2024-02-26 21:49:42,585] Trial 33 finished with value: 0.2977399435796011 and parameters: {'objective': 'poisson', 'n_estimators': 132, 'learning_rate': 0.2807383111639979, 'max_depth': 2, 'reg_alpha': 0.2619984974476934, 'reg_lambda': 0.6257810041871874}. Best is trial 33 with value: 0.2977399435796011.\n",
      "[I 2024-02-26 21:49:42,828] Trial 34 finished with value: 0.29735739363431324 and parameters: {'objective': 'poisson', 'n_estimators': 100, 'learning_rate': 0.28892281926403285, 'max_depth': 2, 'reg_alpha': 0.23847612866916357, 'reg_lambda': 0.5708436119462281}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:42,998] Trial 26 finished with value: 0.30825002938659113 and parameters: {'objective': 'poisson', 'n_estimators': 985, 'learning_rate': 0.27999589707045186, 'max_depth': 3, 'reg_alpha': 0.15394200985142492, 'reg_lambda': 0.4759142570542265}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:43,203] Trial 35 finished with value: 0.297849209464277 and parameters: {'objective': 'poisson', 'n_estimators': 101, 'learning_rate': 0.27550299927211797, 'max_depth': 2, 'reg_alpha': 0.23817097171128143, 'reg_lambda': 0.5447005005967105}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:43,407] Trial 36 finished with value: 0.297668134709376 and parameters: {'objective': 'poisson', 'n_estimators': 109, 'learning_rate': 0.2748058554145033, 'max_depth': 2, 'reg_alpha': 0.20101780419733423, 'reg_lambda': 0.48357223284917844}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:43,567] Trial 37 finished with value: 0.2978167474808391 and parameters: {'objective': 'poisson', 'n_estimators': 103, 'learning_rate': 0.259143686045928, 'max_depth': 2, 'reg_alpha': 0.2416927425937755, 'reg_lambda': 0.9957636131785523}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:43,754] Trial 24 finished with value: 0.3021966293346338 and parameters: {'objective': 'poisson', 'n_estimators': 373, 'learning_rate': 0.11280178917955527, 'max_depth': 9, 'reg_alpha': 0.6922630455693223, 'reg_lambda': 0.05533677203013165}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:43,983] Trial 38 finished with value: 0.2979927134794221 and parameters: {'objective': 'poisson', 'n_estimators': 142, 'learning_rate': 0.24813038651385128, 'max_depth': 2, 'reg_alpha': 0.25551464435530036, 'reg_lambda': 0.46983743520889365}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:44,166] Trial 40 finished with value: 0.2980360192243323 and parameters: {'objective': 'poisson', 'n_estimators': 101, 'learning_rate': 0.23460613472470876, 'max_depth': 2, 'reg_alpha': 0.34241367916238247, 'reg_lambda': 0.43215828128106437}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:44,334] Trial 27 finished with value: 0.3291614784363919 and parameters: {'objective': 'tweedie', 'n_estimators': 279, 'learning_rate': 0.13234570415347827, 'max_depth': 5, 'reg_alpha': 0.06538074603557753, 'reg_lambda': 0.14425615262987246}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:44,897] Trial 3 finished with value: 0.5659456293323328 and parameters: {'objective': 'tweedie', 'n_estimators': 596, 'learning_rate': 0.3435921893058292, 'max_depth': 8, 'reg_alpha': 0.2971896063964101, 'reg_lambda': 0.7200592528774965}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:45,543] Trial 41 finished with value: 0.29792504179831264 and parameters: {'objective': 'poisson', 'n_estimators': 201, 'learning_rate': 0.23359578302502254, 'max_depth': 3, 'reg_alpha': 0.31708426346503016, 'reg_lambda': 0.4000030220508578}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:45,560] Trial 21 finished with value: 0.6104437864309239 and parameters: {'objective': 'tweedie', 'n_estimators': 486, 'learning_rate': 0.4670001712740363, 'max_depth': 8, 'reg_alpha': 0.5400536225972096, 'reg_lambda': 0.24069342808292338}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:45,938] Trial 30 finished with value: 0.2992142366412226 and parameters: {'objective': 'poisson', 'n_estimators': 954, 'learning_rate': 0.2627503039980898, 'max_depth': 2, 'reg_alpha': 0.18035273402917007, 'reg_lambda': 0.937329123772198}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:46,055] Trial 42 finished with value: 0.2974079787573957 and parameters: {'objective': 'poisson', 'n_estimators': 229, 'learning_rate': 0.22238462327204617, 'max_depth': 3, 'reg_alpha': 0.34920658268835425, 'reg_lambda': 0.44005653403975165}. Best is trial 34 with value: 0.29735739363431324.\n",
      "[I 2024-02-26 21:49:46,178] Trial 43 finished with value: 0.2972207030139008 and parameters: {'objective': 'poisson', 'n_estimators': 211, 'learning_rate': 0.20690687475609168, 'max_depth': 3, 'reg_alpha': 0.39971897412735147, 'reg_lambda': 0.3998289515122351}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:46,311] Trial 44 finished with value: 0.2985032229287848 and parameters: {'objective': 'poisson', 'n_estimators': 209, 'learning_rate': 0.35332899340867324, 'max_depth': 3, 'reg_alpha': 0.44091882533844784, 'reg_lambda': 0.6763226617362973}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:46,551] Trial 45 finished with value: 0.29838406616836827 and parameters: {'objective': 'poisson', 'n_estimators': 206, 'learning_rate': 0.35962540740084936, 'max_depth': 3, 'reg_alpha': 0.43307561139795564, 'reg_lambda': 0.6811214508533798}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:46,831] Trial 46 finished with value: 0.29790203013226235 and parameters: {'objective': 'poisson', 'n_estimators': 181, 'learning_rate': 0.3838433508086378, 'max_depth': 3, 'reg_alpha': 0.42033458947370494, 'reg_lambda': 0.276653637076521}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:46,958] Trial 31 finished with value: 0.2992132015051338 and parameters: {'objective': 'poisson', 'n_estimators': 957, 'learning_rate': 0.2611493936048497, 'max_depth': 2, 'reg_alpha': 0.21976939574536447, 'reg_lambda': 0.6077681023704711}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:47,413] Trial 32 finished with value: 0.2996040315162034 and parameters: {'objective': 'poisson', 'n_estimators': 937, 'learning_rate': 0.26408078183224193, 'max_depth': 2, 'reg_alpha': 0.6610625614102841, 'reg_lambda': 0.9630867067588037}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:47,639] Trial 13 finished with value: 0.32301125273915876 and parameters: {'objective': 'tweedie', 'n_estimators': 691, 'learning_rate': 0.039680554969356414, 'max_depth': 7, 'reg_alpha': 0.3823112255070481, 'reg_lambda': 0.21701024177622918}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:47,709] Trial 47 finished with value: 0.299958982040203 and parameters: {'objective': 'poisson', 'n_estimators': 183, 'learning_rate': 0.49113074710687976, 'max_depth': 3, 'reg_alpha': 0.45487290680761683, 'reg_lambda': 0.2839270751905574}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:48,201] Trial 51 finished with value: 0.29734932928003477 and parameters: {'objective': 'poisson', 'n_estimators': 192, 'learning_rate': 0.1917880234613285, 'max_depth': 3, 'reg_alpha': 0.412104976060649, 'reg_lambda': 0.2980356519635261}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:48,496] Trial 48 finished with value: 0.3018396900697867 and parameters: {'objective': 'poisson', 'n_estimators': 291, 'learning_rate': 0.49644287402325327, 'max_depth': 3, 'reg_alpha': 0.42115743350503654, 'reg_lambda': 0.6461135632783176}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:48,797] Trial 50 finished with value: 0.29758691286898137 and parameters: {'objective': 'poisson', 'n_estimators': 279, 'learning_rate': 0.18871735869573436, 'max_depth': 3, 'reg_alpha': 0.3952966354599467, 'reg_lambda': 0.6168716367274405}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:48,950] Trial 52 finished with value: 0.2974877842191409 and parameters: {'objective': 'poisson', 'n_estimators': 273, 'learning_rate': 0.19810065820639017, 'max_depth': 3, 'reg_alpha': 0.4003873596635447, 'reg_lambda': 0.3221405474268739}. Best is trial 43 with value: 0.2972207030139008.\n",
      "[I 2024-02-26 21:49:49,466] Trial 53 finished with value: 0.29666534966821434 and parameters: {'objective': 'poisson', 'n_estimators': 165, 'learning_rate': 0.20386386836588313, 'max_depth': 4, 'reg_alpha': 0.5733258386555029, 'reg_lambda': 0.35263680559302446}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:49,659] Trial 1 finished with value: 0.31338335189875666 and parameters: {'objective': 'poisson', 'n_estimators': 731, 'learning_rate': 0.1682724743278572, 'max_depth': 8, 'reg_alpha': 0.5667774174858993, 'reg_lambda': 0.23064620928337343}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:50,741] Trial 17 finished with value: 0.5977151297303596 and parameters: {'objective': 'tweedie', 'n_estimators': 858, 'learning_rate': 0.4030267171938447, 'max_depth': 5, 'reg_alpha': 0.6319775514232744, 'reg_lambda': 0.3145864995243548}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:51,159] Trial 55 finished with value: 0.29863774668277615 and parameters: {'objective': 'poisson', 'n_estimators': 289, 'learning_rate': 0.1917446149640336, 'max_depth': 4, 'reg_alpha': 0.5592401751020384, 'reg_lambda': 0.38758798610477446}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:51,625] Trial 57 finished with value: 0.30043674892093103 and parameters: {'objective': 'poisson', 'n_estimators': 290, 'learning_rate': 0.18439357398551304, 'max_depth': 4, 'reg_alpha': 0.12025363638462964, 'reg_lambda': 0.36377905021544626}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:52,049] Trial 56 finished with value: 0.3003462229999445 and parameters: {'objective': 'poisson', 'n_estimators': 303, 'learning_rate': 0.1958618806149636, 'max_depth': 4, 'reg_alpha': 0.11762836249724054, 'reg_lambda': 0.3729875247626838}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:52,171] Trial 60 finished with value: 0.2995124960952957 and parameters: {'objective': 'poisson', 'n_estimators': 266, 'learning_rate': 0.19575104300763435, 'max_depth': 4, 'reg_alpha': 0.6200882337833853, 'reg_lambda': 0.3713318889664035}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:52,410] Trial 11 finished with value: 0.41326395608166216 and parameters: {'objective': 'tweedie', 'n_estimators': 948, 'learning_rate': 0.14899976833813566, 'max_depth': 5, 'reg_alpha': 0.46639583354859115, 'reg_lambda': 0.9544863569137049}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:52,545] Trial 49 finished with value: 0.30138614953042625 and parameters: {'objective': 'poisson', 'n_estimators': 745, 'learning_rate': 0.19932183097503753, 'max_depth': 3, 'reg_alpha': 0.4456490795171427, 'reg_lambda': 0.6199846337849254}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:52,656] Trial 58 finished with value: 0.3009514792397837 and parameters: {'objective': 'poisson', 'n_estimators': 308, 'learning_rate': 0.20081613804520465, 'max_depth': 4, 'reg_alpha': 0.12215535327649774, 'reg_lambda': 0.3749624379552665}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:53,418] Trial 14 finished with value: 0.34315801129644385 and parameters: {'objective': 'poisson', 'n_estimators': 914, 'learning_rate': 0.3605274893295255, 'max_depth': 6, 'reg_alpha': 0.7625869282227647, 'reg_lambda': 0.6263606299089053}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:54,031] Trial 8 finished with value: 0.48485223427097696 and parameters: {'objective': 'tweedie', 'n_estimators': 825, 'learning_rate': 0.1429117470574791, 'max_depth': 10, 'reg_alpha': 0.03512567785453746, 'reg_lambda': 0.1978018584655954}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:54,169] Trial 19 finished with value: 0.6083460957559623 and parameters: {'objective': 'tweedie', 'n_estimators': 756, 'learning_rate': 0.34781744127144115, 'max_depth': 10, 'reg_alpha': 0.6338534044036415, 'reg_lambda': 0.8303191907181711}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:54,500] Trial 54 finished with value: 0.30330644394656375 and parameters: {'objective': 'poisson', 'n_estimators': 291, 'learning_rate': 0.2048925751290591, 'max_depth': 5, 'reg_alpha': 0.5579066903600796, 'reg_lambda': 0.38488602467310895}. Best is trial 53 with value: 0.29666534966821434.\n",
      "[I 2024-02-26 21:49:55,709] Trial 76 finished with value: 0.29631702214963007 and parameters: {'objective': 'poisson', 'n_estimators': 158, 'learning_rate': 0.1473716673810163, 'max_depth': 3, 'reg_alpha': 0.5218204789899946, 'reg_lambda': 0.5319847187330717}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:55,887] Trial 61 finished with value: 0.30084879576886364 and parameters: {'objective': 'poisson', 'n_estimators': 272, 'learning_rate': 0.19463066938934373, 'max_depth': 5, 'reg_alpha': 0.5850841677190658, 'reg_lambda': 0.370077590964971}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:56,251] Trial 71 finished with value: 0.2988807776378619 and parameters: {'objective': 'poisson', 'n_estimators': 418, 'learning_rate': 0.21749858361672963, 'max_depth': 3, 'reg_alpha': 0.5040627820296764, 'reg_lambda': 0.5651617777549056}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:56,368] Trial 59 finished with value: 0.30217622453506104 and parameters: {'objective': 'poisson', 'n_estimators': 294, 'learning_rate': 0.1962505833285666, 'max_depth': 5, 'reg_alpha': 0.5960127050225471, 'reg_lambda': 0.3692988612060283}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:56,623] Trial 20 finished with value: 0.549800410165734 and parameters: {'objective': 'tweedie', 'n_estimators': 903, 'learning_rate': 0.247946011934609, 'max_depth': 6, 'reg_alpha': 0.21102629842215437, 'reg_lambda': 0.7202635083294326}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:57,175] Trial 70 finished with value: 0.2983571699190963 and parameters: {'objective': 'poisson', 'n_estimators': 170, 'learning_rate': 0.15776343742697507, 'max_depth': 5, 'reg_alpha': 0.34319957663337514, 'reg_lambda': 0.5366746354534205}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:57,300] Trial 77 finished with value: 0.2977753455086225 and parameters: {'objective': 'poisson', 'n_estimators': 168, 'learning_rate': 0.3032882345431424, 'max_depth': 3, 'reg_alpha': 0.5198366114401903, 'reg_lambda': 0.5583042012612089}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:57,693] Trial 64 finished with value: 0.30089965013966097 and parameters: {'objective': 'poisson', 'n_estimators': 325, 'learning_rate': 0.15233331604983963, 'max_depth': 5, 'reg_alpha': 0.619926686236499, 'reg_lambda': 0.5471529051201519}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:57,766] Trial 73 finished with value: 0.2982742589184002 and parameters: {'objective': 'poisson', 'n_estimators': 168, 'learning_rate': 0.157528451547281, 'max_depth': 5, 'reg_alpha': 0.5043922967792942, 'reg_lambda': 0.556105880656993}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:58,157] Trial 81 finished with value: 0.29664678966102626 and parameters: {'objective': 'poisson', 'n_estimators': 167, 'learning_rate': 0.16768478054910088, 'max_depth': 3, 'reg_alpha': 0.35828202436286766, 'reg_lambda': 0.4393823742825507}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:58,757] Trial 80 finished with value: 0.2967153424507656 and parameters: {'objective': 'poisson', 'n_estimators': 155, 'learning_rate': 0.16517571075268506, 'max_depth': 4, 'reg_alpha': 0.34141286787380765, 'reg_lambda': 0.44698223729679715}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:58,798] Trial 75 finished with value: 0.3025015490410182 and parameters: {'objective': 'poisson', 'n_estimators': 169, 'learning_rate': 0.3169932770828113, 'max_depth': 5, 'reg_alpha': 0.5069539692376558, 'reg_lambda': 0.5369762443578665}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:58,912] Trial 79 finished with value: 0.2974718540244032 and parameters: {'objective': 'poisson', 'n_estimators': 170, 'learning_rate': 0.15856248952592825, 'max_depth': 4, 'reg_alpha': 0.34571752668504957, 'reg_lambda': 0.43669801914438544}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:59,032] Trial 63 finished with value: 0.30200738769915547 and parameters: {'objective': 'poisson', 'n_estimators': 324, 'learning_rate': 0.15930015768383826, 'max_depth': 5, 'reg_alpha': 0.5956492900211268, 'reg_lambda': 0.3637494213541342}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:59,377] Trial 66 finished with value: 0.3013157265632812 and parameters: {'objective': 'poisson', 'n_estimators': 328, 'learning_rate': 0.14797889755569546, 'max_depth': 5, 'reg_alpha': 0.596033839130475, 'reg_lambda': 0.36522918393067827}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:59,455] Trial 82 finished with value: 0.2971346221565393 and parameters: {'objective': 'poisson', 'n_estimators': 247, 'learning_rate': 0.16552364849414714, 'max_depth': 3, 'reg_alpha': 0.48688090045439436, 'reg_lambda': 0.43245065598554444}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:49:59,899] Trial 65 finished with value: 0.3049449133426174 and parameters: {'objective': 'poisson', 'n_estimators': 571, 'learning_rate': 0.20856011758564333, 'max_depth': 4, 'reg_alpha': 0.580333207328737, 'reg_lambda': 0.41046276351308325}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:50:00,095] Trial 39 finished with value: 0.3146422338118724 and parameters: {'objective': 'poisson', 'n_estimators': 517, 'learning_rate': 0.2198194961144461, 'max_depth': 8, 'reg_alpha': 0.5886527201795313, 'reg_lambda': 0.4315141069022189}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:50:00,301] Trial 62 finished with value: 0.3077977167831712 and parameters: {'objective': 'poisson', 'n_estimators': 434, 'learning_rate': 0.20435322593524932, 'max_depth': 5, 'reg_alpha': 0.5848399001963503, 'reg_lambda': 0.3669388319161633}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:50:00,885] Trial 94 finished with value: 0.2987015775487821 and parameters: {'objective': 'poisson', 'n_estimators': 130, 'learning_rate': 0.09366450699602, 'max_depth': 2, 'reg_alpha': 0.2793691377215568, 'reg_lambda': 0.5021622522403552}. Best is trial 76 with value: 0.29631702214963007.\n",
      "[I 2024-02-26 21:50:00,983] Trial 83 finished with value: 0.2960713375000396 and parameters: {'objective': 'poisson', 'n_estimators': 247, 'learning_rate': 0.08303649464713397, 'max_depth': 4, 'reg_alpha': 0.48022771688447174, 'reg_lambda': 0.32517474881944775}. Best is trial 83 with value: 0.2960713375000396.\n",
      "[I 2024-02-26 21:50:01,276] Trial 88 finished with value: 0.29535804927084625 and parameters: {'objective': 'poisson', 'n_estimators': 136, 'learning_rate': 0.08191333451924739, 'max_depth': 4, 'reg_alpha': 0.3726013541116729, 'reg_lambda': 0.42790783884460515}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:01,396] Trial 93 finished with value: 0.2981107374153455 and parameters: {'objective': 'poisson', 'n_estimators': 243, 'learning_rate': 0.07157067076969094, 'max_depth': 2, 'reg_alpha': 0.48091318116927023, 'reg_lambda': 0.4947428301440179}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:01,657] Trial 95 finished with value: 0.29754436687981173 and parameters: {'objective': 'poisson', 'n_estimators': 240, 'learning_rate': 0.12586657010964503, 'max_depth': 2, 'reg_alpha': 0.4769069214797411, 'reg_lambda': 0.5048294724391078}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:01,966] Trial 67 finished with value: 0.30339269423036624 and parameters: {'objective': 'poisson', 'n_estimators': 408, 'learning_rate': 0.15461305231084538, 'max_depth': 5, 'reg_alpha': 0.5058896143060788, 'reg_lambda': 0.555827035833511}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:02,124] Trial 84 finished with value: 0.2974953989545414 and parameters: {'objective': 'poisson', 'n_estimators': 518, 'learning_rate': 0.13256171108397252, 'max_depth': 3, 'reg_alpha': 0.4856091983982992, 'reg_lambda': 0.44554261847376536}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:02,670] Trial 87 finished with value: 0.29616379582189867 and parameters: {'objective': 'poisson', 'n_estimators': 244, 'learning_rate': 0.08088752679615312, 'max_depth': 4, 'reg_alpha': 0.2974914081888063, 'reg_lambda': 0.4330812582373804}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:02,797] Trial 91 finished with value: 0.29679220041573035 and parameters: {'objective': 'poisson', 'n_estimators': 243, 'learning_rate': 0.08334159824810573, 'max_depth': 4, 'reg_alpha': 0.30353124749668714, 'reg_lambda': 0.43654039746798057}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:03,714] Trial 96 finished with value: 0.2973880107330543 and parameters: {'objective': 'poisson', 'n_estimators': 246, 'learning_rate': 0.13033537539236886, 'max_depth': 4, 'reg_alpha': 0.3057234272228252, 'reg_lambda': 0.46869900345021603}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:03,756] Trial 97 finished with value: 0.29720836867484707 and parameters: {'objective': 'poisson', 'n_estimators': 241, 'learning_rate': 0.12429462320524522, 'max_depth': 4, 'reg_alpha': 0.5330393208288466, 'reg_lambda': 0.0045414068669256125}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:03,946] Trial 72 finished with value: 0.3038932597852651 and parameters: {'objective': 'poisson', 'n_estimators': 430, 'learning_rate': 0.1561264020174453, 'max_depth': 5, 'reg_alpha': 0.3445532340334424, 'reg_lambda': 0.43248990396782416}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:04,125] Trial 74 finished with value: 0.3043585013878199 and parameters: {'objective': 'poisson', 'n_estimators': 430, 'learning_rate': 0.15091826208110526, 'max_depth': 5, 'reg_alpha': 0.5025711368127029, 'reg_lambda': 0.5535987372167914}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:04,287] Trial 85 finished with value: 0.2981732292214632 and parameters: {'objective': 'poisson', 'n_estimators': 243, 'learning_rate': 0.09207482949691656, 'max_depth': 7, 'reg_alpha': 0.3603208095882025, 'reg_lambda': 0.4234649645069597}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:04,332] Trial 98 finished with value: 0.29557093938449813 and parameters: {'objective': 'poisson', 'n_estimators': 248, 'learning_rate': 0.0651972806649226, 'max_depth': 4, 'reg_alpha': 0.4699944781842316, 'reg_lambda': 0.2442390897868739}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:04,812] Trial 69 finished with value: 0.30630610647346707 and parameters: {'objective': 'poisson', 'n_estimators': 573, 'learning_rate': 0.1504148709894637, 'max_depth': 5, 'reg_alpha': 0.5101817734571622, 'reg_lambda': 0.4230802564982309}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:05,000] Trial 90 finished with value: 0.2983527353369134 and parameters: {'objective': 'poisson', 'n_estimators': 230, 'learning_rate': 0.07856841085164162, 'max_depth': 7, 'reg_alpha': 0.29019236212554544, 'reg_lambda': 0.44727715738538915}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:05,328] Trial 92 finished with value: 0.30048438937351857 and parameters: {'objective': 'poisson', 'n_estimators': 249, 'learning_rate': 0.12485813377253582, 'max_depth': 7, 'reg_alpha': 0.47367157015359745, 'reg_lambda': 0.4915588494442898}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:05,538] Trial 68 finished with value: 0.30728701875291997 and parameters: {'objective': 'poisson', 'n_estimators': 541, 'learning_rate': 0.1576996740316038, 'max_depth': 5, 'reg_alpha': 0.6053342997099137, 'reg_lambda': 0.7890933597845089}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:05,863] Trial 78 finished with value: 0.305914758446764 and parameters: {'objective': 'poisson', 'n_estimators': 428, 'learning_rate': 0.1629807109577211, 'max_depth': 7, 'reg_alpha': 0.4998864121879161, 'reg_lambda': 0.5558969512543417}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:05,928] Trial 99 finished with value: 0.30115295204794107 and parameters: {'objective': 'poisson', 'n_estimators': 252, 'learning_rate': 0.12965832394783944, 'max_depth': 7, 'reg_alpha': 0.3790907553552628, 'reg_lambda': 0.2522200376605256}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:06,545] Trial 29 finished with value: 0.6370283449659699 and parameters: {'objective': 'tweedie', 'n_estimators': 964, 'learning_rate': 0.280510127467791, 'max_depth': 9, 'reg_alpha': 0.04016840704891256, 'reg_lambda': 0.9813104498496651}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:07,174] Trial 89 finished with value: 0.30138534819792867 and parameters: {'objective': 'poisson', 'n_estimators': 564, 'learning_rate': 0.07170294412318919, 'max_depth': 7, 'reg_alpha': 0.47978721023208626, 'reg_lambda': 0.2615141255010425}. Best is trial 88 with value: 0.29535804927084625.\n",
      "[I 2024-02-26 21:50:07,212] Trial 86 finished with value: 0.3052682644747046 and parameters: {'objective': 'poisson', 'n_estimators': 580, 'learning_rate': 0.11588033455489695, 'max_depth': 7, 'reg_alpha': 0.3558330856070082, 'reg_lambda': 0.4346238876059123}. Best is trial 88 with value: 0.29535804927084625.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a1bf40ad30049bf9bb8f828c8b95ca6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 994\n",
      "[LightGBM] [Info] Number of data points in the train set: 54240, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.774594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 996\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.765472\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.786055\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 996\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.777346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 54241, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -2.780441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric   Mean    Std\n1   LGB-H   test                  MAE 0.0965 0.0005\n5   LGB-H   test             MaxError 2.9484 0.6124\n7   LGB-H   test  MeanPoissonDeviance 0.2910 0.0038\n9   LGB-H   test                  PDE 0.0716 0.0054\n11  LGB-H   test                   R2 0.0301 0.0022\n3   LGB-H   test                 RMSE 0.2305 0.0033\n15  LGB-H   test               memory 0.0481 0.0000\n13  LGB-H   test                 time 0.0790 0.0120\n0   LGB-H  train                  MAE 0.0953 0.0003\n4   LGB-H  train             MaxError 3.7293 0.3831\n6   LGB-H  train  MeanPoissonDeviance 0.2794 0.0011\n8   LGB-H  train                  PDE 0.1087 0.0020\n10  LGB-H  train                   R2 0.0553 0.0012\n2   LGB-H  train                 RMSE 0.2275 0.0008\n14  LGB-H  train               memory 0.0477 0.0000\n12  LGB-H  train                 time 0.2381 0.0534",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0965</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9484</td>\n      <td>0.6124</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2910</td>\n      <td>0.0038</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0716</td>\n      <td>0.0054</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0301</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2305</td>\n      <td>0.0033</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0481</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LGB-H</td>\n      <td>test</td>\n      <td>time</td>\n      <td>0.0790</td>\n      <td>0.0120</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0953</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7293</td>\n      <td>0.3831</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2794</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.1087</td>\n      <td>0.0020</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0553</td>\n      <td>0.0012</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2275</td>\n      <td>0.0008</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0477</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>LGB-H</td>\n      <td>train</td>\n      <td>time</td>\n      <td>0.2381</td>\n      <td>0.0534</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"LGB-H\")\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"poisson\", \"tweedie\"]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "    }\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LGBMRegressor(**param)\n",
    "    model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import mean_poisson_deviance\n",
    "    score = mean_poisson_deviance(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "lightboost_poisson_model = LGBMRegressor(silent=True, **study.best_params)\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = lightboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "hyperparameters[\"LGB\"] = study.best_params\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:50:08.982519300Z",
     "start_time": "2024-02-26T21:49:27.027047500Z"
    }
   },
   "id": "eb852da6b5d4a545",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CatBoost Poisson Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c890690cbc2ddc08"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f336d24a5b074ae68fb3ee463bca6bdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9238315\ttotal: 180ms\tremaining: 2m 59s\n",
      "1:\tlearn: 0.8564954\ttotal: 200ms\tremaining: 1m 40s\n",
      "2:\tlearn: 0.7975278\ttotal: 217ms\tremaining: 1m 12s\n",
      "3:\tlearn: 0.7446885\ttotal: 235ms\tremaining: 58.4s\n",
      "4:\tlearn: 0.6973951\ttotal: 250ms\tremaining: 49.8s\n",
      "5:\tlearn: 0.6548931\ttotal: 267ms\tremaining: 44.3s\n",
      "6:\tlearn: 0.6168948\ttotal: 284ms\tremaining: 40.3s\n",
      "7:\tlearn: 0.5827130\ttotal: 301ms\tremaining: 37.3s\n",
      "8:\tlearn: 0.5522995\ttotal: 318ms\tremaining: 35s\n",
      "9:\tlearn: 0.5245250\ttotal: 334ms\tremaining: 33.1s\n",
      "10:\tlearn: 0.4995898\ttotal: 352ms\tremaining: 31.6s\n",
      "11:\tlearn: 0.4770029\ttotal: 369ms\tremaining: 30.4s\n",
      "12:\tlearn: 0.4564500\ttotal: 386ms\tremaining: 29.3s\n",
      "13:\tlearn: 0.4380304\ttotal: 404ms\tremaining: 28.5s\n",
      "14:\tlearn: 0.4206024\ttotal: 422ms\tremaining: 27.7s\n",
      "15:\tlearn: 0.4054818\ttotal: 443ms\tremaining: 27.2s\n",
      "16:\tlearn: 0.3913623\ttotal: 462ms\tremaining: 26.7s\n",
      "17:\tlearn: 0.3787996\ttotal: 478ms\tremaining: 26.1s\n",
      "18:\tlearn: 0.3673587\ttotal: 497ms\tremaining: 25.6s\n",
      "19:\tlearn: 0.3564712\ttotal: 514ms\tremaining: 25.2s\n",
      "20:\tlearn: 0.3467971\ttotal: 532ms\tremaining: 24.8s\n",
      "21:\tlearn: 0.3380819\ttotal: 549ms\tremaining: 24.4s\n",
      "22:\tlearn: 0.3295985\ttotal: 567ms\tremaining: 24.1s\n",
      "23:\tlearn: 0.3223314\ttotal: 583ms\tremaining: 23.7s\n",
      "24:\tlearn: 0.3156095\ttotal: 603ms\tremaining: 23.5s\n",
      "25:\tlearn: 0.3093231\ttotal: 621ms\tremaining: 23.3s\n",
      "26:\tlearn: 0.3036535\ttotal: 639ms\tremaining: 23s\n",
      "27:\tlearn: 0.2983419\ttotal: 658ms\tremaining: 22.9s\n",
      "28:\tlearn: 0.2931482\ttotal: 678ms\tremaining: 22.7s\n",
      "29:\tlearn: 0.2885623\ttotal: 695ms\tremaining: 22.5s\n",
      "30:\tlearn: 0.2843723\ttotal: 713ms\tremaining: 22.3s\n",
      "31:\tlearn: 0.2805451\ttotal: 730ms\tremaining: 22.1s\n",
      "32:\tlearn: 0.2768359\ttotal: 747ms\tremaining: 21.9s\n",
      "33:\tlearn: 0.2736771\ttotal: 762ms\tremaining: 21.7s\n",
      "34:\tlearn: 0.2703644\ttotal: 779ms\tremaining: 21.5s\n",
      "35:\tlearn: 0.2674177\ttotal: 796ms\tremaining: 21.3s\n",
      "36:\tlearn: 0.2647720\ttotal: 813ms\tremaining: 21.2s\n",
      "37:\tlearn: 0.2622809\ttotal: 832ms\tremaining: 21.1s\n",
      "38:\tlearn: 0.2599457\ttotal: 851ms\tremaining: 21s\n",
      "39:\tlearn: 0.2577236\ttotal: 869ms\tremaining: 20.9s\n",
      "40:\tlearn: 0.2557361\ttotal: 887ms\tremaining: 20.8s\n",
      "41:\tlearn: 0.2536798\ttotal: 905ms\tremaining: 20.6s\n",
      "42:\tlearn: 0.2520490\ttotal: 923ms\tremaining: 20.5s\n",
      "43:\tlearn: 0.2504171\ttotal: 941ms\tremaining: 20.4s\n",
      "44:\tlearn: 0.2489636\ttotal: 958ms\tremaining: 20.3s\n",
      "45:\tlearn: 0.2475961\ttotal: 976ms\tremaining: 20.2s\n",
      "46:\tlearn: 0.2461777\ttotal: 994ms\tremaining: 20.1s\n",
      "47:\tlearn: 0.2449274\ttotal: 1.01s\tremaining: 20.1s\n",
      "48:\tlearn: 0.2437448\ttotal: 1.03s\tremaining: 20s\n",
      "49:\tlearn: 0.2425579\ttotal: 1.05s\tremaining: 19.9s\n",
      "50:\tlearn: 0.2416180\ttotal: 1.06s\tremaining: 19.8s\n",
      "51:\tlearn: 0.2406495\ttotal: 1.08s\tremaining: 19.7s\n",
      "52:\tlearn: 0.2397915\ttotal: 1.1s\tremaining: 19.7s\n",
      "53:\tlearn: 0.2389214\ttotal: 1.12s\tremaining: 19.6s\n",
      "54:\tlearn: 0.2381330\ttotal: 1.14s\tremaining: 19.5s\n",
      "55:\tlearn: 0.2373315\ttotal: 1.15s\tremaining: 19.5s\n",
      "56:\tlearn: 0.2365496\ttotal: 1.17s\tremaining: 19.4s\n",
      "57:\tlearn: 0.2358545\ttotal: 1.19s\tremaining: 19.4s\n",
      "58:\tlearn: 0.2351377\ttotal: 1.21s\tremaining: 19.3s\n",
      "59:\tlearn: 0.2345900\ttotal: 1.23s\tremaining: 19.2s\n",
      "60:\tlearn: 0.2339747\ttotal: 1.25s\tremaining: 19.2s\n",
      "61:\tlearn: 0.2334399\ttotal: 1.26s\tremaining: 19.1s\n",
      "62:\tlearn: 0.2329853\ttotal: 1.28s\tremaining: 19.1s\n",
      "63:\tlearn: 0.2324903\ttotal: 1.3s\tremaining: 19s\n",
      "64:\tlearn: 0.2320856\ttotal: 1.32s\tremaining: 18.9s\n",
      "65:\tlearn: 0.2316136\ttotal: 1.33s\tremaining: 18.9s\n",
      "66:\tlearn: 0.2312604\ttotal: 1.35s\tremaining: 18.8s\n",
      "67:\tlearn: 0.2308859\ttotal: 1.37s\tremaining: 18.8s\n",
      "68:\tlearn: 0.2305765\ttotal: 1.39s\tremaining: 18.7s\n",
      "69:\tlearn: 0.2302604\ttotal: 1.4s\tremaining: 18.7s\n",
      "70:\tlearn: 0.2299566\ttotal: 1.42s\tremaining: 18.6s\n",
      "71:\tlearn: 0.2296051\ttotal: 1.44s\tremaining: 18.5s\n",
      "72:\tlearn: 0.2293531\ttotal: 1.46s\tremaining: 18.5s\n",
      "73:\tlearn: 0.2290484\ttotal: 1.47s\tremaining: 18.4s\n",
      "74:\tlearn: 0.2288312\ttotal: 1.49s\tremaining: 18.4s\n",
      "75:\tlearn: 0.2285390\ttotal: 1.51s\tremaining: 18.4s\n",
      "76:\tlearn: 0.2283013\ttotal: 1.53s\tremaining: 18.3s\n",
      "77:\tlearn: 0.2280304\ttotal: 1.55s\tremaining: 18.3s\n",
      "78:\tlearn: 0.2278256\ttotal: 1.56s\tremaining: 18.2s\n",
      "79:\tlearn: 0.2276599\ttotal: 1.58s\tremaining: 18.2s\n",
      "80:\tlearn: 0.2274581\ttotal: 1.6s\tremaining: 18.1s\n",
      "81:\tlearn: 0.2272667\ttotal: 1.61s\tremaining: 18.1s\n",
      "82:\tlearn: 0.2270455\ttotal: 1.63s\tremaining: 18.1s\n",
      "83:\tlearn: 0.2268875\ttotal: 1.65s\tremaining: 18s\n",
      "84:\tlearn: 0.2267578\ttotal: 1.67s\tremaining: 18s\n",
      "85:\tlearn: 0.2265462\ttotal: 1.69s\tremaining: 18s\n",
      "86:\tlearn: 0.2263441\ttotal: 1.71s\tremaining: 18s\n",
      "87:\tlearn: 0.2261683\ttotal: 1.73s\tremaining: 17.9s\n",
      "88:\tlearn: 0.2259936\ttotal: 1.75s\tremaining: 17.9s\n",
      "89:\tlearn: 0.2258767\ttotal: 1.77s\tremaining: 17.9s\n",
      "90:\tlearn: 0.2257589\ttotal: 1.78s\tremaining: 17.8s\n",
      "91:\tlearn: 0.2256466\ttotal: 1.8s\tremaining: 17.8s\n",
      "92:\tlearn: 0.2255197\ttotal: 1.82s\tremaining: 17.7s\n",
      "93:\tlearn: 0.2254263\ttotal: 1.84s\tremaining: 17.7s\n",
      "94:\tlearn: 0.2253170\ttotal: 1.85s\tremaining: 17.7s\n",
      "95:\tlearn: 0.2251847\ttotal: 1.87s\tremaining: 17.6s\n",
      "96:\tlearn: 0.2250301\ttotal: 1.89s\tremaining: 17.6s\n",
      "97:\tlearn: 0.2248945\ttotal: 1.91s\tremaining: 17.6s\n",
      "98:\tlearn: 0.2248122\ttotal: 1.93s\tremaining: 17.5s\n",
      "99:\tlearn: 0.2246828\ttotal: 1.94s\tremaining: 17.5s\n",
      "100:\tlearn: 0.2245347\ttotal: 1.96s\tremaining: 17.5s\n",
      "101:\tlearn: 0.2244184\ttotal: 1.98s\tremaining: 17.4s\n",
      "102:\tlearn: 0.2243272\ttotal: 2s\tremaining: 17.4s\n",
      "103:\tlearn: 0.2242267\ttotal: 2.01s\tremaining: 17.4s\n",
      "104:\tlearn: 0.2241303\ttotal: 2.03s\tremaining: 17.3s\n",
      "105:\tlearn: 0.2240264\ttotal: 2.05s\tremaining: 17.3s\n",
      "106:\tlearn: 0.2239666\ttotal: 2.06s\tremaining: 17.2s\n",
      "107:\tlearn: 0.2238553\ttotal: 2.08s\tremaining: 17.2s\n",
      "108:\tlearn: 0.2237587\ttotal: 2.1s\tremaining: 17.2s\n",
      "109:\tlearn: 0.2236946\ttotal: 2.12s\tremaining: 17.1s\n",
      "110:\tlearn: 0.2236287\ttotal: 2.14s\tremaining: 17.1s\n",
      "111:\tlearn: 0.2235160\ttotal: 2.15s\tremaining: 17.1s\n",
      "112:\tlearn: 0.2234455\ttotal: 2.17s\tremaining: 17.1s\n",
      "113:\tlearn: 0.2233975\ttotal: 2.19s\tremaining: 17s\n",
      "114:\tlearn: 0.2233357\ttotal: 2.21s\tremaining: 17s\n",
      "115:\tlearn: 0.2232379\ttotal: 2.23s\tremaining: 17s\n",
      "116:\tlearn: 0.2231735\ttotal: 2.24s\tremaining: 16.9s\n",
      "117:\tlearn: 0.2231312\ttotal: 2.26s\tremaining: 16.9s\n",
      "118:\tlearn: 0.2230825\ttotal: 2.28s\tremaining: 16.9s\n",
      "119:\tlearn: 0.2230366\ttotal: 2.3s\tremaining: 16.8s\n",
      "120:\tlearn: 0.2229925\ttotal: 2.31s\tremaining: 16.8s\n",
      "121:\tlearn: 0.2229459\ttotal: 2.33s\tremaining: 16.8s\n",
      "122:\tlearn: 0.2229088\ttotal: 2.35s\tremaining: 16.8s\n",
      "123:\tlearn: 0.2228136\ttotal: 2.37s\tremaining: 16.8s\n",
      "124:\tlearn: 0.2227698\ttotal: 2.39s\tremaining: 16.7s\n",
      "125:\tlearn: 0.2227164\ttotal: 2.41s\tremaining: 16.7s\n",
      "126:\tlearn: 0.2226701\ttotal: 2.42s\tremaining: 16.7s\n",
      "127:\tlearn: 0.2226309\ttotal: 2.44s\tremaining: 16.6s\n",
      "128:\tlearn: 0.2225441\ttotal: 2.46s\tremaining: 16.6s\n",
      "129:\tlearn: 0.2225127\ttotal: 2.48s\tremaining: 16.6s\n",
      "130:\tlearn: 0.2224375\ttotal: 2.49s\tremaining: 16.5s\n",
      "131:\tlearn: 0.2223907\ttotal: 2.51s\tremaining: 16.5s\n",
      "132:\tlearn: 0.2223299\ttotal: 2.53s\tremaining: 16.5s\n",
      "133:\tlearn: 0.2222637\ttotal: 2.55s\tremaining: 16.5s\n",
      "134:\tlearn: 0.2222374\ttotal: 2.56s\tremaining: 16.4s\n",
      "135:\tlearn: 0.2221863\ttotal: 2.58s\tremaining: 16.4s\n",
      "136:\tlearn: 0.2221308\ttotal: 2.6s\tremaining: 16.4s\n",
      "137:\tlearn: 0.2220414\ttotal: 2.62s\tremaining: 16.4s\n",
      "138:\tlearn: 0.2220078\ttotal: 2.64s\tremaining: 16.3s\n",
      "139:\tlearn: 0.2219403\ttotal: 2.65s\tremaining: 16.3s\n",
      "140:\tlearn: 0.2219049\ttotal: 2.67s\tremaining: 16.3s\n",
      "141:\tlearn: 0.2218676\ttotal: 2.69s\tremaining: 16.3s\n",
      "142:\tlearn: 0.2217990\ttotal: 2.71s\tremaining: 16.2s\n",
      "143:\tlearn: 0.2217663\ttotal: 2.73s\tremaining: 16.2s\n",
      "144:\tlearn: 0.2217054\ttotal: 2.75s\tremaining: 16.2s\n",
      "145:\tlearn: 0.2216545\ttotal: 2.76s\tremaining: 16.2s\n",
      "146:\tlearn: 0.2216176\ttotal: 2.78s\tremaining: 16.1s\n",
      "147:\tlearn: 0.2215633\ttotal: 2.8s\tremaining: 16.1s\n",
      "148:\tlearn: 0.2215325\ttotal: 2.82s\tremaining: 16.1s\n",
      "149:\tlearn: 0.2215061\ttotal: 2.83s\tremaining: 16.1s\n",
      "150:\tlearn: 0.2214301\ttotal: 2.85s\tremaining: 16s\n",
      "151:\tlearn: 0.2213796\ttotal: 2.87s\tremaining: 16s\n",
      "152:\tlearn: 0.2213373\ttotal: 2.89s\tremaining: 16s\n",
      "153:\tlearn: 0.2212603\ttotal: 2.91s\tremaining: 16s\n",
      "154:\tlearn: 0.2211749\ttotal: 2.92s\tremaining: 15.9s\n",
      "155:\tlearn: 0.2210782\ttotal: 2.94s\tremaining: 15.9s\n",
      "156:\tlearn: 0.2210311\ttotal: 2.96s\tremaining: 15.9s\n",
      "157:\tlearn: 0.2209968\ttotal: 2.98s\tremaining: 15.9s\n",
      "158:\tlearn: 0.2209544\ttotal: 3s\tremaining: 15.9s\n",
      "159:\tlearn: 0.2208900\ttotal: 3.02s\tremaining: 15.8s\n",
      "160:\tlearn: 0.2208395\ttotal: 3.04s\tremaining: 15.8s\n",
      "161:\tlearn: 0.2207841\ttotal: 3.05s\tremaining: 15.8s\n",
      "162:\tlearn: 0.2207456\ttotal: 3.07s\tremaining: 15.8s\n",
      "163:\tlearn: 0.2207084\ttotal: 3.09s\tremaining: 15.8s\n",
      "164:\tlearn: 0.2206450\ttotal: 3.11s\tremaining: 15.7s\n",
      "165:\tlearn: 0.2205798\ttotal: 3.13s\tremaining: 15.7s\n",
      "166:\tlearn: 0.2205428\ttotal: 3.14s\tremaining: 15.7s\n",
      "167:\tlearn: 0.2205167\ttotal: 3.16s\tremaining: 15.7s\n",
      "168:\tlearn: 0.2204560\ttotal: 3.18s\tremaining: 15.6s\n",
      "169:\tlearn: 0.2204179\ttotal: 3.2s\tremaining: 15.6s\n",
      "170:\tlearn: 0.2203748\ttotal: 3.22s\tremaining: 15.6s\n",
      "171:\tlearn: 0.2203346\ttotal: 3.23s\tremaining: 15.6s\n",
      "172:\tlearn: 0.2203122\ttotal: 3.25s\tremaining: 15.5s\n",
      "173:\tlearn: 0.2202743\ttotal: 3.27s\tremaining: 15.5s\n",
      "174:\tlearn: 0.2202477\ttotal: 3.29s\tremaining: 15.5s\n",
      "175:\tlearn: 0.2202042\ttotal: 3.3s\tremaining: 15.5s\n",
      "176:\tlearn: 0.2201689\ttotal: 3.32s\tremaining: 15.4s\n",
      "177:\tlearn: 0.2201356\ttotal: 3.34s\tremaining: 15.4s\n",
      "178:\tlearn: 0.2200984\ttotal: 3.36s\tremaining: 15.4s\n",
      "179:\tlearn: 0.2200753\ttotal: 3.37s\tremaining: 15.4s\n",
      "180:\tlearn: 0.2200233\ttotal: 3.39s\tremaining: 15.3s\n",
      "181:\tlearn: 0.2199633\ttotal: 3.41s\tremaining: 15.3s\n",
      "182:\tlearn: 0.2199362\ttotal: 3.43s\tremaining: 15.3s\n",
      "183:\tlearn: 0.2198602\ttotal: 3.45s\tremaining: 15.3s\n",
      "184:\tlearn: 0.2198113\ttotal: 3.46s\tremaining: 15.3s\n",
      "185:\tlearn: 0.2197653\ttotal: 3.48s\tremaining: 15.2s\n",
      "186:\tlearn: 0.2197300\ttotal: 3.5s\tremaining: 15.2s\n",
      "187:\tlearn: 0.2196944\ttotal: 3.52s\tremaining: 15.2s\n",
      "188:\tlearn: 0.2196560\ttotal: 3.54s\tremaining: 15.2s\n",
      "189:\tlearn: 0.2195828\ttotal: 3.55s\tremaining: 15.1s\n",
      "190:\tlearn: 0.2195640\ttotal: 3.57s\tremaining: 15.1s\n",
      "191:\tlearn: 0.2195239\ttotal: 3.59s\tremaining: 15.1s\n",
      "192:\tlearn: 0.2195031\ttotal: 3.6s\tremaining: 15.1s\n",
      "193:\tlearn: 0.2194709\ttotal: 3.62s\tremaining: 15.1s\n",
      "194:\tlearn: 0.2194400\ttotal: 3.64s\tremaining: 15s\n",
      "195:\tlearn: 0.2194155\ttotal: 3.66s\tremaining: 15s\n",
      "196:\tlearn: 0.2193838\ttotal: 3.68s\tremaining: 15s\n",
      "197:\tlearn: 0.2193535\ttotal: 3.7s\tremaining: 15s\n",
      "198:\tlearn: 0.2193268\ttotal: 3.71s\tremaining: 15s\n",
      "199:\tlearn: 0.2192962\ttotal: 3.73s\tremaining: 14.9s\n",
      "200:\tlearn: 0.2192727\ttotal: 3.75s\tremaining: 14.9s\n",
      "201:\tlearn: 0.2192522\ttotal: 3.77s\tremaining: 14.9s\n",
      "202:\tlearn: 0.2192188\ttotal: 3.78s\tremaining: 14.9s\n",
      "203:\tlearn: 0.2191869\ttotal: 3.8s\tremaining: 14.8s\n",
      "204:\tlearn: 0.2191681\ttotal: 3.82s\tremaining: 14.8s\n",
      "205:\tlearn: 0.2191349\ttotal: 3.84s\tremaining: 14.8s\n",
      "206:\tlearn: 0.2191027\ttotal: 3.86s\tremaining: 14.8s\n",
      "207:\tlearn: 0.2190564\ttotal: 3.88s\tremaining: 14.8s\n",
      "208:\tlearn: 0.2190378\ttotal: 3.89s\tremaining: 14.7s\n",
      "209:\tlearn: 0.2190103\ttotal: 3.91s\tremaining: 14.7s\n",
      "210:\tlearn: 0.2189825\ttotal: 3.93s\tremaining: 14.7s\n",
      "211:\tlearn: 0.2189696\ttotal: 3.95s\tremaining: 14.7s\n",
      "212:\tlearn: 0.2189473\ttotal: 3.96s\tremaining: 14.6s\n",
      "213:\tlearn: 0.2189196\ttotal: 3.98s\tremaining: 14.6s\n",
      "214:\tlearn: 0.2188952\ttotal: 4s\tremaining: 14.6s\n",
      "215:\tlearn: 0.2188712\ttotal: 4.02s\tremaining: 14.6s\n",
      "216:\tlearn: 0.2188541\ttotal: 4.03s\tremaining: 14.6s\n",
      "217:\tlearn: 0.2188296\ttotal: 4.05s\tremaining: 14.5s\n",
      "218:\tlearn: 0.2187850\ttotal: 4.07s\tremaining: 14.5s\n",
      "219:\tlearn: 0.2187357\ttotal: 4.09s\tremaining: 14.5s\n",
      "220:\tlearn: 0.2187091\ttotal: 4.11s\tremaining: 14.5s\n",
      "221:\tlearn: 0.2186770\ttotal: 4.13s\tremaining: 14.5s\n",
      "222:\tlearn: 0.2186480\ttotal: 4.14s\tremaining: 14.4s\n",
      "223:\tlearn: 0.2186124\ttotal: 4.16s\tremaining: 14.4s\n",
      "224:\tlearn: 0.2185886\ttotal: 4.18s\tremaining: 14.4s\n",
      "225:\tlearn: 0.2185625\ttotal: 4.2s\tremaining: 14.4s\n",
      "226:\tlearn: 0.2185298\ttotal: 4.21s\tremaining: 14.4s\n",
      "227:\tlearn: 0.2185066\ttotal: 4.23s\tremaining: 14.3s\n",
      "228:\tlearn: 0.2184766\ttotal: 4.25s\tremaining: 14.3s\n",
      "229:\tlearn: 0.2184510\ttotal: 4.27s\tremaining: 14.3s\n",
      "230:\tlearn: 0.2183960\ttotal: 4.29s\tremaining: 14.3s\n",
      "231:\tlearn: 0.2183815\ttotal: 4.3s\tremaining: 14.2s\n",
      "232:\tlearn: 0.2183471\ttotal: 4.32s\tremaining: 14.2s\n",
      "233:\tlearn: 0.2183128\ttotal: 4.34s\tremaining: 14.2s\n",
      "234:\tlearn: 0.2182747\ttotal: 4.36s\tremaining: 14.2s\n",
      "235:\tlearn: 0.2182376\ttotal: 4.38s\tremaining: 14.2s\n",
      "236:\tlearn: 0.2181797\ttotal: 4.39s\tremaining: 14.1s\n",
      "237:\tlearn: 0.2181260\ttotal: 4.41s\tremaining: 14.1s\n",
      "238:\tlearn: 0.2180771\ttotal: 4.43s\tremaining: 14.1s\n",
      "239:\tlearn: 0.2180531\ttotal: 4.45s\tremaining: 14.1s\n",
      "240:\tlearn: 0.2180413\ttotal: 4.46s\tremaining: 14.1s\n",
      "241:\tlearn: 0.2180166\ttotal: 4.48s\tremaining: 14s\n",
      "242:\tlearn: 0.2179869\ttotal: 4.5s\tremaining: 14s\n",
      "243:\tlearn: 0.2179152\ttotal: 4.52s\tremaining: 14s\n",
      "244:\tlearn: 0.2178822\ttotal: 4.54s\tremaining: 14s\n",
      "245:\tlearn: 0.2178539\ttotal: 4.55s\tremaining: 14s\n",
      "246:\tlearn: 0.2178396\ttotal: 4.57s\tremaining: 13.9s\n",
      "247:\tlearn: 0.2178181\ttotal: 4.59s\tremaining: 13.9s\n",
      "248:\tlearn: 0.2177747\ttotal: 4.6s\tremaining: 13.9s\n",
      "249:\tlearn: 0.2177247\ttotal: 4.62s\tremaining: 13.9s\n",
      "250:\tlearn: 0.2176975\ttotal: 4.64s\tremaining: 13.8s\n",
      "251:\tlearn: 0.2176510\ttotal: 4.66s\tremaining: 13.8s\n",
      "252:\tlearn: 0.2176212\ttotal: 4.67s\tremaining: 13.8s\n",
      "253:\tlearn: 0.2175973\ttotal: 4.69s\tremaining: 13.8s\n",
      "254:\tlearn: 0.2175748\ttotal: 4.71s\tremaining: 13.8s\n",
      "255:\tlearn: 0.2175331\ttotal: 4.73s\tremaining: 13.7s\n",
      "256:\tlearn: 0.2175039\ttotal: 4.75s\tremaining: 13.7s\n",
      "257:\tlearn: 0.2174713\ttotal: 4.76s\tremaining: 13.7s\n",
      "258:\tlearn: 0.2174289\ttotal: 4.78s\tremaining: 13.7s\n",
      "259:\tlearn: 0.2174125\ttotal: 4.8s\tremaining: 13.7s\n",
      "260:\tlearn: 0.2173859\ttotal: 4.82s\tremaining: 13.6s\n",
      "261:\tlearn: 0.2173614\ttotal: 4.83s\tremaining: 13.6s\n",
      "262:\tlearn: 0.2173129\ttotal: 4.85s\tremaining: 13.6s\n",
      "263:\tlearn: 0.2172794\ttotal: 4.87s\tremaining: 13.6s\n",
      "264:\tlearn: 0.2172490\ttotal: 4.89s\tremaining: 13.6s\n",
      "265:\tlearn: 0.2171982\ttotal: 4.91s\tremaining: 13.5s\n",
      "266:\tlearn: 0.2171702\ttotal: 4.93s\tremaining: 13.5s\n",
      "267:\tlearn: 0.2171400\ttotal: 4.95s\tremaining: 13.5s\n",
      "268:\tlearn: 0.2171068\ttotal: 4.96s\tremaining: 13.5s\n",
      "269:\tlearn: 0.2170487\ttotal: 4.98s\tremaining: 13.5s\n",
      "270:\tlearn: 0.2170223\ttotal: 5s\tremaining: 13.5s\n",
      "271:\tlearn: 0.2169855\ttotal: 5.02s\tremaining: 13.4s\n",
      "272:\tlearn: 0.2169534\ttotal: 5.04s\tremaining: 13.4s\n",
      "273:\tlearn: 0.2169178\ttotal: 5.05s\tremaining: 13.4s\n",
      "274:\tlearn: 0.2168786\ttotal: 5.07s\tremaining: 13.4s\n",
      "275:\tlearn: 0.2168442\ttotal: 5.09s\tremaining: 13.3s\n",
      "276:\tlearn: 0.2168082\ttotal: 5.11s\tremaining: 13.3s\n",
      "277:\tlearn: 0.2167894\ttotal: 5.12s\tremaining: 13.3s\n",
      "278:\tlearn: 0.2167703\ttotal: 5.14s\tremaining: 13.3s\n",
      "279:\tlearn: 0.2167495\ttotal: 5.16s\tremaining: 13.3s\n",
      "280:\tlearn: 0.2167164\ttotal: 5.18s\tremaining: 13.3s\n",
      "281:\tlearn: 0.2166733\ttotal: 5.2s\tremaining: 13.2s\n",
      "282:\tlearn: 0.2166332\ttotal: 5.22s\tremaining: 13.2s\n",
      "283:\tlearn: 0.2166138\ttotal: 5.24s\tremaining: 13.2s\n",
      "284:\tlearn: 0.2165859\ttotal: 5.25s\tremaining: 13.2s\n",
      "285:\tlearn: 0.2165381\ttotal: 5.27s\tremaining: 13.2s\n",
      "286:\tlearn: 0.2165173\ttotal: 5.29s\tremaining: 13.1s\n",
      "287:\tlearn: 0.2165065\ttotal: 5.31s\tremaining: 13.1s\n",
      "288:\tlearn: 0.2164874\ttotal: 5.33s\tremaining: 13.1s\n",
      "289:\tlearn: 0.2164633\ttotal: 5.34s\tremaining: 13.1s\n",
      "290:\tlearn: 0.2164305\ttotal: 5.36s\tremaining: 13.1s\n",
      "291:\tlearn: 0.2163984\ttotal: 5.38s\tremaining: 13.1s\n",
      "292:\tlearn: 0.2163807\ttotal: 5.4s\tremaining: 13s\n",
      "293:\tlearn: 0.2163578\ttotal: 5.42s\tremaining: 13s\n",
      "294:\tlearn: 0.2163270\ttotal: 5.43s\tremaining: 13s\n",
      "295:\tlearn: 0.2162972\ttotal: 5.45s\tremaining: 13s\n",
      "296:\tlearn: 0.2162492\ttotal: 5.47s\tremaining: 12.9s\n",
      "297:\tlearn: 0.2162252\ttotal: 5.49s\tremaining: 12.9s\n",
      "298:\tlearn: 0.2161924\ttotal: 5.5s\tremaining: 12.9s\n",
      "299:\tlearn: 0.2161740\ttotal: 5.52s\tremaining: 12.9s\n",
      "300:\tlearn: 0.2161599\ttotal: 5.54s\tremaining: 12.9s\n",
      "301:\tlearn: 0.2161351\ttotal: 5.56s\tremaining: 12.8s\n",
      "302:\tlearn: 0.2161148\ttotal: 5.58s\tremaining: 12.8s\n",
      "303:\tlearn: 0.2160854\ttotal: 5.6s\tremaining: 12.8s\n",
      "304:\tlearn: 0.2160536\ttotal: 5.62s\tremaining: 12.8s\n",
      "305:\tlearn: 0.2160247\ttotal: 5.63s\tremaining: 12.8s\n",
      "306:\tlearn: 0.2159958\ttotal: 5.65s\tremaining: 12.8s\n",
      "307:\tlearn: 0.2159590\ttotal: 5.67s\tremaining: 12.7s\n",
      "308:\tlearn: 0.2159331\ttotal: 5.69s\tremaining: 12.7s\n",
      "309:\tlearn: 0.2159139\ttotal: 5.71s\tremaining: 12.7s\n",
      "310:\tlearn: 0.2158623\ttotal: 5.72s\tremaining: 12.7s\n",
      "311:\tlearn: 0.2158406\ttotal: 5.74s\tremaining: 12.7s\n",
      "312:\tlearn: 0.2158066\ttotal: 5.76s\tremaining: 12.6s\n",
      "313:\tlearn: 0.2157816\ttotal: 5.78s\tremaining: 12.6s\n",
      "314:\tlearn: 0.2157555\ttotal: 5.79s\tremaining: 12.6s\n",
      "315:\tlearn: 0.2157342\ttotal: 5.81s\tremaining: 12.6s\n",
      "316:\tlearn: 0.2157106\ttotal: 5.83s\tremaining: 12.6s\n",
      "317:\tlearn: 0.2156827\ttotal: 5.85s\tremaining: 12.5s\n",
      "318:\tlearn: 0.2156466\ttotal: 5.86s\tremaining: 12.5s\n",
      "319:\tlearn: 0.2156261\ttotal: 5.88s\tremaining: 12.5s\n",
      "320:\tlearn: 0.2156111\ttotal: 5.9s\tremaining: 12.5s\n",
      "321:\tlearn: 0.2155872\ttotal: 5.92s\tremaining: 12.5s\n",
      "322:\tlearn: 0.2155570\ttotal: 5.94s\tremaining: 12.4s\n",
      "323:\tlearn: 0.2155028\ttotal: 5.95s\tremaining: 12.4s\n",
      "324:\tlearn: 0.2154820\ttotal: 5.97s\tremaining: 12.4s\n",
      "325:\tlearn: 0.2154570\ttotal: 5.99s\tremaining: 12.4s\n",
      "326:\tlearn: 0.2154324\ttotal: 6.01s\tremaining: 12.4s\n",
      "327:\tlearn: 0.2153838\ttotal: 6.03s\tremaining: 12.3s\n",
      "328:\tlearn: 0.2153639\ttotal: 6.04s\tremaining: 12.3s\n",
      "329:\tlearn: 0.2153446\ttotal: 6.06s\tremaining: 12.3s\n",
      "330:\tlearn: 0.2153176\ttotal: 6.08s\tremaining: 12.3s\n",
      "331:\tlearn: 0.2152997\ttotal: 6.1s\tremaining: 12.3s\n",
      "332:\tlearn: 0.2152773\ttotal: 6.12s\tremaining: 12.3s\n",
      "333:\tlearn: 0.2152505\ttotal: 6.13s\tremaining: 12.2s\n",
      "334:\tlearn: 0.2152236\ttotal: 6.15s\tremaining: 12.2s\n",
      "335:\tlearn: 0.2151910\ttotal: 6.17s\tremaining: 12.2s\n",
      "336:\tlearn: 0.2151537\ttotal: 6.19s\tremaining: 12.2s\n",
      "337:\tlearn: 0.2151232\ttotal: 6.21s\tremaining: 12.2s\n",
      "338:\tlearn: 0.2151001\ttotal: 6.23s\tremaining: 12.1s\n",
      "339:\tlearn: 0.2150696\ttotal: 6.25s\tremaining: 12.1s\n",
      "340:\tlearn: 0.2150472\ttotal: 6.27s\tremaining: 12.1s\n",
      "341:\tlearn: 0.2150109\ttotal: 6.29s\tremaining: 12.1s\n",
      "342:\tlearn: 0.2149546\ttotal: 6.3s\tremaining: 12.1s\n",
      "343:\tlearn: 0.2149054\ttotal: 6.32s\tremaining: 12.1s\n",
      "344:\tlearn: 0.2148733\ttotal: 6.34s\tremaining: 12s\n",
      "345:\tlearn: 0.2148353\ttotal: 6.36s\tremaining: 12s\n",
      "346:\tlearn: 0.2148067\ttotal: 6.37s\tremaining: 12s\n",
      "347:\tlearn: 0.2147764\ttotal: 6.39s\tremaining: 12s\n",
      "348:\tlearn: 0.2147344\ttotal: 6.41s\tremaining: 12s\n",
      "349:\tlearn: 0.2147198\ttotal: 6.43s\tremaining: 11.9s\n",
      "350:\tlearn: 0.2146984\ttotal: 6.45s\tremaining: 11.9s\n",
      "351:\tlearn: 0.2146726\ttotal: 6.46s\tremaining: 11.9s\n",
      "352:\tlearn: 0.2146283\ttotal: 6.48s\tremaining: 11.9s\n",
      "353:\tlearn: 0.2145938\ttotal: 6.5s\tremaining: 11.9s\n",
      "354:\tlearn: 0.2145722\ttotal: 6.52s\tremaining: 11.8s\n",
      "355:\tlearn: 0.2145298\ttotal: 6.53s\tremaining: 11.8s\n",
      "356:\tlearn: 0.2145012\ttotal: 6.55s\tremaining: 11.8s\n",
      "357:\tlearn: 0.2144662\ttotal: 6.57s\tremaining: 11.8s\n",
      "358:\tlearn: 0.2144202\ttotal: 6.59s\tremaining: 11.8s\n",
      "359:\tlearn: 0.2143941\ttotal: 6.6s\tremaining: 11.7s\n",
      "360:\tlearn: 0.2143628\ttotal: 6.62s\tremaining: 11.7s\n",
      "361:\tlearn: 0.2143432\ttotal: 6.64s\tremaining: 11.7s\n",
      "362:\tlearn: 0.2143236\ttotal: 6.66s\tremaining: 11.7s\n",
      "363:\tlearn: 0.2143047\ttotal: 6.68s\tremaining: 11.7s\n",
      "364:\tlearn: 0.2142800\ttotal: 6.69s\tremaining: 11.6s\n",
      "365:\tlearn: 0.2142552\ttotal: 6.71s\tremaining: 11.6s\n",
      "366:\tlearn: 0.2142212\ttotal: 6.73s\tremaining: 11.6s\n",
      "367:\tlearn: 0.2141682\ttotal: 6.75s\tremaining: 11.6s\n",
      "368:\tlearn: 0.2141138\ttotal: 6.76s\tremaining: 11.6s\n",
      "369:\tlearn: 0.2140969\ttotal: 6.78s\tremaining: 11.6s\n",
      "370:\tlearn: 0.2140812\ttotal: 6.8s\tremaining: 11.5s\n",
      "371:\tlearn: 0.2140359\ttotal: 6.82s\tremaining: 11.5s\n",
      "372:\tlearn: 0.2139991\ttotal: 6.84s\tremaining: 11.5s\n",
      "373:\tlearn: 0.2139782\ttotal: 6.86s\tremaining: 11.5s\n",
      "374:\tlearn: 0.2139560\ttotal: 6.88s\tremaining: 11.5s\n",
      "375:\tlearn: 0.2139354\ttotal: 6.89s\tremaining: 11.4s\n",
      "376:\tlearn: 0.2138763\ttotal: 6.91s\tremaining: 11.4s\n",
      "377:\tlearn: 0.2138482\ttotal: 6.93s\tremaining: 11.4s\n",
      "378:\tlearn: 0.2138267\ttotal: 6.95s\tremaining: 11.4s\n",
      "379:\tlearn: 0.2138028\ttotal: 6.96s\tremaining: 11.4s\n",
      "380:\tlearn: 0.2137798\ttotal: 6.98s\tremaining: 11.3s\n",
      "381:\tlearn: 0.2137626\ttotal: 7s\tremaining: 11.3s\n",
      "382:\tlearn: 0.2137354\ttotal: 7.02s\tremaining: 11.3s\n",
      "383:\tlearn: 0.2137169\ttotal: 7.04s\tremaining: 11.3s\n",
      "384:\tlearn: 0.2136896\ttotal: 7.05s\tremaining: 11.3s\n",
      "385:\tlearn: 0.2136510\ttotal: 7.08s\tremaining: 11.3s\n",
      "386:\tlearn: 0.2136228\ttotal: 7.09s\tremaining: 11.2s\n",
      "387:\tlearn: 0.2135983\ttotal: 7.11s\tremaining: 11.2s\n",
      "388:\tlearn: 0.2135810\ttotal: 7.13s\tremaining: 11.2s\n",
      "389:\tlearn: 0.2135528\ttotal: 7.15s\tremaining: 11.2s\n",
      "390:\tlearn: 0.2135346\ttotal: 7.16s\tremaining: 11.2s\n",
      "391:\tlearn: 0.2135166\ttotal: 7.18s\tremaining: 11.1s\n",
      "392:\tlearn: 0.2134734\ttotal: 7.2s\tremaining: 11.1s\n",
      "393:\tlearn: 0.2134385\ttotal: 7.22s\tremaining: 11.1s\n",
      "394:\tlearn: 0.2134185\ttotal: 7.24s\tremaining: 11.1s\n",
      "395:\tlearn: 0.2133840\ttotal: 7.26s\tremaining: 11.1s\n",
      "396:\tlearn: 0.2133625\ttotal: 7.27s\tremaining: 11s\n",
      "397:\tlearn: 0.2133455\ttotal: 7.29s\tremaining: 11s\n",
      "398:\tlearn: 0.2133270\ttotal: 7.31s\tremaining: 11s\n",
      "399:\tlearn: 0.2133152\ttotal: 7.33s\tremaining: 11s\n",
      "400:\tlearn: 0.2132931\ttotal: 7.34s\tremaining: 11s\n",
      "401:\tlearn: 0.2132610\ttotal: 7.36s\tremaining: 11s\n",
      "402:\tlearn: 0.2132124\ttotal: 7.38s\tremaining: 10.9s\n",
      "403:\tlearn: 0.2131982\ttotal: 7.4s\tremaining: 10.9s\n",
      "404:\tlearn: 0.2131755\ttotal: 7.42s\tremaining: 10.9s\n",
      "405:\tlearn: 0.2131486\ttotal: 7.43s\tremaining: 10.9s\n",
      "406:\tlearn: 0.2131140\ttotal: 7.45s\tremaining: 10.9s\n",
      "407:\tlearn: 0.2130891\ttotal: 7.47s\tremaining: 10.8s\n",
      "408:\tlearn: 0.2130645\ttotal: 7.49s\tremaining: 10.8s\n",
      "409:\tlearn: 0.2130330\ttotal: 7.5s\tremaining: 10.8s\n",
      "410:\tlearn: 0.2130154\ttotal: 7.52s\tremaining: 10.8s\n",
      "411:\tlearn: 0.2129619\ttotal: 7.54s\tremaining: 10.8s\n",
      "412:\tlearn: 0.2129356\ttotal: 7.56s\tremaining: 10.7s\n",
      "413:\tlearn: 0.2129153\ttotal: 7.57s\tremaining: 10.7s\n",
      "414:\tlearn: 0.2128959\ttotal: 7.59s\tremaining: 10.7s\n",
      "415:\tlearn: 0.2128775\ttotal: 7.61s\tremaining: 10.7s\n",
      "416:\tlearn: 0.2128599\ttotal: 7.63s\tremaining: 10.7s\n",
      "417:\tlearn: 0.2128416\ttotal: 7.64s\tremaining: 10.6s\n",
      "418:\tlearn: 0.2128052\ttotal: 7.66s\tremaining: 10.6s\n",
      "419:\tlearn: 0.2127882\ttotal: 7.68s\tremaining: 10.6s\n",
      "420:\tlearn: 0.2127770\ttotal: 7.7s\tremaining: 10.6s\n",
      "421:\tlearn: 0.2127515\ttotal: 7.72s\tremaining: 10.6s\n",
      "422:\tlearn: 0.2127168\ttotal: 7.74s\tremaining: 10.6s\n",
      "423:\tlearn: 0.2126945\ttotal: 7.75s\tremaining: 10.5s\n",
      "424:\tlearn: 0.2126758\ttotal: 7.77s\tremaining: 10.5s\n",
      "425:\tlearn: 0.2126495\ttotal: 7.79s\tremaining: 10.5s\n",
      "426:\tlearn: 0.2126239\ttotal: 7.8s\tremaining: 10.5s\n",
      "427:\tlearn: 0.2126082\ttotal: 7.82s\tremaining: 10.5s\n",
      "428:\tlearn: 0.2125822\ttotal: 7.84s\tremaining: 10.4s\n",
      "429:\tlearn: 0.2125333\ttotal: 7.86s\tremaining: 10.4s\n",
      "430:\tlearn: 0.2125112\ttotal: 7.88s\tremaining: 10.4s\n",
      "431:\tlearn: 0.2124930\ttotal: 7.89s\tremaining: 10.4s\n",
      "432:\tlearn: 0.2124496\ttotal: 7.91s\tremaining: 10.4s\n",
      "433:\tlearn: 0.2124224\ttotal: 7.93s\tremaining: 10.3s\n",
      "434:\tlearn: 0.2124079\ttotal: 7.95s\tremaining: 10.3s\n",
      "435:\tlearn: 0.2123605\ttotal: 7.97s\tremaining: 10.3s\n",
      "436:\tlearn: 0.2123396\ttotal: 7.98s\tremaining: 10.3s\n",
      "437:\tlearn: 0.2122952\ttotal: 8s\tremaining: 10.3s\n",
      "438:\tlearn: 0.2122793\ttotal: 8.02s\tremaining: 10.2s\n",
      "439:\tlearn: 0.2122581\ttotal: 8.04s\tremaining: 10.2s\n",
      "440:\tlearn: 0.2122410\ttotal: 8.05s\tremaining: 10.2s\n",
      "441:\tlearn: 0.2121962\ttotal: 8.07s\tremaining: 10.2s\n",
      "442:\tlearn: 0.2121518\ttotal: 8.09s\tremaining: 10.2s\n",
      "443:\tlearn: 0.2121242\ttotal: 8.11s\tremaining: 10.2s\n",
      "444:\tlearn: 0.2121070\ttotal: 8.13s\tremaining: 10.1s\n",
      "445:\tlearn: 0.2120656\ttotal: 8.15s\tremaining: 10.1s\n",
      "446:\tlearn: 0.2120243\ttotal: 8.16s\tremaining: 10.1s\n",
      "447:\tlearn: 0.2119852\ttotal: 8.18s\tremaining: 10.1s\n",
      "448:\tlearn: 0.2119614\ttotal: 8.2s\tremaining: 10.1s\n",
      "449:\tlearn: 0.2119317\ttotal: 8.22s\tremaining: 10s\n",
      "450:\tlearn: 0.2118938\ttotal: 8.24s\tremaining: 10s\n",
      "451:\tlearn: 0.2118510\ttotal: 8.26s\tremaining: 10s\n",
      "452:\tlearn: 0.2118154\ttotal: 8.27s\tremaining: 9.99s\n",
      "453:\tlearn: 0.2117777\ttotal: 8.29s\tremaining: 9.97s\n",
      "454:\tlearn: 0.2117504\ttotal: 8.31s\tremaining: 9.95s\n",
      "455:\tlearn: 0.2117148\ttotal: 8.33s\tremaining: 9.93s\n",
      "456:\tlearn: 0.2116707\ttotal: 8.35s\tremaining: 9.91s\n",
      "457:\tlearn: 0.2116472\ttotal: 8.36s\tremaining: 9.9s\n",
      "458:\tlearn: 0.2116144\ttotal: 8.38s\tremaining: 9.88s\n",
      "459:\tlearn: 0.2115825\ttotal: 8.4s\tremaining: 9.86s\n",
      "460:\tlearn: 0.2115501\ttotal: 8.42s\tremaining: 9.84s\n",
      "461:\tlearn: 0.2115296\ttotal: 8.43s\tremaining: 9.82s\n",
      "462:\tlearn: 0.2115133\ttotal: 8.45s\tremaining: 9.8s\n",
      "463:\tlearn: 0.2114848\ttotal: 8.47s\tremaining: 9.79s\n",
      "464:\tlearn: 0.2114642\ttotal: 8.49s\tremaining: 9.77s\n",
      "465:\tlearn: 0.2114452\ttotal: 8.51s\tremaining: 9.75s\n",
      "466:\tlearn: 0.2114207\ttotal: 8.52s\tremaining: 9.73s\n",
      "467:\tlearn: 0.2113903\ttotal: 8.54s\tremaining: 9.71s\n",
      "468:\tlearn: 0.2113740\ttotal: 8.56s\tremaining: 9.69s\n",
      "469:\tlearn: 0.2113554\ttotal: 8.58s\tremaining: 9.67s\n",
      "470:\tlearn: 0.2113325\ttotal: 8.6s\tremaining: 9.66s\n",
      "471:\tlearn: 0.2113001\ttotal: 8.62s\tremaining: 9.64s\n",
      "472:\tlearn: 0.2112803\ttotal: 8.63s\tremaining: 9.62s\n",
      "473:\tlearn: 0.2112599\ttotal: 8.65s\tremaining: 9.6s\n",
      "474:\tlearn: 0.2112336\ttotal: 8.67s\tremaining: 9.58s\n",
      "475:\tlearn: 0.2112174\ttotal: 8.69s\tremaining: 9.56s\n",
      "476:\tlearn: 0.2111959\ttotal: 8.7s\tremaining: 9.54s\n",
      "477:\tlearn: 0.2111736\ttotal: 8.72s\tremaining: 9.53s\n",
      "478:\tlearn: 0.2111461\ttotal: 8.74s\tremaining: 9.51s\n",
      "479:\tlearn: 0.2111060\ttotal: 8.76s\tremaining: 9.49s\n",
      "480:\tlearn: 0.2110840\ttotal: 8.78s\tremaining: 9.47s\n",
      "481:\tlearn: 0.2110707\ttotal: 8.79s\tremaining: 9.45s\n",
      "482:\tlearn: 0.2110502\ttotal: 8.81s\tremaining: 9.43s\n",
      "483:\tlearn: 0.2110197\ttotal: 8.83s\tremaining: 9.41s\n",
      "484:\tlearn: 0.2109867\ttotal: 8.85s\tremaining: 9.4s\n",
      "485:\tlearn: 0.2109446\ttotal: 8.87s\tremaining: 9.38s\n",
      "486:\tlearn: 0.2109296\ttotal: 8.88s\tremaining: 9.36s\n",
      "487:\tlearn: 0.2108991\ttotal: 8.9s\tremaining: 9.34s\n",
      "488:\tlearn: 0.2108837\ttotal: 8.92s\tremaining: 9.32s\n",
      "489:\tlearn: 0.2108648\ttotal: 8.94s\tremaining: 9.3s\n",
      "490:\tlearn: 0.2108550\ttotal: 8.95s\tremaining: 9.28s\n",
      "491:\tlearn: 0.2108317\ttotal: 8.97s\tremaining: 9.26s\n",
      "492:\tlearn: 0.2108110\ttotal: 8.99s\tremaining: 9.25s\n",
      "493:\tlearn: 0.2107958\ttotal: 9.01s\tremaining: 9.23s\n",
      "494:\tlearn: 0.2107779\ttotal: 9.03s\tremaining: 9.21s\n",
      "495:\tlearn: 0.2107429\ttotal: 9.04s\tremaining: 9.19s\n",
      "496:\tlearn: 0.2107145\ttotal: 9.06s\tremaining: 9.17s\n",
      "497:\tlearn: 0.2106923\ttotal: 9.08s\tremaining: 9.15s\n",
      "498:\tlearn: 0.2106630\ttotal: 9.1s\tremaining: 9.13s\n",
      "499:\tlearn: 0.2106434\ttotal: 9.12s\tremaining: 9.12s\n",
      "500:\tlearn: 0.2106233\ttotal: 9.13s\tremaining: 9.1s\n",
      "501:\tlearn: 0.2106023\ttotal: 9.15s\tremaining: 9.08s\n",
      "502:\tlearn: 0.2105880\ttotal: 9.17s\tremaining: 9.06s\n",
      "503:\tlearn: 0.2105636\ttotal: 9.19s\tremaining: 9.04s\n",
      "504:\tlearn: 0.2105395\ttotal: 9.21s\tremaining: 9.02s\n",
      "505:\tlearn: 0.2105087\ttotal: 9.23s\tremaining: 9.01s\n",
      "506:\tlearn: 0.2104917\ttotal: 9.25s\tremaining: 8.99s\n",
      "507:\tlearn: 0.2104567\ttotal: 9.26s\tremaining: 8.97s\n",
      "508:\tlearn: 0.2104263\ttotal: 9.28s\tremaining: 8.95s\n",
      "509:\tlearn: 0.2104000\ttotal: 9.3s\tremaining: 8.94s\n",
      "510:\tlearn: 0.2103853\ttotal: 9.32s\tremaining: 8.92s\n",
      "511:\tlearn: 0.2103680\ttotal: 9.34s\tremaining: 8.9s\n",
      "512:\tlearn: 0.2103230\ttotal: 9.35s\tremaining: 8.88s\n",
      "513:\tlearn: 0.2103032\ttotal: 9.37s\tremaining: 8.86s\n",
      "514:\tlearn: 0.2102836\ttotal: 9.39s\tremaining: 8.84s\n",
      "515:\tlearn: 0.2102649\ttotal: 9.42s\tremaining: 8.83s\n",
      "516:\tlearn: 0.2102488\ttotal: 9.44s\tremaining: 8.81s\n",
      "517:\tlearn: 0.2102329\ttotal: 9.45s\tremaining: 8.8s\n",
      "518:\tlearn: 0.2102069\ttotal: 9.47s\tremaining: 8.78s\n",
      "519:\tlearn: 0.2101788\ttotal: 9.49s\tremaining: 8.76s\n",
      "520:\tlearn: 0.2101407\ttotal: 9.51s\tremaining: 8.74s\n",
      "521:\tlearn: 0.2101257\ttotal: 9.52s\tremaining: 8.72s\n",
      "522:\tlearn: 0.2100979\ttotal: 9.54s\tremaining: 8.7s\n",
      "523:\tlearn: 0.2100773\ttotal: 9.56s\tremaining: 8.69s\n",
      "524:\tlearn: 0.2100623\ttotal: 9.58s\tremaining: 8.67s\n",
      "525:\tlearn: 0.2100362\ttotal: 9.6s\tremaining: 8.65s\n",
      "526:\tlearn: 0.2100193\ttotal: 9.61s\tremaining: 8.63s\n",
      "527:\tlearn: 0.2100026\ttotal: 9.63s\tremaining: 8.61s\n",
      "528:\tlearn: 0.2099886\ttotal: 9.65s\tremaining: 8.59s\n",
      "529:\tlearn: 0.2099612\ttotal: 9.67s\tremaining: 8.57s\n",
      "530:\tlearn: 0.2099482\ttotal: 9.69s\tremaining: 8.55s\n",
      "531:\tlearn: 0.2099239\ttotal: 9.7s\tremaining: 8.54s\n",
      "532:\tlearn: 0.2098943\ttotal: 9.72s\tremaining: 8.52s\n",
      "533:\tlearn: 0.2098714\ttotal: 9.74s\tremaining: 8.5s\n",
      "534:\tlearn: 0.2098295\ttotal: 9.76s\tremaining: 8.48s\n",
      "535:\tlearn: 0.2097998\ttotal: 9.78s\tremaining: 8.46s\n",
      "536:\tlearn: 0.2097736\ttotal: 9.79s\tremaining: 8.45s\n",
      "537:\tlearn: 0.2097466\ttotal: 9.81s\tremaining: 8.43s\n",
      "538:\tlearn: 0.2097304\ttotal: 9.83s\tremaining: 8.41s\n",
      "539:\tlearn: 0.2096922\ttotal: 9.85s\tremaining: 8.39s\n",
      "540:\tlearn: 0.2096768\ttotal: 9.87s\tremaining: 8.37s\n",
      "541:\tlearn: 0.2096512\ttotal: 9.88s\tremaining: 8.35s\n",
      "542:\tlearn: 0.2096376\ttotal: 9.9s\tremaining: 8.33s\n",
      "543:\tlearn: 0.2096177\ttotal: 9.92s\tremaining: 8.31s\n",
      "544:\tlearn: 0.2095908\ttotal: 9.94s\tremaining: 8.29s\n",
      "545:\tlearn: 0.2095649\ttotal: 9.95s\tremaining: 8.28s\n",
      "546:\tlearn: 0.2095436\ttotal: 9.97s\tremaining: 8.26s\n",
      "547:\tlearn: 0.2095280\ttotal: 9.99s\tremaining: 8.24s\n",
      "548:\tlearn: 0.2095153\ttotal: 10s\tremaining: 8.22s\n",
      "549:\tlearn: 0.2095014\ttotal: 10s\tremaining: 8.2s\n",
      "550:\tlearn: 0.2094722\ttotal: 10s\tremaining: 8.19s\n",
      "551:\tlearn: 0.2094642\ttotal: 10.1s\tremaining: 8.17s\n",
      "552:\tlearn: 0.2094497\ttotal: 10.1s\tremaining: 8.15s\n",
      "553:\tlearn: 0.2094204\ttotal: 10.1s\tremaining: 8.13s\n",
      "554:\tlearn: 0.2093879\ttotal: 10.1s\tremaining: 8.11s\n",
      "555:\tlearn: 0.2093754\ttotal: 10.1s\tremaining: 8.09s\n",
      "556:\tlearn: 0.2093612\ttotal: 10.2s\tremaining: 8.07s\n",
      "557:\tlearn: 0.2093226\ttotal: 10.2s\tremaining: 8.06s\n",
      "558:\tlearn: 0.2093038\ttotal: 10.2s\tremaining: 8.04s\n",
      "559:\tlearn: 0.2092816\ttotal: 10.2s\tremaining: 8.02s\n",
      "560:\tlearn: 0.2092610\ttotal: 10.2s\tremaining: 8s\n",
      "561:\tlearn: 0.2092482\ttotal: 10.2s\tremaining: 7.98s\n",
      "562:\tlearn: 0.2092359\ttotal: 10.3s\tremaining: 7.96s\n",
      "563:\tlearn: 0.2092219\ttotal: 10.3s\tremaining: 7.95s\n",
      "564:\tlearn: 0.2092077\ttotal: 10.3s\tremaining: 7.93s\n",
      "565:\tlearn: 0.2091851\ttotal: 10.3s\tremaining: 7.91s\n",
      "566:\tlearn: 0.2091711\ttotal: 10.3s\tremaining: 7.89s\n",
      "567:\tlearn: 0.2091392\ttotal: 10.4s\tremaining: 7.88s\n",
      "568:\tlearn: 0.2091116\ttotal: 10.4s\tremaining: 7.86s\n",
      "569:\tlearn: 0.2091006\ttotal: 10.4s\tremaining: 7.84s\n",
      "570:\tlearn: 0.2090865\ttotal: 10.4s\tremaining: 7.82s\n",
      "571:\tlearn: 0.2090568\ttotal: 10.4s\tremaining: 7.8s\n",
      "572:\tlearn: 0.2090315\ttotal: 10.4s\tremaining: 7.78s\n",
      "573:\tlearn: 0.2090189\ttotal: 10.5s\tremaining: 7.76s\n",
      "574:\tlearn: 0.2090012\ttotal: 10.5s\tremaining: 7.75s\n",
      "575:\tlearn: 0.2089881\ttotal: 10.5s\tremaining: 7.73s\n",
      "576:\tlearn: 0.2089675\ttotal: 10.5s\tremaining: 7.71s\n",
      "577:\tlearn: 0.2089549\ttotal: 10.5s\tremaining: 7.69s\n",
      "578:\tlearn: 0.2089295\ttotal: 10.6s\tremaining: 7.67s\n",
      "579:\tlearn: 0.2089129\ttotal: 10.6s\tremaining: 7.65s\n",
      "580:\tlearn: 0.2088974\ttotal: 10.6s\tremaining: 7.63s\n",
      "581:\tlearn: 0.2088848\ttotal: 10.6s\tremaining: 7.62s\n",
      "582:\tlearn: 0.2088603\ttotal: 10.6s\tremaining: 7.6s\n",
      "583:\tlearn: 0.2088389\ttotal: 10.6s\tremaining: 7.58s\n",
      "584:\tlearn: 0.2088116\ttotal: 10.7s\tremaining: 7.56s\n",
      "585:\tlearn: 0.2087972\ttotal: 10.7s\tremaining: 7.54s\n",
      "586:\tlearn: 0.2087794\ttotal: 10.7s\tremaining: 7.52s\n",
      "587:\tlearn: 0.2087674\ttotal: 10.7s\tremaining: 7.5s\n",
      "588:\tlearn: 0.2087551\ttotal: 10.7s\tremaining: 7.49s\n",
      "589:\tlearn: 0.2087184\ttotal: 10.7s\tremaining: 7.47s\n",
      "590:\tlearn: 0.2087039\ttotal: 10.8s\tremaining: 7.45s\n",
      "591:\tlearn: 0.2086810\ttotal: 10.8s\tremaining: 7.43s\n",
      "592:\tlearn: 0.2086550\ttotal: 10.8s\tremaining: 7.41s\n",
      "593:\tlearn: 0.2086394\ttotal: 10.8s\tremaining: 7.39s\n",
      "594:\tlearn: 0.2086061\ttotal: 10.8s\tremaining: 7.37s\n",
      "595:\tlearn: 0.2085730\ttotal: 10.8s\tremaining: 7.36s\n",
      "596:\tlearn: 0.2085383\ttotal: 10.9s\tremaining: 7.34s\n",
      "597:\tlearn: 0.2085271\ttotal: 10.9s\tremaining: 7.32s\n",
      "598:\tlearn: 0.2085134\ttotal: 10.9s\tremaining: 7.3s\n",
      "599:\tlearn: 0.2084849\ttotal: 10.9s\tremaining: 7.28s\n",
      "600:\tlearn: 0.2084722\ttotal: 10.9s\tremaining: 7.26s\n",
      "601:\tlearn: 0.2084595\ttotal: 11s\tremaining: 7.24s\n",
      "602:\tlearn: 0.2084470\ttotal: 11s\tremaining: 7.23s\n",
      "603:\tlearn: 0.2084306\ttotal: 11s\tremaining: 7.21s\n",
      "604:\tlearn: 0.2084133\ttotal: 11s\tremaining: 7.19s\n",
      "605:\tlearn: 0.2083866\ttotal: 11s\tremaining: 7.17s\n",
      "606:\tlearn: 0.2083720\ttotal: 11s\tremaining: 7.15s\n",
      "607:\tlearn: 0.2083605\ttotal: 11.1s\tremaining: 7.13s\n",
      "608:\tlearn: 0.2083401\ttotal: 11.1s\tremaining: 7.11s\n",
      "609:\tlearn: 0.2083306\ttotal: 11.1s\tremaining: 7.09s\n",
      "610:\tlearn: 0.2083184\ttotal: 11.1s\tremaining: 7.08s\n",
      "611:\tlearn: 0.2083022\ttotal: 11.1s\tremaining: 7.06s\n",
      "612:\tlearn: 0.2082842\ttotal: 11.2s\tremaining: 7.04s\n",
      "613:\tlearn: 0.2082730\ttotal: 11.2s\tremaining: 7.02s\n",
      "614:\tlearn: 0.2082475\ttotal: 11.2s\tremaining: 7s\n",
      "615:\tlearn: 0.2082118\ttotal: 11.2s\tremaining: 6.99s\n",
      "616:\tlearn: 0.2081848\ttotal: 11.2s\tremaining: 6.97s\n",
      "617:\tlearn: 0.2081646\ttotal: 11.2s\tremaining: 6.95s\n",
      "618:\tlearn: 0.2081393\ttotal: 11.3s\tremaining: 6.93s\n",
      "619:\tlearn: 0.2081143\ttotal: 11.3s\tremaining: 6.91s\n",
      "620:\tlearn: 0.2081018\ttotal: 11.3s\tremaining: 6.9s\n",
      "621:\tlearn: 0.2080779\ttotal: 11.3s\tremaining: 6.88s\n",
      "622:\tlearn: 0.2080656\ttotal: 11.3s\tremaining: 6.86s\n",
      "623:\tlearn: 0.2080521\ttotal: 11.4s\tremaining: 6.84s\n",
      "624:\tlearn: 0.2080216\ttotal: 11.4s\tremaining: 6.82s\n",
      "625:\tlearn: 0.2080064\ttotal: 11.4s\tremaining: 6.8s\n",
      "626:\tlearn: 0.2079765\ttotal: 11.4s\tremaining: 6.79s\n",
      "627:\tlearn: 0.2079635\ttotal: 11.4s\tremaining: 6.77s\n",
      "628:\tlearn: 0.2079523\ttotal: 11.4s\tremaining: 6.75s\n",
      "629:\tlearn: 0.2079288\ttotal: 11.5s\tremaining: 6.73s\n",
      "630:\tlearn: 0.2079042\ttotal: 11.5s\tremaining: 6.71s\n",
      "631:\tlearn: 0.2078815\ttotal: 11.5s\tremaining: 6.69s\n",
      "632:\tlearn: 0.2078438\ttotal: 11.5s\tremaining: 6.67s\n",
      "633:\tlearn: 0.2078312\ttotal: 11.5s\tremaining: 6.66s\n",
      "634:\tlearn: 0.2078017\ttotal: 11.5s\tremaining: 6.64s\n",
      "635:\tlearn: 0.2077892\ttotal: 11.6s\tremaining: 6.62s\n",
      "636:\tlearn: 0.2077762\ttotal: 11.6s\tremaining: 6.6s\n",
      "637:\tlearn: 0.2077550\ttotal: 11.6s\tremaining: 6.58s\n",
      "638:\tlearn: 0.2077428\ttotal: 11.6s\tremaining: 6.56s\n",
      "639:\tlearn: 0.2077291\ttotal: 11.6s\tremaining: 6.54s\n",
      "640:\tlearn: 0.2077213\ttotal: 11.7s\tremaining: 6.53s\n",
      "641:\tlearn: 0.2076981\ttotal: 11.7s\tremaining: 6.51s\n",
      "642:\tlearn: 0.2076792\ttotal: 11.7s\tremaining: 6.49s\n",
      "643:\tlearn: 0.2076658\ttotal: 11.7s\tremaining: 6.47s\n",
      "644:\tlearn: 0.2076552\ttotal: 11.7s\tremaining: 6.45s\n",
      "645:\tlearn: 0.2076269\ttotal: 11.7s\tremaining: 6.44s\n",
      "646:\tlearn: 0.2076027\ttotal: 11.8s\tremaining: 6.42s\n",
      "647:\tlearn: 0.2075778\ttotal: 11.8s\tremaining: 6.4s\n",
      "648:\tlearn: 0.2075608\ttotal: 11.8s\tremaining: 6.38s\n",
      "649:\tlearn: 0.2075494\ttotal: 11.8s\tremaining: 6.37s\n",
      "650:\tlearn: 0.2075388\ttotal: 11.8s\tremaining: 6.35s\n",
      "651:\tlearn: 0.2075265\ttotal: 11.9s\tremaining: 6.33s\n",
      "652:\tlearn: 0.2074953\ttotal: 11.9s\tremaining: 6.31s\n",
      "653:\tlearn: 0.2074738\ttotal: 11.9s\tremaining: 6.29s\n",
      "654:\tlearn: 0.2074461\ttotal: 11.9s\tremaining: 6.27s\n",
      "655:\tlearn: 0.2074247\ttotal: 11.9s\tremaining: 6.25s\n",
      "656:\tlearn: 0.2074114\ttotal: 11.9s\tremaining: 6.24s\n",
      "657:\tlearn: 0.2073909\ttotal: 12s\tremaining: 6.22s\n",
      "658:\tlearn: 0.2073754\ttotal: 12s\tremaining: 6.2s\n",
      "659:\tlearn: 0.2073641\ttotal: 12s\tremaining: 6.18s\n",
      "660:\tlearn: 0.2073496\ttotal: 12s\tremaining: 6.16s\n",
      "661:\tlearn: 0.2073366\ttotal: 12s\tremaining: 6.14s\n",
      "662:\tlearn: 0.2073139\ttotal: 12s\tremaining: 6.12s\n",
      "663:\tlearn: 0.2072923\ttotal: 12.1s\tremaining: 6.11s\n",
      "664:\tlearn: 0.2072872\ttotal: 12.1s\tremaining: 6.09s\n",
      "665:\tlearn: 0.2072379\ttotal: 12.1s\tremaining: 6.07s\n",
      "666:\tlearn: 0.2072100\ttotal: 12.1s\tremaining: 6.05s\n",
      "667:\tlearn: 0.2071882\ttotal: 12.1s\tremaining: 6.03s\n",
      "668:\tlearn: 0.2071705\ttotal: 12.2s\tremaining: 6.01s\n",
      "669:\tlearn: 0.2071163\ttotal: 12.2s\tremaining: 6s\n",
      "670:\tlearn: 0.2070989\ttotal: 12.2s\tremaining: 5.98s\n",
      "671:\tlearn: 0.2070794\ttotal: 12.2s\tremaining: 5.96s\n",
      "672:\tlearn: 0.2070604\ttotal: 12.2s\tremaining: 5.94s\n",
      "673:\tlearn: 0.2070424\ttotal: 12.2s\tremaining: 5.92s\n",
      "674:\tlearn: 0.2070207\ttotal: 12.3s\tremaining: 5.9s\n",
      "675:\tlearn: 0.2070026\ttotal: 12.3s\tremaining: 5.88s\n",
      "676:\tlearn: 0.2069833\ttotal: 12.3s\tremaining: 5.87s\n",
      "677:\tlearn: 0.2069629\ttotal: 12.3s\tremaining: 5.85s\n",
      "678:\tlearn: 0.2069507\ttotal: 12.3s\tremaining: 5.83s\n",
      "679:\tlearn: 0.2069415\ttotal: 12.3s\tremaining: 5.81s\n",
      "680:\tlearn: 0.2069131\ttotal: 12.4s\tremaining: 5.79s\n",
      "681:\tlearn: 0.2068866\ttotal: 12.4s\tremaining: 5.77s\n",
      "682:\tlearn: 0.2068730\ttotal: 12.4s\tremaining: 5.76s\n",
      "683:\tlearn: 0.2068612\ttotal: 12.4s\tremaining: 5.74s\n",
      "684:\tlearn: 0.2068494\ttotal: 12.4s\tremaining: 5.72s\n",
      "685:\tlearn: 0.2067830\ttotal: 12.5s\tremaining: 5.7s\n",
      "686:\tlearn: 0.2067562\ttotal: 12.5s\tremaining: 5.68s\n",
      "687:\tlearn: 0.2067261\ttotal: 12.5s\tremaining: 5.67s\n",
      "688:\tlearn: 0.2067159\ttotal: 12.5s\tremaining: 5.65s\n",
      "689:\tlearn: 0.2067031\ttotal: 12.5s\tremaining: 5.63s\n",
      "690:\tlearn: 0.2066843\ttotal: 12.5s\tremaining: 5.61s\n",
      "691:\tlearn: 0.2066705\ttotal: 12.6s\tremaining: 5.59s\n",
      "692:\tlearn: 0.2066522\ttotal: 12.6s\tremaining: 5.57s\n",
      "693:\tlearn: 0.2066362\ttotal: 12.6s\tremaining: 5.56s\n",
      "694:\tlearn: 0.2065986\ttotal: 12.6s\tremaining: 5.54s\n",
      "695:\tlearn: 0.2065514\ttotal: 12.6s\tremaining: 5.52s\n",
      "696:\tlearn: 0.2065302\ttotal: 12.7s\tremaining: 5.5s\n",
      "697:\tlearn: 0.2065152\ttotal: 12.7s\tremaining: 5.48s\n",
      "698:\tlearn: 0.2064828\ttotal: 12.7s\tremaining: 5.46s\n",
      "699:\tlearn: 0.2064388\ttotal: 12.7s\tremaining: 5.45s\n",
      "700:\tlearn: 0.2064136\ttotal: 12.7s\tremaining: 5.43s\n",
      "701:\tlearn: 0.2064035\ttotal: 12.7s\tremaining: 5.41s\n",
      "702:\tlearn: 0.2063841\ttotal: 12.8s\tremaining: 5.39s\n",
      "703:\tlearn: 0.2063646\ttotal: 12.8s\tremaining: 5.37s\n",
      "704:\tlearn: 0.2063311\ttotal: 12.8s\tremaining: 5.36s\n",
      "705:\tlearn: 0.2063014\ttotal: 12.8s\tremaining: 5.34s\n",
      "706:\tlearn: 0.2062891\ttotal: 12.8s\tremaining: 5.32s\n",
      "707:\tlearn: 0.2062663\ttotal: 12.9s\tremaining: 5.3s\n",
      "708:\tlearn: 0.2062543\ttotal: 12.9s\tremaining: 5.28s\n",
      "709:\tlearn: 0.2062328\ttotal: 12.9s\tremaining: 5.26s\n",
      "710:\tlearn: 0.2062216\ttotal: 12.9s\tremaining: 5.25s\n",
      "711:\tlearn: 0.2062075\ttotal: 12.9s\tremaining: 5.23s\n",
      "712:\tlearn: 0.2061965\ttotal: 12.9s\tremaining: 5.21s\n",
      "713:\tlearn: 0.2061543\ttotal: 13s\tremaining: 5.19s\n",
      "714:\tlearn: 0.2061459\ttotal: 13s\tremaining: 5.17s\n",
      "715:\tlearn: 0.2061206\ttotal: 13s\tremaining: 5.15s\n",
      "716:\tlearn: 0.2061005\ttotal: 13s\tremaining: 5.13s\n",
      "717:\tlearn: 0.2060898\ttotal: 13s\tremaining: 5.12s\n",
      "718:\tlearn: 0.2060810\ttotal: 13s\tremaining: 5.1s\n",
      "719:\tlearn: 0.2060588\ttotal: 13.1s\tremaining: 5.08s\n",
      "720:\tlearn: 0.2060313\ttotal: 13.1s\tremaining: 5.06s\n",
      "721:\tlearn: 0.2059914\ttotal: 13.1s\tremaining: 5.04s\n",
      "722:\tlearn: 0.2059677\ttotal: 13.1s\tremaining: 5.03s\n",
      "723:\tlearn: 0.2059539\ttotal: 13.1s\tremaining: 5.01s\n",
      "724:\tlearn: 0.2059281\ttotal: 13.2s\tremaining: 4.99s\n",
      "725:\tlearn: 0.2059061\ttotal: 13.2s\tremaining: 4.97s\n",
      "726:\tlearn: 0.2058952\ttotal: 13.2s\tremaining: 4.95s\n",
      "727:\tlearn: 0.2058686\ttotal: 13.2s\tremaining: 4.93s\n",
      "728:\tlearn: 0.2058407\ttotal: 13.2s\tremaining: 4.92s\n",
      "729:\tlearn: 0.2058193\ttotal: 13.2s\tremaining: 4.9s\n",
      "730:\tlearn: 0.2058024\ttotal: 13.3s\tremaining: 4.88s\n",
      "731:\tlearn: 0.2057748\ttotal: 13.3s\tremaining: 4.86s\n",
      "732:\tlearn: 0.2057467\ttotal: 13.3s\tremaining: 4.84s\n",
      "733:\tlearn: 0.2057393\ttotal: 13.3s\tremaining: 4.83s\n",
      "734:\tlearn: 0.2057012\ttotal: 13.3s\tremaining: 4.81s\n",
      "735:\tlearn: 0.2056785\ttotal: 13.3s\tremaining: 4.79s\n",
      "736:\tlearn: 0.2056696\ttotal: 13.4s\tremaining: 4.77s\n",
      "737:\tlearn: 0.2056611\ttotal: 13.4s\tremaining: 4.75s\n",
      "738:\tlearn: 0.2056332\ttotal: 13.4s\tremaining: 4.73s\n",
      "739:\tlearn: 0.2056162\ttotal: 13.4s\tremaining: 4.71s\n",
      "740:\tlearn: 0.2055891\ttotal: 13.4s\tremaining: 4.7s\n",
      "741:\tlearn: 0.2055703\ttotal: 13.5s\tremaining: 4.68s\n",
      "742:\tlearn: 0.2055328\ttotal: 13.5s\tremaining: 4.66s\n",
      "743:\tlearn: 0.2055127\ttotal: 13.5s\tremaining: 4.64s\n",
      "744:\tlearn: 0.2054736\ttotal: 13.5s\tremaining: 4.62s\n",
      "745:\tlearn: 0.2054506\ttotal: 13.5s\tremaining: 4.61s\n",
      "746:\tlearn: 0.2054295\ttotal: 13.5s\tremaining: 4.59s\n",
      "747:\tlearn: 0.2054104\ttotal: 13.6s\tremaining: 4.57s\n",
      "748:\tlearn: 0.2053698\ttotal: 13.6s\tremaining: 4.55s\n",
      "749:\tlearn: 0.2053575\ttotal: 13.6s\tremaining: 4.53s\n",
      "750:\tlearn: 0.2053288\ttotal: 13.6s\tremaining: 4.51s\n",
      "751:\tlearn: 0.2053180\ttotal: 13.6s\tremaining: 4.5s\n",
      "752:\tlearn: 0.2052985\ttotal: 13.7s\tremaining: 4.48s\n",
      "753:\tlearn: 0.2052911\ttotal: 13.7s\tremaining: 4.46s\n",
      "754:\tlearn: 0.2052671\ttotal: 13.7s\tremaining: 4.44s\n",
      "755:\tlearn: 0.2052480\ttotal: 13.7s\tremaining: 4.42s\n",
      "756:\tlearn: 0.2052205\ttotal: 13.7s\tremaining: 4.41s\n",
      "757:\tlearn: 0.2052016\ttotal: 13.7s\tremaining: 4.39s\n",
      "758:\tlearn: 0.2051797\ttotal: 13.8s\tremaining: 4.37s\n",
      "759:\tlearn: 0.2051514\ttotal: 13.8s\tremaining: 4.35s\n",
      "760:\tlearn: 0.2051299\ttotal: 13.8s\tremaining: 4.33s\n",
      "761:\tlearn: 0.2051097\ttotal: 13.8s\tremaining: 4.31s\n",
      "762:\tlearn: 0.2050894\ttotal: 13.8s\tremaining: 4.3s\n",
      "763:\tlearn: 0.2050723\ttotal: 13.8s\tremaining: 4.28s\n",
      "764:\tlearn: 0.2050479\ttotal: 13.9s\tremaining: 4.26s\n",
      "765:\tlearn: 0.2050264\ttotal: 13.9s\tremaining: 4.24s\n",
      "766:\tlearn: 0.2049842\ttotal: 13.9s\tremaining: 4.22s\n",
      "767:\tlearn: 0.2049672\ttotal: 13.9s\tremaining: 4.2s\n",
      "768:\tlearn: 0.2049441\ttotal: 13.9s\tremaining: 4.19s\n",
      "769:\tlearn: 0.2049154\ttotal: 14s\tremaining: 4.17s\n",
      "770:\tlearn: 0.2048942\ttotal: 14s\tremaining: 4.15s\n",
      "771:\tlearn: 0.2048793\ttotal: 14s\tremaining: 4.13s\n",
      "772:\tlearn: 0.2048576\ttotal: 14s\tremaining: 4.11s\n",
      "773:\tlearn: 0.2048417\ttotal: 14s\tremaining: 4.09s\n",
      "774:\tlearn: 0.2048132\ttotal: 14s\tremaining: 4.08s\n",
      "775:\tlearn: 0.2047982\ttotal: 14.1s\tremaining: 4.06s\n",
      "776:\tlearn: 0.2047759\ttotal: 14.1s\tremaining: 4.04s\n",
      "777:\tlearn: 0.2047618\ttotal: 14.1s\tremaining: 4.02s\n",
      "778:\tlearn: 0.2047233\ttotal: 14.1s\tremaining: 4s\n",
      "779:\tlearn: 0.2047026\ttotal: 14.1s\tremaining: 3.99s\n",
      "780:\tlearn: 0.2046801\ttotal: 14.2s\tremaining: 3.97s\n",
      "781:\tlearn: 0.2046425\ttotal: 14.2s\tremaining: 3.95s\n",
      "782:\tlearn: 0.2046131\ttotal: 14.2s\tremaining: 3.93s\n",
      "783:\tlearn: 0.2045944\ttotal: 14.2s\tremaining: 3.91s\n",
      "784:\tlearn: 0.2045588\ttotal: 14.2s\tremaining: 3.9s\n",
      "785:\tlearn: 0.2045328\ttotal: 14.2s\tremaining: 3.88s\n",
      "786:\tlearn: 0.2045184\ttotal: 14.3s\tremaining: 3.86s\n",
      "787:\tlearn: 0.2045055\ttotal: 14.3s\tremaining: 3.84s\n",
      "788:\tlearn: 0.2044722\ttotal: 14.3s\tremaining: 3.82s\n",
      "789:\tlearn: 0.2044513\ttotal: 14.3s\tremaining: 3.8s\n",
      "790:\tlearn: 0.2044183\ttotal: 14.3s\tremaining: 3.79s\n",
      "791:\tlearn: 0.2043881\ttotal: 14.3s\tremaining: 3.77s\n",
      "792:\tlearn: 0.2043760\ttotal: 14.4s\tremaining: 3.75s\n",
      "793:\tlearn: 0.2043562\ttotal: 14.4s\tremaining: 3.73s\n",
      "794:\tlearn: 0.2043325\ttotal: 14.4s\tremaining: 3.71s\n",
      "795:\tlearn: 0.2043122\ttotal: 14.4s\tremaining: 3.7s\n",
      "796:\tlearn: 0.2043031\ttotal: 14.4s\tremaining: 3.68s\n",
      "797:\tlearn: 0.2042835\ttotal: 14.5s\tremaining: 3.66s\n",
      "798:\tlearn: 0.2042654\ttotal: 14.5s\tremaining: 3.64s\n",
      "799:\tlearn: 0.2042509\ttotal: 14.5s\tremaining: 3.62s\n",
      "800:\tlearn: 0.2042423\ttotal: 14.5s\tremaining: 3.6s\n",
      "801:\tlearn: 0.2042259\ttotal: 14.5s\tremaining: 3.59s\n",
      "802:\tlearn: 0.2042013\ttotal: 14.5s\tremaining: 3.57s\n",
      "803:\tlearn: 0.2041908\ttotal: 14.6s\tremaining: 3.55s\n",
      "804:\tlearn: 0.2041734\ttotal: 14.6s\tremaining: 3.53s\n",
      "805:\tlearn: 0.2041622\ttotal: 14.6s\tremaining: 3.51s\n",
      "806:\tlearn: 0.2041445\ttotal: 14.6s\tremaining: 3.5s\n",
      "807:\tlearn: 0.2041263\ttotal: 14.6s\tremaining: 3.48s\n",
      "808:\tlearn: 0.2041035\ttotal: 14.7s\tremaining: 3.46s\n",
      "809:\tlearn: 0.2040868\ttotal: 14.7s\tremaining: 3.44s\n",
      "810:\tlearn: 0.2040789\ttotal: 14.7s\tremaining: 3.42s\n",
      "811:\tlearn: 0.2040598\ttotal: 14.7s\tremaining: 3.4s\n",
      "812:\tlearn: 0.2040458\ttotal: 14.7s\tremaining: 3.39s\n",
      "813:\tlearn: 0.2040371\ttotal: 14.7s\tremaining: 3.37s\n",
      "814:\tlearn: 0.2040186\ttotal: 14.8s\tremaining: 3.35s\n",
      "815:\tlearn: 0.2040092\ttotal: 14.8s\tremaining: 3.33s\n",
      "816:\tlearn: 0.2039896\ttotal: 14.8s\tremaining: 3.31s\n",
      "817:\tlearn: 0.2039705\ttotal: 14.8s\tremaining: 3.3s\n",
      "818:\tlearn: 0.2039491\ttotal: 14.8s\tremaining: 3.28s\n",
      "819:\tlearn: 0.2039363\ttotal: 14.9s\tremaining: 3.26s\n",
      "820:\tlearn: 0.2039251\ttotal: 14.9s\tremaining: 3.24s\n",
      "821:\tlearn: 0.2039062\ttotal: 14.9s\tremaining: 3.22s\n",
      "822:\tlearn: 0.2038821\ttotal: 14.9s\tremaining: 3.21s\n",
      "823:\tlearn: 0.2038629\ttotal: 14.9s\tremaining: 3.19s\n",
      "824:\tlearn: 0.2038505\ttotal: 14.9s\tremaining: 3.17s\n",
      "825:\tlearn: 0.2038215\ttotal: 15s\tremaining: 3.15s\n",
      "826:\tlearn: 0.2038014\ttotal: 15s\tremaining: 3.13s\n",
      "827:\tlearn: 0.2037932\ttotal: 15s\tremaining: 3.11s\n",
      "828:\tlearn: 0.2037574\ttotal: 15s\tremaining: 3.1s\n",
      "829:\tlearn: 0.2037343\ttotal: 15s\tremaining: 3.08s\n",
      "830:\tlearn: 0.2037176\ttotal: 15.1s\tremaining: 3.06s\n",
      "831:\tlearn: 0.2036784\ttotal: 15.1s\tremaining: 3.04s\n",
      "832:\tlearn: 0.2036544\ttotal: 15.1s\tremaining: 3.02s\n",
      "833:\tlearn: 0.2036448\ttotal: 15.1s\tremaining: 3.01s\n",
      "834:\tlearn: 0.2036092\ttotal: 15.1s\tremaining: 2.99s\n",
      "835:\tlearn: 0.2035720\ttotal: 15.1s\tremaining: 2.97s\n",
      "836:\tlearn: 0.2035409\ttotal: 15.2s\tremaining: 2.95s\n",
      "837:\tlearn: 0.2035254\ttotal: 15.2s\tremaining: 2.93s\n",
      "838:\tlearn: 0.2034912\ttotal: 15.2s\tremaining: 2.92s\n",
      "839:\tlearn: 0.2034780\ttotal: 15.2s\tremaining: 2.9s\n",
      "840:\tlearn: 0.2034597\ttotal: 15.2s\tremaining: 2.88s\n",
      "841:\tlearn: 0.2034271\ttotal: 15.2s\tremaining: 2.86s\n",
      "842:\tlearn: 0.2033926\ttotal: 15.3s\tremaining: 2.84s\n",
      "843:\tlearn: 0.2033783\ttotal: 15.3s\tremaining: 2.82s\n",
      "844:\tlearn: 0.2033487\ttotal: 15.3s\tremaining: 2.81s\n",
      "845:\tlearn: 0.2033174\ttotal: 15.3s\tremaining: 2.79s\n",
      "846:\tlearn: 0.2032979\ttotal: 15.3s\tremaining: 2.77s\n",
      "847:\tlearn: 0.2032762\ttotal: 15.4s\tremaining: 2.75s\n",
      "848:\tlearn: 0.2032595\ttotal: 15.4s\tremaining: 2.73s\n",
      "849:\tlearn: 0.2032449\ttotal: 15.4s\tremaining: 2.71s\n",
      "850:\tlearn: 0.2032248\ttotal: 15.4s\tremaining: 2.7s\n",
      "851:\tlearn: 0.2031961\ttotal: 15.4s\tremaining: 2.68s\n",
      "852:\tlearn: 0.2031750\ttotal: 15.4s\tremaining: 2.66s\n",
      "853:\tlearn: 0.2031688\ttotal: 15.5s\tremaining: 2.64s\n",
      "854:\tlearn: 0.2031510\ttotal: 15.5s\tremaining: 2.62s\n",
      "855:\tlearn: 0.2031341\ttotal: 15.5s\tremaining: 2.61s\n",
      "856:\tlearn: 0.2031026\ttotal: 15.5s\tremaining: 2.59s\n",
      "857:\tlearn: 0.2030787\ttotal: 15.5s\tremaining: 2.57s\n",
      "858:\tlearn: 0.2030497\ttotal: 15.5s\tremaining: 2.55s\n",
      "859:\tlearn: 0.2030154\ttotal: 15.6s\tremaining: 2.53s\n",
      "860:\tlearn: 0.2030055\ttotal: 15.6s\tremaining: 2.52s\n",
      "861:\tlearn: 0.2029712\ttotal: 15.6s\tremaining: 2.5s\n",
      "862:\tlearn: 0.2029527\ttotal: 15.6s\tremaining: 2.48s\n",
      "863:\tlearn: 0.2029231\ttotal: 15.6s\tremaining: 2.46s\n",
      "864:\tlearn: 0.2029001\ttotal: 15.7s\tremaining: 2.44s\n",
      "865:\tlearn: 0.2028873\ttotal: 15.7s\tremaining: 2.42s\n",
      "866:\tlearn: 0.2028688\ttotal: 15.7s\tremaining: 2.41s\n",
      "867:\tlearn: 0.2028578\ttotal: 15.7s\tremaining: 2.39s\n",
      "868:\tlearn: 0.2028319\ttotal: 15.7s\tremaining: 2.37s\n",
      "869:\tlearn: 0.2028215\ttotal: 15.7s\tremaining: 2.35s\n",
      "870:\tlearn: 0.2028048\ttotal: 15.8s\tremaining: 2.33s\n",
      "871:\tlearn: 0.2027934\ttotal: 15.8s\tremaining: 2.32s\n",
      "872:\tlearn: 0.2027721\ttotal: 15.8s\tremaining: 2.3s\n",
      "873:\tlearn: 0.2027436\ttotal: 15.8s\tremaining: 2.28s\n",
      "874:\tlearn: 0.2027198\ttotal: 15.8s\tremaining: 2.26s\n",
      "875:\tlearn: 0.2026878\ttotal: 15.9s\tremaining: 2.24s\n",
      "876:\tlearn: 0.2026786\ttotal: 15.9s\tremaining: 2.23s\n",
      "877:\tlearn: 0.2026627\ttotal: 15.9s\tremaining: 2.21s\n",
      "878:\tlearn: 0.2026416\ttotal: 15.9s\tremaining: 2.19s\n",
      "879:\tlearn: 0.2026168\ttotal: 15.9s\tremaining: 2.17s\n",
      "880:\tlearn: 0.2026064\ttotal: 15.9s\tremaining: 2.15s\n",
      "881:\tlearn: 0.2025997\ttotal: 16s\tremaining: 2.13s\n",
      "882:\tlearn: 0.2025726\ttotal: 16s\tremaining: 2.12s\n",
      "883:\tlearn: 0.2025642\ttotal: 16s\tremaining: 2.1s\n",
      "884:\tlearn: 0.2025324\ttotal: 16s\tremaining: 2.08s\n",
      "885:\tlearn: 0.2025105\ttotal: 16s\tremaining: 2.06s\n",
      "886:\tlearn: 0.2024876\ttotal: 16.1s\tremaining: 2.04s\n",
      "887:\tlearn: 0.2024779\ttotal: 16.1s\tremaining: 2.03s\n",
      "888:\tlearn: 0.2024503\ttotal: 16.1s\tremaining: 2.01s\n",
      "889:\tlearn: 0.2024347\ttotal: 16.1s\tremaining: 1.99s\n",
      "890:\tlearn: 0.2024267\ttotal: 16.1s\tremaining: 1.97s\n",
      "891:\tlearn: 0.2024161\ttotal: 16.1s\tremaining: 1.95s\n",
      "892:\tlearn: 0.2023901\ttotal: 16.2s\tremaining: 1.94s\n",
      "893:\tlearn: 0.2023779\ttotal: 16.2s\tremaining: 1.92s\n",
      "894:\tlearn: 0.2023510\ttotal: 16.2s\tremaining: 1.9s\n",
      "895:\tlearn: 0.2023356\ttotal: 16.2s\tremaining: 1.88s\n",
      "896:\tlearn: 0.2023292\ttotal: 16.2s\tremaining: 1.86s\n",
      "897:\tlearn: 0.2023114\ttotal: 16.2s\tremaining: 1.84s\n",
      "898:\tlearn: 0.2022926\ttotal: 16.3s\tremaining: 1.83s\n",
      "899:\tlearn: 0.2022598\ttotal: 16.3s\tremaining: 1.81s\n",
      "900:\tlearn: 0.2022288\ttotal: 16.3s\tremaining: 1.79s\n",
      "901:\tlearn: 0.2022082\ttotal: 16.3s\tremaining: 1.77s\n",
      "902:\tlearn: 0.2021884\ttotal: 16.3s\tremaining: 1.75s\n",
      "903:\tlearn: 0.2021738\ttotal: 16.4s\tremaining: 1.74s\n",
      "904:\tlearn: 0.2021528\ttotal: 16.4s\tremaining: 1.72s\n",
      "905:\tlearn: 0.2021273\ttotal: 16.4s\tremaining: 1.7s\n",
      "906:\tlearn: 0.2021210\ttotal: 16.4s\tremaining: 1.68s\n",
      "907:\tlearn: 0.2021035\ttotal: 16.4s\tremaining: 1.66s\n",
      "908:\tlearn: 0.2020787\ttotal: 16.4s\tremaining: 1.65s\n",
      "909:\tlearn: 0.2020714\ttotal: 16.5s\tremaining: 1.63s\n",
      "910:\tlearn: 0.2020622\ttotal: 16.5s\tremaining: 1.61s\n",
      "911:\tlearn: 0.2020229\ttotal: 16.5s\tremaining: 1.59s\n",
      "912:\tlearn: 0.2020143\ttotal: 16.5s\tremaining: 1.57s\n",
      "913:\tlearn: 0.2019959\ttotal: 16.5s\tremaining: 1.56s\n",
      "914:\tlearn: 0.2019736\ttotal: 16.6s\tremaining: 1.54s\n",
      "915:\tlearn: 0.2019471\ttotal: 16.6s\tremaining: 1.52s\n",
      "916:\tlearn: 0.2019222\ttotal: 16.6s\tremaining: 1.5s\n",
      "917:\tlearn: 0.2019069\ttotal: 16.6s\tremaining: 1.48s\n",
      "918:\tlearn: 0.2018888\ttotal: 16.6s\tremaining: 1.47s\n",
      "919:\tlearn: 0.2018691\ttotal: 16.6s\tremaining: 1.45s\n",
      "920:\tlearn: 0.2018321\ttotal: 16.7s\tremaining: 1.43s\n",
      "921:\tlearn: 0.2018015\ttotal: 16.7s\tremaining: 1.41s\n",
      "922:\tlearn: 0.2017791\ttotal: 16.7s\tremaining: 1.39s\n",
      "923:\tlearn: 0.2017408\ttotal: 16.7s\tremaining: 1.38s\n",
      "924:\tlearn: 0.2016946\ttotal: 16.7s\tremaining: 1.36s\n",
      "925:\tlearn: 0.2016680\ttotal: 16.8s\tremaining: 1.34s\n",
      "926:\tlearn: 0.2016536\ttotal: 16.8s\tremaining: 1.32s\n",
      "927:\tlearn: 0.2016426\ttotal: 16.8s\tremaining: 1.3s\n",
      "928:\tlearn: 0.2016357\ttotal: 16.8s\tremaining: 1.28s\n",
      "929:\tlearn: 0.2016258\ttotal: 16.8s\tremaining: 1.27s\n",
      "930:\tlearn: 0.2016084\ttotal: 16.8s\tremaining: 1.25s\n",
      "931:\tlearn: 0.2015884\ttotal: 16.9s\tremaining: 1.23s\n",
      "932:\tlearn: 0.2015609\ttotal: 16.9s\tremaining: 1.21s\n",
      "933:\tlearn: 0.2015307\ttotal: 16.9s\tremaining: 1.19s\n",
      "934:\tlearn: 0.2015110\ttotal: 16.9s\tremaining: 1.18s\n",
      "935:\tlearn: 0.2014954\ttotal: 16.9s\tremaining: 1.16s\n",
      "936:\tlearn: 0.2014785\ttotal: 17s\tremaining: 1.14s\n",
      "937:\tlearn: 0.2014699\ttotal: 17s\tremaining: 1.12s\n",
      "938:\tlearn: 0.2014579\ttotal: 17s\tremaining: 1.1s\n",
      "939:\tlearn: 0.2014505\ttotal: 17s\tremaining: 1.08s\n",
      "940:\tlearn: 0.2014339\ttotal: 17s\tremaining: 1.07s\n",
      "941:\tlearn: 0.2014138\ttotal: 17s\tremaining: 1.05s\n",
      "942:\tlearn: 0.2013868\ttotal: 17.1s\tremaining: 1.03s\n",
      "943:\tlearn: 0.2013614\ttotal: 17.1s\tremaining: 1.01s\n",
      "944:\tlearn: 0.2013411\ttotal: 17.1s\tremaining: 995ms\n",
      "945:\tlearn: 0.2013112\ttotal: 17.1s\tremaining: 977ms\n",
      "946:\tlearn: 0.2013049\ttotal: 17.1s\tremaining: 959ms\n",
      "947:\tlearn: 0.2012862\ttotal: 17.1s\tremaining: 941ms\n",
      "948:\tlearn: 0.2012419\ttotal: 17.2s\tremaining: 923ms\n",
      "949:\tlearn: 0.2012360\ttotal: 17.2s\tremaining: 904ms\n",
      "950:\tlearn: 0.2012171\ttotal: 17.2s\tremaining: 886ms\n",
      "951:\tlearn: 0.2011931\ttotal: 17.2s\tremaining: 868ms\n",
      "952:\tlearn: 0.2011649\ttotal: 17.2s\tremaining: 850ms\n",
      "953:\tlearn: 0.2011489\ttotal: 17.3s\tremaining: 832ms\n",
      "954:\tlearn: 0.2011134\ttotal: 17.3s\tremaining: 814ms\n",
      "955:\tlearn: 0.2011059\ttotal: 17.3s\tremaining: 796ms\n",
      "956:\tlearn: 0.2011004\ttotal: 17.3s\tremaining: 778ms\n",
      "957:\tlearn: 0.2010706\ttotal: 17.3s\tremaining: 760ms\n",
      "958:\tlearn: 0.2010311\ttotal: 17.3s\tremaining: 742ms\n",
      "959:\tlearn: 0.2010150\ttotal: 17.4s\tremaining: 724ms\n",
      "960:\tlearn: 0.2009976\ttotal: 17.4s\tremaining: 705ms\n",
      "961:\tlearn: 0.2009607\ttotal: 17.4s\tremaining: 687ms\n",
      "962:\tlearn: 0.2009434\ttotal: 17.4s\tremaining: 669ms\n",
      "963:\tlearn: 0.2009373\ttotal: 17.4s\tremaining: 651ms\n",
      "964:\tlearn: 0.2009114\ttotal: 17.5s\tremaining: 633ms\n",
      "965:\tlearn: 0.2008930\ttotal: 17.5s\tremaining: 615ms\n",
      "966:\tlearn: 0.2008744\ttotal: 17.5s\tremaining: 597ms\n",
      "967:\tlearn: 0.2008343\ttotal: 17.5s\tremaining: 579ms\n",
      "968:\tlearn: 0.2008173\ttotal: 17.5s\tremaining: 561ms\n",
      "969:\tlearn: 0.2007888\ttotal: 17.5s\tremaining: 543ms\n",
      "970:\tlearn: 0.2007800\ttotal: 17.6s\tremaining: 525ms\n",
      "971:\tlearn: 0.2007633\ttotal: 17.6s\tremaining: 506ms\n",
      "972:\tlearn: 0.2007478\ttotal: 17.6s\tremaining: 488ms\n",
      "973:\tlearn: 0.2007220\ttotal: 17.6s\tremaining: 470ms\n",
      "974:\tlearn: 0.2006973\ttotal: 17.6s\tremaining: 452ms\n",
      "975:\tlearn: 0.2006870\ttotal: 17.6s\tremaining: 434ms\n",
      "976:\tlearn: 0.2006596\ttotal: 17.7s\tremaining: 416ms\n",
      "977:\tlearn: 0.2006542\ttotal: 17.7s\tremaining: 398ms\n",
      "978:\tlearn: 0.2006372\ttotal: 17.7s\tremaining: 380ms\n",
      "979:\tlearn: 0.2006088\ttotal: 17.7s\tremaining: 362ms\n",
      "980:\tlearn: 0.2005944\ttotal: 17.7s\tremaining: 344ms\n",
      "981:\tlearn: 0.2005786\ttotal: 17.8s\tremaining: 325ms\n",
      "982:\tlearn: 0.2005614\ttotal: 17.8s\tremaining: 307ms\n",
      "983:\tlearn: 0.2005557\ttotal: 17.8s\tremaining: 289ms\n",
      "984:\tlearn: 0.2005403\ttotal: 17.8s\tremaining: 271ms\n",
      "985:\tlearn: 0.2005185\ttotal: 17.8s\tremaining: 253ms\n",
      "986:\tlearn: 0.2004749\ttotal: 17.8s\tremaining: 235ms\n",
      "987:\tlearn: 0.2004166\ttotal: 17.9s\tremaining: 217ms\n",
      "988:\tlearn: 0.2004014\ttotal: 17.9s\tremaining: 199ms\n",
      "989:\tlearn: 0.2003815\ttotal: 17.9s\tremaining: 181ms\n",
      "990:\tlearn: 0.2003660\ttotal: 17.9s\tremaining: 163ms\n",
      "991:\tlearn: 0.2003410\ttotal: 17.9s\tremaining: 145ms\n",
      "992:\tlearn: 0.2003266\ttotal: 18s\tremaining: 127ms\n",
      "993:\tlearn: 0.2003107\ttotal: 18s\tremaining: 108ms\n",
      "994:\tlearn: 0.2003038\ttotal: 18s\tremaining: 90.4ms\n",
      "995:\tlearn: 0.2002837\ttotal: 18s\tremaining: 72.3ms\n",
      "996:\tlearn: 0.2002514\ttotal: 18s\tremaining: 54.2ms\n",
      "997:\tlearn: 0.2002222\ttotal: 18s\tremaining: 36.2ms\n",
      "998:\tlearn: 0.2002123\ttotal: 18.1s\tremaining: 18.1ms\n",
      "999:\tlearn: 0.2001879\ttotal: 18.1s\tremaining: 0us\n",
      "0:\tlearn: 0.9242952\ttotal: 20.4ms\tremaining: 20.4s\n",
      "1:\tlearn: 0.8573702\ttotal: 38.1ms\tremaining: 19s\n",
      "2:\tlearn: 0.7985435\ttotal: 55.3ms\tremaining: 18.4s\n",
      "3:\tlearn: 0.7458509\ttotal: 71.5ms\tremaining: 17.8s\n",
      "4:\tlearn: 0.6988282\ttotal: 88.2ms\tremaining: 17.6s\n",
      "5:\tlearn: 0.6567040\ttotal: 104ms\tremaining: 17.3s\n",
      "6:\tlearn: 0.6189959\ttotal: 121ms\tremaining: 17.2s\n",
      "7:\tlearn: 0.5850629\ttotal: 138ms\tremaining: 17.1s\n",
      "8:\tlearn: 0.5543873\ttotal: 154ms\tremaining: 17s\n",
      "9:\tlearn: 0.5265202\ttotal: 171ms\tremaining: 16.9s\n",
      "10:\tlearn: 0.5016225\ttotal: 187ms\tremaining: 16.8s\n",
      "11:\tlearn: 0.4791208\ttotal: 203ms\tremaining: 16.8s\n",
      "12:\tlearn: 0.4585242\ttotal: 220ms\tremaining: 16.7s\n",
      "13:\tlearn: 0.4399219\ttotal: 237ms\tremaining: 16.7s\n",
      "14:\tlearn: 0.4226187\ttotal: 254ms\tremaining: 16.7s\n",
      "15:\tlearn: 0.4073797\ttotal: 272ms\tremaining: 16.7s\n",
      "16:\tlearn: 0.3936350\ttotal: 287ms\tremaining: 16.6s\n",
      "17:\tlearn: 0.3810289\ttotal: 303ms\tremaining: 16.6s\n",
      "18:\tlearn: 0.3696362\ttotal: 319ms\tremaining: 16.5s\n",
      "19:\tlearn: 0.3589656\ttotal: 337ms\tremaining: 16.5s\n",
      "20:\tlearn: 0.3493757\ttotal: 354ms\tremaining: 16.5s\n",
      "21:\tlearn: 0.3406028\ttotal: 370ms\tremaining: 16.5s\n",
      "22:\tlearn: 0.3321472\ttotal: 387ms\tremaining: 16.4s\n",
      "23:\tlearn: 0.3248153\ttotal: 403ms\tremaining: 16.4s\n",
      "24:\tlearn: 0.3180490\ttotal: 420ms\tremaining: 16.4s\n",
      "25:\tlearn: 0.3116057\ttotal: 437ms\tremaining: 16.4s\n",
      "26:\tlearn: 0.3058433\ttotal: 454ms\tremaining: 16.4s\n",
      "27:\tlearn: 0.3005965\ttotal: 472ms\tremaining: 16.4s\n",
      "28:\tlearn: 0.2957771\ttotal: 489ms\tremaining: 16.4s\n",
      "29:\tlearn: 0.2910106\ttotal: 507ms\tremaining: 16.4s\n",
      "30:\tlearn: 0.2868326\ttotal: 524ms\tremaining: 16.4s\n",
      "31:\tlearn: 0.2829760\ttotal: 540ms\tremaining: 16.3s\n",
      "32:\tlearn: 0.2792576\ttotal: 558ms\tremaining: 16.3s\n",
      "33:\tlearn: 0.2760109\ttotal: 575ms\tremaining: 16.3s\n",
      "34:\tlearn: 0.2729408\ttotal: 592ms\tremaining: 16.3s\n",
      "35:\tlearn: 0.2700195\ttotal: 609ms\tremaining: 16.3s\n",
      "36:\tlearn: 0.2673967\ttotal: 626ms\tremaining: 16.3s\n",
      "37:\tlearn: 0.2649703\ttotal: 644ms\tremaining: 16.3s\n",
      "38:\tlearn: 0.2627001\ttotal: 662ms\tremaining: 16.3s\n",
      "39:\tlearn: 0.2605382\ttotal: 679ms\tremaining: 16.3s\n",
      "40:\tlearn: 0.2586046\ttotal: 698ms\tremaining: 16.3s\n",
      "41:\tlearn: 0.2565884\ttotal: 715ms\tremaining: 16.3s\n",
      "42:\tlearn: 0.2547681\ttotal: 732ms\tremaining: 16.3s\n",
      "43:\tlearn: 0.2530514\ttotal: 750ms\tremaining: 16.3s\n",
      "44:\tlearn: 0.2515877\ttotal: 766ms\tremaining: 16.3s\n",
      "45:\tlearn: 0.2500205\ttotal: 784ms\tremaining: 16.2s\n",
      "46:\tlearn: 0.2485355\ttotal: 801ms\tremaining: 16.2s\n",
      "47:\tlearn: 0.2472106\ttotal: 818ms\tremaining: 16.2s\n",
      "48:\tlearn: 0.2459132\ttotal: 836ms\tremaining: 16.2s\n",
      "49:\tlearn: 0.2448726\ttotal: 852ms\tremaining: 16.2s\n",
      "50:\tlearn: 0.2438832\ttotal: 869ms\tremaining: 16.2s\n",
      "51:\tlearn: 0.2429013\ttotal: 887ms\tremaining: 16.2s\n",
      "52:\tlearn: 0.2419611\ttotal: 904ms\tremaining: 16.1s\n",
      "53:\tlearn: 0.2410172\ttotal: 922ms\tremaining: 16.1s\n",
      "54:\tlearn: 0.2402052\ttotal: 939ms\tremaining: 16.1s\n",
      "55:\tlearn: 0.2393748\ttotal: 957ms\tremaining: 16.1s\n",
      "56:\tlearn: 0.2387211\ttotal: 975ms\tremaining: 16.1s\n",
      "57:\tlearn: 0.2380925\ttotal: 992ms\tremaining: 16.1s\n",
      "58:\tlearn: 0.2374629\ttotal: 1.01s\tremaining: 16.1s\n",
      "59:\tlearn: 0.2368636\ttotal: 1.02s\tremaining: 16.1s\n",
      "60:\tlearn: 0.2363954\ttotal: 1.04s\tremaining: 16s\n",
      "61:\tlearn: 0.2358031\ttotal: 1.06s\tremaining: 16s\n",
      "62:\tlearn: 0.2353260\ttotal: 1.08s\tremaining: 16s\n",
      "63:\tlearn: 0.2348150\ttotal: 1.09s\tremaining: 16s\n",
      "64:\tlearn: 0.2343832\ttotal: 1.11s\tremaining: 16s\n",
      "65:\tlearn: 0.2339538\ttotal: 1.13s\tremaining: 16s\n",
      "66:\tlearn: 0.2336444\ttotal: 1.15s\tremaining: 16s\n",
      "67:\tlearn: 0.2332926\ttotal: 1.17s\tremaining: 16s\n",
      "68:\tlearn: 0.2328596\ttotal: 1.18s\tremaining: 16s\n",
      "69:\tlearn: 0.2325590\ttotal: 1.2s\tremaining: 15.9s\n",
      "70:\tlearn: 0.2322077\ttotal: 1.22s\tremaining: 15.9s\n",
      "71:\tlearn: 0.2319188\ttotal: 1.23s\tremaining: 15.9s\n",
      "72:\tlearn: 0.2315748\ttotal: 1.25s\tremaining: 15.9s\n",
      "73:\tlearn: 0.2313088\ttotal: 1.27s\tremaining: 15.9s\n",
      "74:\tlearn: 0.2310322\ttotal: 1.28s\tremaining: 15.8s\n",
      "75:\tlearn: 0.2307836\ttotal: 1.3s\tremaining: 15.8s\n",
      "76:\tlearn: 0.2305516\ttotal: 1.32s\tremaining: 15.8s\n",
      "77:\tlearn: 0.2303356\ttotal: 1.34s\tremaining: 15.8s\n",
      "78:\tlearn: 0.2300675\ttotal: 1.35s\tremaining: 15.8s\n",
      "79:\tlearn: 0.2298621\ttotal: 1.37s\tremaining: 15.8s\n",
      "80:\tlearn: 0.2296773\ttotal: 1.39s\tremaining: 15.7s\n",
      "81:\tlearn: 0.2295060\ttotal: 1.4s\tremaining: 15.7s\n",
      "82:\tlearn: 0.2293587\ttotal: 1.42s\tremaining: 15.7s\n",
      "83:\tlearn: 0.2291905\ttotal: 1.44s\tremaining: 15.7s\n",
      "84:\tlearn: 0.2289778\ttotal: 1.46s\tremaining: 15.7s\n",
      "85:\tlearn: 0.2288153\ttotal: 1.47s\tremaining: 15.6s\n",
      "86:\tlearn: 0.2286988\ttotal: 1.49s\tremaining: 15.6s\n",
      "87:\tlearn: 0.2285742\ttotal: 1.5s\tremaining: 15.6s\n",
      "88:\tlearn: 0.2283902\ttotal: 1.52s\tremaining: 15.6s\n",
      "89:\tlearn: 0.2282503\ttotal: 1.54s\tremaining: 15.6s\n",
      "90:\tlearn: 0.2281273\ttotal: 1.56s\tremaining: 15.6s\n",
      "91:\tlearn: 0.2279679\ttotal: 1.57s\tremaining: 15.5s\n",
      "92:\tlearn: 0.2278242\ttotal: 1.59s\tremaining: 15.5s\n",
      "93:\tlearn: 0.2277341\ttotal: 1.61s\tremaining: 15.5s\n",
      "94:\tlearn: 0.2276875\ttotal: 1.62s\tremaining: 15.5s\n",
      "95:\tlearn: 0.2275751\ttotal: 1.64s\tremaining: 15.4s\n",
      "96:\tlearn: 0.2274322\ttotal: 1.66s\tremaining: 15.4s\n",
      "97:\tlearn: 0.2273076\ttotal: 1.68s\tremaining: 15.4s\n",
      "98:\tlearn: 0.2271967\ttotal: 1.69s\tremaining: 15.4s\n",
      "99:\tlearn: 0.2270988\ttotal: 1.71s\tremaining: 15.4s\n",
      "100:\tlearn: 0.2269860\ttotal: 1.73s\tremaining: 15.4s\n",
      "101:\tlearn: 0.2268787\ttotal: 1.75s\tremaining: 15.4s\n",
      "102:\tlearn: 0.2267453\ttotal: 1.76s\tremaining: 15.4s\n",
      "103:\tlearn: 0.2266789\ttotal: 1.78s\tremaining: 15.3s\n",
      "104:\tlearn: 0.2266111\ttotal: 1.8s\tremaining: 15.3s\n",
      "105:\tlearn: 0.2265070\ttotal: 1.81s\tremaining: 15.3s\n",
      "106:\tlearn: 0.2264405\ttotal: 1.83s\tremaining: 15.3s\n",
      "107:\tlearn: 0.2263664\ttotal: 1.85s\tremaining: 15.3s\n",
      "108:\tlearn: 0.2262659\ttotal: 1.87s\tremaining: 15.3s\n",
      "109:\tlearn: 0.2261818\ttotal: 1.88s\tremaining: 15.2s\n",
      "110:\tlearn: 0.2261199\ttotal: 1.9s\tremaining: 15.2s\n",
      "111:\tlearn: 0.2260553\ttotal: 1.92s\tremaining: 15.2s\n",
      "112:\tlearn: 0.2259619\ttotal: 1.93s\tremaining: 15.2s\n",
      "113:\tlearn: 0.2259150\ttotal: 1.95s\tremaining: 15.2s\n",
      "114:\tlearn: 0.2258426\ttotal: 1.97s\tremaining: 15.1s\n",
      "115:\tlearn: 0.2258039\ttotal: 1.99s\tremaining: 15.1s\n",
      "116:\tlearn: 0.2257219\ttotal: 2s\tremaining: 15.1s\n",
      "117:\tlearn: 0.2256288\ttotal: 2.02s\tremaining: 15.1s\n",
      "118:\tlearn: 0.2255859\ttotal: 2.04s\tremaining: 15.1s\n",
      "119:\tlearn: 0.2255236\ttotal: 2.06s\tremaining: 15.1s\n",
      "120:\tlearn: 0.2254662\ttotal: 2.07s\tremaining: 15.1s\n",
      "121:\tlearn: 0.2253902\ttotal: 2.09s\tremaining: 15s\n",
      "122:\tlearn: 0.2253240\ttotal: 2.11s\tremaining: 15s\n",
      "123:\tlearn: 0.2252589\ttotal: 2.12s\tremaining: 15s\n",
      "124:\tlearn: 0.2251901\ttotal: 2.14s\tremaining: 15s\n",
      "125:\tlearn: 0.2251309\ttotal: 2.16s\tremaining: 15s\n",
      "126:\tlearn: 0.2250794\ttotal: 2.18s\tremaining: 15s\n",
      "127:\tlearn: 0.2250286\ttotal: 2.2s\tremaining: 15s\n",
      "128:\tlearn: 0.2249394\ttotal: 2.21s\tremaining: 14.9s\n",
      "129:\tlearn: 0.2248951\ttotal: 2.23s\tremaining: 14.9s\n",
      "130:\tlearn: 0.2248634\ttotal: 2.25s\tremaining: 14.9s\n",
      "131:\tlearn: 0.2248117\ttotal: 2.26s\tremaining: 14.9s\n",
      "132:\tlearn: 0.2247714\ttotal: 2.28s\tremaining: 14.9s\n",
      "133:\tlearn: 0.2247106\ttotal: 2.3s\tremaining: 14.9s\n",
      "134:\tlearn: 0.2246320\ttotal: 2.32s\tremaining: 14.8s\n",
      "135:\tlearn: 0.2245809\ttotal: 2.33s\tremaining: 14.8s\n",
      "136:\tlearn: 0.2245505\ttotal: 2.35s\tremaining: 14.8s\n",
      "137:\tlearn: 0.2244942\ttotal: 2.37s\tremaining: 14.8s\n",
      "138:\tlearn: 0.2244548\ttotal: 2.38s\tremaining: 14.8s\n",
      "139:\tlearn: 0.2244006\ttotal: 2.4s\tremaining: 14.8s\n",
      "140:\tlearn: 0.2243333\ttotal: 2.42s\tremaining: 14.7s\n",
      "141:\tlearn: 0.2242738\ttotal: 2.44s\tremaining: 14.7s\n",
      "142:\tlearn: 0.2242025\ttotal: 2.45s\tremaining: 14.7s\n",
      "143:\tlearn: 0.2241337\ttotal: 2.47s\tremaining: 14.7s\n",
      "144:\tlearn: 0.2240838\ttotal: 2.49s\tremaining: 14.7s\n",
      "145:\tlearn: 0.2240295\ttotal: 2.5s\tremaining: 14.7s\n",
      "146:\tlearn: 0.2239594\ttotal: 2.52s\tremaining: 14.6s\n",
      "147:\tlearn: 0.2239205\ttotal: 2.54s\tremaining: 14.6s\n",
      "148:\tlearn: 0.2238657\ttotal: 2.56s\tremaining: 14.6s\n",
      "149:\tlearn: 0.2238285\ttotal: 2.57s\tremaining: 14.6s\n",
      "150:\tlearn: 0.2237937\ttotal: 2.59s\tremaining: 14.6s\n",
      "151:\tlearn: 0.2237377\ttotal: 2.61s\tremaining: 14.5s\n",
      "152:\tlearn: 0.2236922\ttotal: 2.63s\tremaining: 14.5s\n",
      "153:\tlearn: 0.2236643\ttotal: 2.65s\tremaining: 14.5s\n",
      "154:\tlearn: 0.2236221\ttotal: 2.66s\tremaining: 14.5s\n",
      "155:\tlearn: 0.2235920\ttotal: 2.68s\tremaining: 14.5s\n",
      "156:\tlearn: 0.2235254\ttotal: 2.7s\tremaining: 14.5s\n",
      "157:\tlearn: 0.2234879\ttotal: 2.71s\tremaining: 14.5s\n",
      "158:\tlearn: 0.2234506\ttotal: 2.73s\tremaining: 14.4s\n",
      "159:\tlearn: 0.2233921\ttotal: 2.75s\tremaining: 14.4s\n",
      "160:\tlearn: 0.2233514\ttotal: 2.77s\tremaining: 14.4s\n",
      "161:\tlearn: 0.2232851\ttotal: 2.78s\tremaining: 14.4s\n",
      "162:\tlearn: 0.2232443\ttotal: 2.8s\tremaining: 14.4s\n",
      "163:\tlearn: 0.2232057\ttotal: 2.81s\tremaining: 14.4s\n",
      "164:\tlearn: 0.2231849\ttotal: 2.83s\tremaining: 14.3s\n",
      "165:\tlearn: 0.2231050\ttotal: 2.85s\tremaining: 14.3s\n",
      "166:\tlearn: 0.2230659\ttotal: 2.87s\tremaining: 14.3s\n",
      "167:\tlearn: 0.2230457\ttotal: 2.89s\tremaining: 14.3s\n",
      "168:\tlearn: 0.2230049\ttotal: 2.9s\tremaining: 14.3s\n",
      "169:\tlearn: 0.2229584\ttotal: 2.92s\tremaining: 14.3s\n",
      "170:\tlearn: 0.2229250\ttotal: 2.94s\tremaining: 14.2s\n",
      "171:\tlearn: 0.2228891\ttotal: 2.95s\tremaining: 14.2s\n",
      "172:\tlearn: 0.2228224\ttotal: 2.97s\tremaining: 14.2s\n",
      "173:\tlearn: 0.2227849\ttotal: 2.99s\tremaining: 14.2s\n",
      "174:\tlearn: 0.2227387\ttotal: 3s\tremaining: 14.2s\n",
      "175:\tlearn: 0.2226919\ttotal: 3.02s\tremaining: 14.1s\n",
      "176:\tlearn: 0.2226474\ttotal: 3.04s\tremaining: 14.1s\n",
      "177:\tlearn: 0.2225957\ttotal: 3.06s\tremaining: 14.1s\n",
      "178:\tlearn: 0.2225374\ttotal: 3.08s\tremaining: 14.1s\n",
      "179:\tlearn: 0.2224768\ttotal: 3.09s\tremaining: 14.1s\n",
      "180:\tlearn: 0.2224385\ttotal: 3.11s\tremaining: 14.1s\n",
      "181:\tlearn: 0.2223906\ttotal: 3.13s\tremaining: 14.1s\n",
      "182:\tlearn: 0.2223343\ttotal: 3.15s\tremaining: 14s\n",
      "183:\tlearn: 0.2222990\ttotal: 3.16s\tremaining: 14s\n",
      "184:\tlearn: 0.2222629\ttotal: 3.18s\tremaining: 14s\n",
      "185:\tlearn: 0.2222126\ttotal: 3.2s\tremaining: 14s\n",
      "186:\tlearn: 0.2221900\ttotal: 3.21s\tremaining: 14s\n",
      "187:\tlearn: 0.2221476\ttotal: 3.23s\tremaining: 14s\n",
      "188:\tlearn: 0.2221216\ttotal: 3.25s\tremaining: 13.9s\n",
      "189:\tlearn: 0.2220789\ttotal: 3.27s\tremaining: 13.9s\n",
      "190:\tlearn: 0.2220210\ttotal: 3.28s\tremaining: 13.9s\n",
      "191:\tlearn: 0.2219763\ttotal: 3.3s\tremaining: 13.9s\n",
      "192:\tlearn: 0.2219456\ttotal: 3.32s\tremaining: 13.9s\n",
      "193:\tlearn: 0.2218963\ttotal: 3.34s\tremaining: 13.9s\n",
      "194:\tlearn: 0.2218508\ttotal: 3.35s\tremaining: 13.8s\n",
      "195:\tlearn: 0.2218124\ttotal: 3.37s\tremaining: 13.8s\n",
      "196:\tlearn: 0.2217734\ttotal: 3.39s\tremaining: 13.8s\n",
      "197:\tlearn: 0.2217517\ttotal: 3.4s\tremaining: 13.8s\n",
      "198:\tlearn: 0.2217180\ttotal: 3.42s\tremaining: 13.8s\n",
      "199:\tlearn: 0.2216819\ttotal: 3.44s\tremaining: 13.8s\n",
      "200:\tlearn: 0.2216553\ttotal: 3.46s\tremaining: 13.7s\n",
      "201:\tlearn: 0.2216381\ttotal: 3.47s\tremaining: 13.7s\n",
      "202:\tlearn: 0.2215935\ttotal: 3.49s\tremaining: 13.7s\n",
      "203:\tlearn: 0.2215353\ttotal: 3.51s\tremaining: 13.7s\n",
      "204:\tlearn: 0.2215127\ttotal: 3.53s\tremaining: 13.7s\n",
      "205:\tlearn: 0.2214754\ttotal: 3.54s\tremaining: 13.7s\n",
      "206:\tlearn: 0.2214493\ttotal: 3.56s\tremaining: 13.6s\n",
      "207:\tlearn: 0.2214013\ttotal: 3.58s\tremaining: 13.6s\n",
      "208:\tlearn: 0.2213625\ttotal: 3.6s\tremaining: 13.6s\n",
      "209:\tlearn: 0.2213258\ttotal: 3.61s\tremaining: 13.6s\n",
      "210:\tlearn: 0.2212905\ttotal: 3.63s\tremaining: 13.6s\n",
      "211:\tlearn: 0.2212382\ttotal: 3.65s\tremaining: 13.6s\n",
      "212:\tlearn: 0.2212134\ttotal: 3.66s\tremaining: 13.5s\n",
      "213:\tlearn: 0.2211650\ttotal: 3.68s\tremaining: 13.5s\n",
      "214:\tlearn: 0.2211427\ttotal: 3.7s\tremaining: 13.5s\n",
      "215:\tlearn: 0.2211033\ttotal: 3.72s\tremaining: 13.5s\n",
      "216:\tlearn: 0.2210753\ttotal: 3.74s\tremaining: 13.5s\n",
      "217:\tlearn: 0.2210432\ttotal: 3.76s\tremaining: 13.5s\n",
      "218:\tlearn: 0.2210100\ttotal: 3.77s\tremaining: 13.5s\n",
      "219:\tlearn: 0.2209724\ttotal: 3.79s\tremaining: 13.4s\n",
      "220:\tlearn: 0.2209279\ttotal: 3.81s\tremaining: 13.4s\n",
      "221:\tlearn: 0.2209119\ttotal: 3.83s\tremaining: 13.4s\n",
      "222:\tlearn: 0.2208748\ttotal: 3.84s\tremaining: 13.4s\n",
      "223:\tlearn: 0.2208315\ttotal: 3.86s\tremaining: 13.4s\n",
      "224:\tlearn: 0.2207927\ttotal: 3.88s\tremaining: 13.4s\n",
      "225:\tlearn: 0.2207207\ttotal: 3.9s\tremaining: 13.3s\n",
      "226:\tlearn: 0.2206895\ttotal: 3.91s\tremaining: 13.3s\n",
      "227:\tlearn: 0.2206734\ttotal: 3.93s\tremaining: 13.3s\n",
      "228:\tlearn: 0.2206316\ttotal: 3.95s\tremaining: 13.3s\n",
      "229:\tlearn: 0.2206063\ttotal: 3.97s\tremaining: 13.3s\n",
      "230:\tlearn: 0.2205515\ttotal: 3.98s\tremaining: 13.3s\n",
      "231:\tlearn: 0.2205287\ttotal: 4s\tremaining: 13.3s\n",
      "232:\tlearn: 0.2204883\ttotal: 4.02s\tremaining: 13.2s\n",
      "233:\tlearn: 0.2204588\ttotal: 4.04s\tremaining: 13.2s\n",
      "234:\tlearn: 0.2204133\ttotal: 4.05s\tremaining: 13.2s\n",
      "235:\tlearn: 0.2203835\ttotal: 4.07s\tremaining: 13.2s\n",
      "236:\tlearn: 0.2203583\ttotal: 4.09s\tremaining: 13.2s\n",
      "237:\tlearn: 0.2203095\ttotal: 4.1s\tremaining: 13.1s\n",
      "238:\tlearn: 0.2202861\ttotal: 4.12s\tremaining: 13.1s\n",
      "239:\tlearn: 0.2202678\ttotal: 4.14s\tremaining: 13.1s\n",
      "240:\tlearn: 0.2202407\ttotal: 4.16s\tremaining: 13.1s\n",
      "241:\tlearn: 0.2201691\ttotal: 4.18s\tremaining: 13.1s\n",
      "242:\tlearn: 0.2201153\ttotal: 4.2s\tremaining: 13.1s\n",
      "243:\tlearn: 0.2200664\ttotal: 4.21s\tremaining: 13.1s\n",
      "244:\tlearn: 0.2200325\ttotal: 4.23s\tremaining: 13s\n",
      "245:\tlearn: 0.2199662\ttotal: 4.25s\tremaining: 13s\n",
      "246:\tlearn: 0.2199277\ttotal: 4.27s\tremaining: 13s\n",
      "247:\tlearn: 0.2199129\ttotal: 4.28s\tremaining: 13s\n",
      "248:\tlearn: 0.2198825\ttotal: 4.3s\tremaining: 13s\n",
      "249:\tlearn: 0.2198510\ttotal: 4.32s\tremaining: 13s\n",
      "250:\tlearn: 0.2198025\ttotal: 4.33s\tremaining: 12.9s\n",
      "251:\tlearn: 0.2197803\ttotal: 4.35s\tremaining: 12.9s\n",
      "252:\tlearn: 0.2197540\ttotal: 4.37s\tremaining: 12.9s\n",
      "253:\tlearn: 0.2197219\ttotal: 4.39s\tremaining: 12.9s\n",
      "254:\tlearn: 0.2196871\ttotal: 4.4s\tremaining: 12.9s\n",
      "255:\tlearn: 0.2196546\ttotal: 4.42s\tremaining: 12.8s\n",
      "256:\tlearn: 0.2196223\ttotal: 4.44s\tremaining: 12.8s\n",
      "257:\tlearn: 0.2195814\ttotal: 4.46s\tremaining: 12.8s\n",
      "258:\tlearn: 0.2195571\ttotal: 4.47s\tremaining: 12.8s\n",
      "259:\tlearn: 0.2195299\ttotal: 4.49s\tremaining: 12.8s\n",
      "260:\tlearn: 0.2194895\ttotal: 4.51s\tremaining: 12.8s\n",
      "261:\tlearn: 0.2194679\ttotal: 4.52s\tremaining: 12.7s\n",
      "262:\tlearn: 0.2194442\ttotal: 4.54s\tremaining: 12.7s\n",
      "263:\tlearn: 0.2194226\ttotal: 4.56s\tremaining: 12.7s\n",
      "264:\tlearn: 0.2193999\ttotal: 4.58s\tremaining: 12.7s\n",
      "265:\tlearn: 0.2193452\ttotal: 4.59s\tremaining: 12.7s\n",
      "266:\tlearn: 0.2193151\ttotal: 4.61s\tremaining: 12.7s\n",
      "267:\tlearn: 0.2192817\ttotal: 4.63s\tremaining: 12.6s\n",
      "268:\tlearn: 0.2192360\ttotal: 4.64s\tremaining: 12.6s\n",
      "269:\tlearn: 0.2191824\ttotal: 4.66s\tremaining: 12.6s\n",
      "270:\tlearn: 0.2191258\ttotal: 4.68s\tremaining: 12.6s\n",
      "271:\tlearn: 0.2191031\ttotal: 4.7s\tremaining: 12.6s\n",
      "272:\tlearn: 0.2190652\ttotal: 4.71s\tremaining: 12.6s\n",
      "273:\tlearn: 0.2190215\ttotal: 4.73s\tremaining: 12.5s\n",
      "274:\tlearn: 0.2189839\ttotal: 4.75s\tremaining: 12.5s\n",
      "275:\tlearn: 0.2189594\ttotal: 4.77s\tremaining: 12.5s\n",
      "276:\tlearn: 0.2189355\ttotal: 4.78s\tremaining: 12.5s\n",
      "277:\tlearn: 0.2188975\ttotal: 4.8s\tremaining: 12.5s\n",
      "278:\tlearn: 0.2188475\ttotal: 4.82s\tremaining: 12.5s\n",
      "279:\tlearn: 0.2188152\ttotal: 4.83s\tremaining: 12.4s\n",
      "280:\tlearn: 0.2187807\ttotal: 4.85s\tremaining: 12.4s\n",
      "281:\tlearn: 0.2187426\ttotal: 4.87s\tremaining: 12.4s\n",
      "282:\tlearn: 0.2187222\ttotal: 4.89s\tremaining: 12.4s\n",
      "283:\tlearn: 0.2186719\ttotal: 4.9s\tremaining: 12.4s\n",
      "284:\tlearn: 0.2186478\ttotal: 4.92s\tremaining: 12.3s\n",
      "285:\tlearn: 0.2186277\ttotal: 4.94s\tremaining: 12.3s\n",
      "286:\tlearn: 0.2185727\ttotal: 4.96s\tremaining: 12.3s\n",
      "287:\tlearn: 0.2185263\ttotal: 4.97s\tremaining: 12.3s\n",
      "288:\tlearn: 0.2185036\ttotal: 4.99s\tremaining: 12.3s\n",
      "289:\tlearn: 0.2184514\ttotal: 5.01s\tremaining: 12.3s\n",
      "290:\tlearn: 0.2184296\ttotal: 5.03s\tremaining: 12.2s\n",
      "291:\tlearn: 0.2183967\ttotal: 5.04s\tremaining: 12.2s\n",
      "292:\tlearn: 0.2183544\ttotal: 5.06s\tremaining: 12.2s\n",
      "293:\tlearn: 0.2183306\ttotal: 5.08s\tremaining: 12.2s\n",
      "294:\tlearn: 0.2183072\ttotal: 5.1s\tremaining: 12.2s\n",
      "295:\tlearn: 0.2182461\ttotal: 5.12s\tremaining: 12.2s\n",
      "296:\tlearn: 0.2182267\ttotal: 5.13s\tremaining: 12.1s\n",
      "297:\tlearn: 0.2181850\ttotal: 5.15s\tremaining: 12.1s\n",
      "298:\tlearn: 0.2181630\ttotal: 5.17s\tremaining: 12.1s\n",
      "299:\tlearn: 0.2181241\ttotal: 5.18s\tremaining: 12.1s\n",
      "300:\tlearn: 0.2180639\ttotal: 5.2s\tremaining: 12.1s\n",
      "301:\tlearn: 0.2180286\ttotal: 5.22s\tremaining: 12.1s\n",
      "302:\tlearn: 0.2179998\ttotal: 5.24s\tremaining: 12.1s\n",
      "303:\tlearn: 0.2179696\ttotal: 5.26s\tremaining: 12s\n",
      "304:\tlearn: 0.2179317\ttotal: 5.28s\tremaining: 12s\n",
      "305:\tlearn: 0.2179177\ttotal: 5.29s\tremaining: 12s\n",
      "306:\tlearn: 0.2178954\ttotal: 5.31s\tremaining: 12s\n",
      "307:\tlearn: 0.2178569\ttotal: 5.33s\tremaining: 12s\n",
      "308:\tlearn: 0.2178042\ttotal: 5.34s\tremaining: 12s\n",
      "309:\tlearn: 0.2177712\ttotal: 5.36s\tremaining: 11.9s\n",
      "310:\tlearn: 0.2177420\ttotal: 5.38s\tremaining: 11.9s\n",
      "311:\tlearn: 0.2177042\ttotal: 5.4s\tremaining: 11.9s\n",
      "312:\tlearn: 0.2176656\ttotal: 5.41s\tremaining: 11.9s\n",
      "313:\tlearn: 0.2176332\ttotal: 5.43s\tremaining: 11.9s\n",
      "314:\tlearn: 0.2176090\ttotal: 5.45s\tremaining: 11.9s\n",
      "315:\tlearn: 0.2175909\ttotal: 5.47s\tremaining: 11.8s\n",
      "316:\tlearn: 0.2175584\ttotal: 5.48s\tremaining: 11.8s\n",
      "317:\tlearn: 0.2175326\ttotal: 5.5s\tremaining: 11.8s\n",
      "318:\tlearn: 0.2174993\ttotal: 5.52s\tremaining: 11.8s\n",
      "319:\tlearn: 0.2174811\ttotal: 5.54s\tremaining: 11.8s\n",
      "320:\tlearn: 0.2174537\ttotal: 5.55s\tremaining: 11.7s\n",
      "321:\tlearn: 0.2173856\ttotal: 5.57s\tremaining: 11.7s\n",
      "322:\tlearn: 0.2173716\ttotal: 5.59s\tremaining: 11.7s\n",
      "323:\tlearn: 0.2173420\ttotal: 5.6s\tremaining: 11.7s\n",
      "324:\tlearn: 0.2173068\ttotal: 5.62s\tremaining: 11.7s\n",
      "325:\tlearn: 0.2172808\ttotal: 5.64s\tremaining: 11.7s\n",
      "326:\tlearn: 0.2172555\ttotal: 5.66s\tremaining: 11.6s\n",
      "327:\tlearn: 0.2172355\ttotal: 5.67s\tremaining: 11.6s\n",
      "328:\tlearn: 0.2171953\ttotal: 5.69s\tremaining: 11.6s\n",
      "329:\tlearn: 0.2171646\ttotal: 5.71s\tremaining: 11.6s\n",
      "330:\tlearn: 0.2171319\ttotal: 5.73s\tremaining: 11.6s\n",
      "331:\tlearn: 0.2170979\ttotal: 5.75s\tremaining: 11.6s\n",
      "332:\tlearn: 0.2170591\ttotal: 5.76s\tremaining: 11.5s\n",
      "333:\tlearn: 0.2170293\ttotal: 5.78s\tremaining: 11.5s\n",
      "334:\tlearn: 0.2170062\ttotal: 5.8s\tremaining: 11.5s\n",
      "335:\tlearn: 0.2169811\ttotal: 5.82s\tremaining: 11.5s\n",
      "336:\tlearn: 0.2169569\ttotal: 5.83s\tremaining: 11.5s\n",
      "337:\tlearn: 0.2169257\ttotal: 5.85s\tremaining: 11.5s\n",
      "338:\tlearn: 0.2169110\ttotal: 5.87s\tremaining: 11.4s\n",
      "339:\tlearn: 0.2168761\ttotal: 5.88s\tremaining: 11.4s\n",
      "340:\tlearn: 0.2168553\ttotal: 5.9s\tremaining: 11.4s\n",
      "341:\tlearn: 0.2168178\ttotal: 5.92s\tremaining: 11.4s\n",
      "342:\tlearn: 0.2167793\ttotal: 5.94s\tremaining: 11.4s\n",
      "343:\tlearn: 0.2167528\ttotal: 5.95s\tremaining: 11.4s\n",
      "344:\tlearn: 0.2166995\ttotal: 5.97s\tremaining: 11.3s\n",
      "345:\tlearn: 0.2166617\ttotal: 5.99s\tremaining: 11.3s\n",
      "346:\tlearn: 0.2166297\ttotal: 6s\tremaining: 11.3s\n",
      "347:\tlearn: 0.2166061\ttotal: 6.02s\tremaining: 11.3s\n",
      "348:\tlearn: 0.2165770\ttotal: 6.04s\tremaining: 11.3s\n",
      "349:\tlearn: 0.2165380\ttotal: 6.06s\tremaining: 11.2s\n",
      "350:\tlearn: 0.2165098\ttotal: 6.07s\tremaining: 11.2s\n",
      "351:\tlearn: 0.2164852\ttotal: 6.09s\tremaining: 11.2s\n",
      "352:\tlearn: 0.2164543\ttotal: 6.11s\tremaining: 11.2s\n",
      "353:\tlearn: 0.2164043\ttotal: 6.13s\tremaining: 11.2s\n",
      "354:\tlearn: 0.2163641\ttotal: 6.14s\tremaining: 11.2s\n",
      "355:\tlearn: 0.2163364\ttotal: 6.16s\tremaining: 11.1s\n",
      "356:\tlearn: 0.2163194\ttotal: 6.18s\tremaining: 11.1s\n",
      "357:\tlearn: 0.2163014\ttotal: 6.2s\tremaining: 11.1s\n",
      "358:\tlearn: 0.2162625\ttotal: 6.22s\tremaining: 11.1s\n",
      "359:\tlearn: 0.2162378\ttotal: 6.24s\tremaining: 11.1s\n",
      "360:\tlearn: 0.2162179\ttotal: 6.25s\tremaining: 11.1s\n",
      "361:\tlearn: 0.2161784\ttotal: 6.27s\tremaining: 11.1s\n",
      "362:\tlearn: 0.2161512\ttotal: 6.29s\tremaining: 11s\n",
      "363:\tlearn: 0.2161294\ttotal: 6.31s\tremaining: 11s\n",
      "364:\tlearn: 0.2161110\ttotal: 6.33s\tremaining: 11s\n",
      "365:\tlearn: 0.2160747\ttotal: 6.34s\tremaining: 11s\n",
      "366:\tlearn: 0.2160425\ttotal: 6.36s\tremaining: 11s\n",
      "367:\tlearn: 0.2160228\ttotal: 6.38s\tremaining: 10.9s\n",
      "368:\tlearn: 0.2159825\ttotal: 6.39s\tremaining: 10.9s\n",
      "369:\tlearn: 0.2159554\ttotal: 6.41s\tremaining: 10.9s\n",
      "370:\tlearn: 0.2159177\ttotal: 6.43s\tremaining: 10.9s\n",
      "371:\tlearn: 0.2159021\ttotal: 6.45s\tremaining: 10.9s\n",
      "372:\tlearn: 0.2158579\ttotal: 6.46s\tremaining: 10.9s\n",
      "373:\tlearn: 0.2158405\ttotal: 6.48s\tremaining: 10.8s\n",
      "374:\tlearn: 0.2158027\ttotal: 6.5s\tremaining: 10.8s\n",
      "375:\tlearn: 0.2157689\ttotal: 6.51s\tremaining: 10.8s\n",
      "376:\tlearn: 0.2157413\ttotal: 6.53s\tremaining: 10.8s\n",
      "377:\tlearn: 0.2157198\ttotal: 6.55s\tremaining: 10.8s\n",
      "378:\tlearn: 0.2156940\ttotal: 6.57s\tremaining: 10.8s\n",
      "379:\tlearn: 0.2156727\ttotal: 6.58s\tremaining: 10.7s\n",
      "380:\tlearn: 0.2156541\ttotal: 6.6s\tremaining: 10.7s\n",
      "381:\tlearn: 0.2156233\ttotal: 6.62s\tremaining: 10.7s\n",
      "382:\tlearn: 0.2155759\ttotal: 6.64s\tremaining: 10.7s\n",
      "383:\tlearn: 0.2155574\ttotal: 6.65s\tremaining: 10.7s\n",
      "384:\tlearn: 0.2155184\ttotal: 6.67s\tremaining: 10.7s\n",
      "385:\tlearn: 0.2154928\ttotal: 6.69s\tremaining: 10.6s\n",
      "386:\tlearn: 0.2154799\ttotal: 6.71s\tremaining: 10.6s\n",
      "387:\tlearn: 0.2154415\ttotal: 6.73s\tremaining: 10.6s\n",
      "388:\tlearn: 0.2154020\ttotal: 6.74s\tremaining: 10.6s\n",
      "389:\tlearn: 0.2153639\ttotal: 6.76s\tremaining: 10.6s\n",
      "390:\tlearn: 0.2153306\ttotal: 6.78s\tremaining: 10.6s\n",
      "391:\tlearn: 0.2153029\ttotal: 6.79s\tremaining: 10.5s\n",
      "392:\tlearn: 0.2152750\ttotal: 6.81s\tremaining: 10.5s\n",
      "393:\tlearn: 0.2152431\ttotal: 6.83s\tremaining: 10.5s\n",
      "394:\tlearn: 0.2152305\ttotal: 6.85s\tremaining: 10.5s\n",
      "395:\tlearn: 0.2152128\ttotal: 6.87s\tremaining: 10.5s\n",
      "396:\tlearn: 0.2151989\ttotal: 6.88s\tremaining: 10.5s\n",
      "397:\tlearn: 0.2151653\ttotal: 6.9s\tremaining: 10.4s\n",
      "398:\tlearn: 0.2151451\ttotal: 6.92s\tremaining: 10.4s\n",
      "399:\tlearn: 0.2150976\ttotal: 6.93s\tremaining: 10.4s\n",
      "400:\tlearn: 0.2150511\ttotal: 6.95s\tremaining: 10.4s\n",
      "401:\tlearn: 0.2150190\ttotal: 6.97s\tremaining: 10.4s\n",
      "402:\tlearn: 0.2149858\ttotal: 6.99s\tremaining: 10.3s\n",
      "403:\tlearn: 0.2149470\ttotal: 7s\tremaining: 10.3s\n",
      "404:\tlearn: 0.2149244\ttotal: 7.02s\tremaining: 10.3s\n",
      "405:\tlearn: 0.2149007\ttotal: 7.04s\tremaining: 10.3s\n",
      "406:\tlearn: 0.2148747\ttotal: 7.05s\tremaining: 10.3s\n",
      "407:\tlearn: 0.2148588\ttotal: 7.07s\tremaining: 10.3s\n",
      "408:\tlearn: 0.2148291\ttotal: 7.09s\tremaining: 10.2s\n",
      "409:\tlearn: 0.2148104\ttotal: 7.11s\tremaining: 10.2s\n",
      "410:\tlearn: 0.2147836\ttotal: 7.12s\tremaining: 10.2s\n",
      "411:\tlearn: 0.2147576\ttotal: 7.14s\tremaining: 10.2s\n",
      "412:\tlearn: 0.2147208\ttotal: 7.16s\tremaining: 10.2s\n",
      "413:\tlearn: 0.2146918\ttotal: 7.18s\tremaining: 10.2s\n",
      "414:\tlearn: 0.2146553\ttotal: 7.2s\tremaining: 10.1s\n",
      "415:\tlearn: 0.2146248\ttotal: 7.21s\tremaining: 10.1s\n",
      "416:\tlearn: 0.2146060\ttotal: 7.23s\tremaining: 10.1s\n",
      "417:\tlearn: 0.2145890\ttotal: 7.25s\tremaining: 10.1s\n",
      "418:\tlearn: 0.2145732\ttotal: 7.26s\tremaining: 10.1s\n",
      "419:\tlearn: 0.2145448\ttotal: 7.28s\tremaining: 10.1s\n",
      "420:\tlearn: 0.2145232\ttotal: 7.3s\tremaining: 10s\n",
      "421:\tlearn: 0.2145045\ttotal: 7.32s\tremaining: 10s\n",
      "422:\tlearn: 0.2144824\ttotal: 7.33s\tremaining: 10s\n",
      "423:\tlearn: 0.2144528\ttotal: 7.35s\tremaining: 9.99s\n",
      "424:\tlearn: 0.2144411\ttotal: 7.37s\tremaining: 9.97s\n",
      "425:\tlearn: 0.2143953\ttotal: 7.39s\tremaining: 9.95s\n",
      "426:\tlearn: 0.2143817\ttotal: 7.4s\tremaining: 9.94s\n",
      "427:\tlearn: 0.2143464\ttotal: 7.42s\tremaining: 9.92s\n",
      "428:\tlearn: 0.2143165\ttotal: 7.44s\tremaining: 9.9s\n",
      "429:\tlearn: 0.2142674\ttotal: 7.46s\tremaining: 9.88s\n",
      "430:\tlearn: 0.2142405\ttotal: 7.47s\tremaining: 9.87s\n",
      "431:\tlearn: 0.2141913\ttotal: 7.49s\tremaining: 9.85s\n",
      "432:\tlearn: 0.2141556\ttotal: 7.51s\tremaining: 9.83s\n",
      "433:\tlearn: 0.2141237\ttotal: 7.54s\tremaining: 9.83s\n",
      "434:\tlearn: 0.2141046\ttotal: 7.56s\tremaining: 9.81s\n",
      "435:\tlearn: 0.2140735\ttotal: 7.57s\tremaining: 9.8s\n",
      "436:\tlearn: 0.2140620\ttotal: 7.59s\tremaining: 9.78s\n",
      "437:\tlearn: 0.2140379\ttotal: 7.61s\tremaining: 9.76s\n",
      "438:\tlearn: 0.2140158\ttotal: 7.63s\tremaining: 9.75s\n",
      "439:\tlearn: 0.2139969\ttotal: 7.64s\tremaining: 9.73s\n",
      "440:\tlearn: 0.2139576\ttotal: 7.66s\tremaining: 9.71s\n",
      "441:\tlearn: 0.2139395\ttotal: 7.68s\tremaining: 9.69s\n",
      "442:\tlearn: 0.2138973\ttotal: 7.7s\tremaining: 9.68s\n",
      "443:\tlearn: 0.2138730\ttotal: 7.71s\tremaining: 9.66s\n",
      "444:\tlearn: 0.2138569\ttotal: 7.73s\tremaining: 9.64s\n",
      "445:\tlearn: 0.2138294\ttotal: 7.75s\tremaining: 9.62s\n",
      "446:\tlearn: 0.2138074\ttotal: 7.76s\tremaining: 9.61s\n",
      "447:\tlearn: 0.2137855\ttotal: 7.78s\tremaining: 9.59s\n",
      "448:\tlearn: 0.2137540\ttotal: 7.8s\tremaining: 9.57s\n",
      "449:\tlearn: 0.2137188\ttotal: 7.82s\tremaining: 9.56s\n",
      "450:\tlearn: 0.2136919\ttotal: 7.84s\tremaining: 9.54s\n",
      "451:\tlearn: 0.2136609\ttotal: 7.85s\tremaining: 9.52s\n",
      "452:\tlearn: 0.2136364\ttotal: 7.87s\tremaining: 9.5s\n",
      "453:\tlearn: 0.2136146\ttotal: 7.89s\tremaining: 9.48s\n",
      "454:\tlearn: 0.2135824\ttotal: 7.9s\tremaining: 9.47s\n",
      "455:\tlearn: 0.2135529\ttotal: 7.92s\tremaining: 9.45s\n",
      "456:\tlearn: 0.2135276\ttotal: 7.94s\tremaining: 9.43s\n",
      "457:\tlearn: 0.2135129\ttotal: 7.96s\tremaining: 9.41s\n",
      "458:\tlearn: 0.2134986\ttotal: 7.97s\tremaining: 9.4s\n",
      "459:\tlearn: 0.2134682\ttotal: 7.99s\tremaining: 9.38s\n",
      "460:\tlearn: 0.2134384\ttotal: 8.01s\tremaining: 9.36s\n",
      "461:\tlearn: 0.2134031\ttotal: 8.03s\tremaining: 9.35s\n",
      "462:\tlearn: 0.2133829\ttotal: 8.04s\tremaining: 9.33s\n",
      "463:\tlearn: 0.2133634\ttotal: 8.06s\tremaining: 9.31s\n",
      "464:\tlearn: 0.2133450\ttotal: 8.08s\tremaining: 9.29s\n",
      "465:\tlearn: 0.2133146\ttotal: 8.1s\tremaining: 9.28s\n",
      "466:\tlearn: 0.2132855\ttotal: 8.11s\tremaining: 9.26s\n",
      "467:\tlearn: 0.2132660\ttotal: 8.13s\tremaining: 9.24s\n",
      "468:\tlearn: 0.2132474\ttotal: 8.15s\tremaining: 9.22s\n",
      "469:\tlearn: 0.2132244\ttotal: 8.16s\tremaining: 9.21s\n",
      "470:\tlearn: 0.2132140\ttotal: 8.18s\tremaining: 9.19s\n",
      "471:\tlearn: 0.2131928\ttotal: 8.2s\tremaining: 9.17s\n",
      "472:\tlearn: 0.2131733\ttotal: 8.22s\tremaining: 9.16s\n",
      "473:\tlearn: 0.2131379\ttotal: 8.24s\tremaining: 9.14s\n",
      "474:\tlearn: 0.2131163\ttotal: 8.26s\tremaining: 9.13s\n",
      "475:\tlearn: 0.2131043\ttotal: 8.28s\tremaining: 9.11s\n",
      "476:\tlearn: 0.2130904\ttotal: 8.3s\tremaining: 9.1s\n",
      "477:\tlearn: 0.2130692\ttotal: 8.31s\tremaining: 9.08s\n",
      "478:\tlearn: 0.2130509\ttotal: 8.33s\tremaining: 9.06s\n",
      "479:\tlearn: 0.2130207\ttotal: 8.35s\tremaining: 9.04s\n",
      "480:\tlearn: 0.2130011\ttotal: 8.37s\tremaining: 9.03s\n",
      "481:\tlearn: 0.2129674\ttotal: 8.38s\tremaining: 9.01s\n",
      "482:\tlearn: 0.2129241\ttotal: 8.4s\tremaining: 8.99s\n",
      "483:\tlearn: 0.2128980\ttotal: 8.42s\tremaining: 8.97s\n",
      "484:\tlearn: 0.2128764\ttotal: 8.43s\tremaining: 8.96s\n",
      "485:\tlearn: 0.2128258\ttotal: 8.45s\tremaining: 8.94s\n",
      "486:\tlearn: 0.2128013\ttotal: 8.47s\tremaining: 8.93s\n",
      "487:\tlearn: 0.2127647\ttotal: 8.49s\tremaining: 8.91s\n",
      "488:\tlearn: 0.2127465\ttotal: 8.51s\tremaining: 8.89s\n",
      "489:\tlearn: 0.2127193\ttotal: 8.53s\tremaining: 8.87s\n",
      "490:\tlearn: 0.2127021\ttotal: 8.54s\tremaining: 8.86s\n",
      "491:\tlearn: 0.2126758\ttotal: 8.56s\tremaining: 8.84s\n",
      "492:\tlearn: 0.2126524\ttotal: 8.58s\tremaining: 8.82s\n",
      "493:\tlearn: 0.2126277\ttotal: 8.59s\tremaining: 8.8s\n",
      "494:\tlearn: 0.2126011\ttotal: 8.61s\tremaining: 8.79s\n",
      "495:\tlearn: 0.2125460\ttotal: 8.63s\tremaining: 8.77s\n",
      "496:\tlearn: 0.2125288\ttotal: 8.65s\tremaining: 8.75s\n",
      "497:\tlearn: 0.2125155\ttotal: 8.66s\tremaining: 8.73s\n",
      "498:\tlearn: 0.2124697\ttotal: 8.68s\tremaining: 8.72s\n",
      "499:\tlearn: 0.2124534\ttotal: 8.7s\tremaining: 8.7s\n",
      "500:\tlearn: 0.2124303\ttotal: 8.72s\tremaining: 8.68s\n",
      "501:\tlearn: 0.2124125\ttotal: 8.73s\tremaining: 8.67s\n",
      "502:\tlearn: 0.2123965\ttotal: 8.75s\tremaining: 8.65s\n",
      "503:\tlearn: 0.2123796\ttotal: 8.77s\tremaining: 8.63s\n",
      "504:\tlearn: 0.2123433\ttotal: 8.79s\tremaining: 8.61s\n",
      "505:\tlearn: 0.2123152\ttotal: 8.8s\tremaining: 8.59s\n",
      "506:\tlearn: 0.2123055\ttotal: 8.82s\tremaining: 8.58s\n",
      "507:\tlearn: 0.2122890\ttotal: 8.84s\tremaining: 8.56s\n",
      "508:\tlearn: 0.2122580\ttotal: 8.86s\tremaining: 8.54s\n",
      "509:\tlearn: 0.2122425\ttotal: 8.87s\tremaining: 8.52s\n",
      "510:\tlearn: 0.2122179\ttotal: 8.89s\tremaining: 8.51s\n",
      "511:\tlearn: 0.2121935\ttotal: 8.91s\tremaining: 8.49s\n",
      "512:\tlearn: 0.2121656\ttotal: 8.93s\tremaining: 8.47s\n",
      "513:\tlearn: 0.2121360\ttotal: 8.94s\tremaining: 8.46s\n",
      "514:\tlearn: 0.2121114\ttotal: 8.96s\tremaining: 8.44s\n",
      "515:\tlearn: 0.2120839\ttotal: 8.98s\tremaining: 8.42s\n",
      "516:\tlearn: 0.2120714\ttotal: 8.99s\tremaining: 8.4s\n",
      "517:\tlearn: 0.2120550\ttotal: 9.01s\tremaining: 8.38s\n",
      "518:\tlearn: 0.2120151\ttotal: 9.03s\tremaining: 8.37s\n",
      "519:\tlearn: 0.2119986\ttotal: 9.04s\tremaining: 8.35s\n",
      "520:\tlearn: 0.2119517\ttotal: 9.06s\tremaining: 8.33s\n",
      "521:\tlearn: 0.2119312\ttotal: 9.08s\tremaining: 8.31s\n",
      "522:\tlearn: 0.2119009\ttotal: 9.1s\tremaining: 8.3s\n",
      "523:\tlearn: 0.2118708\ttotal: 9.12s\tremaining: 8.28s\n",
      "524:\tlearn: 0.2118481\ttotal: 9.13s\tremaining: 8.26s\n",
      "525:\tlearn: 0.2118343\ttotal: 9.15s\tremaining: 8.25s\n",
      "526:\tlearn: 0.2118197\ttotal: 9.17s\tremaining: 8.23s\n",
      "527:\tlearn: 0.2117770\ttotal: 9.19s\tremaining: 8.21s\n",
      "528:\tlearn: 0.2117534\ttotal: 9.2s\tremaining: 8.2s\n",
      "529:\tlearn: 0.2117381\ttotal: 9.22s\tremaining: 8.18s\n",
      "530:\tlearn: 0.2116968\ttotal: 9.24s\tremaining: 8.16s\n",
      "531:\tlearn: 0.2116707\ttotal: 9.25s\tremaining: 8.14s\n",
      "532:\tlearn: 0.2116543\ttotal: 9.27s\tremaining: 8.12s\n",
      "533:\tlearn: 0.2116385\ttotal: 9.29s\tremaining: 8.11s\n",
      "534:\tlearn: 0.2116242\ttotal: 9.31s\tremaining: 8.09s\n",
      "535:\tlearn: 0.2116030\ttotal: 9.33s\tremaining: 8.07s\n",
      "536:\tlearn: 0.2115909\ttotal: 9.34s\tremaining: 8.06s\n",
      "537:\tlearn: 0.2115477\ttotal: 9.36s\tremaining: 8.04s\n",
      "538:\tlearn: 0.2115250\ttotal: 9.38s\tremaining: 8.02s\n",
      "539:\tlearn: 0.2114843\ttotal: 9.39s\tremaining: 8s\n",
      "540:\tlearn: 0.2114691\ttotal: 9.41s\tremaining: 7.99s\n",
      "541:\tlearn: 0.2114577\ttotal: 9.43s\tremaining: 7.97s\n",
      "542:\tlearn: 0.2114469\ttotal: 9.45s\tremaining: 7.95s\n",
      "543:\tlearn: 0.2114189\ttotal: 9.46s\tremaining: 7.93s\n",
      "544:\tlearn: 0.2114087\ttotal: 9.48s\tremaining: 7.92s\n",
      "545:\tlearn: 0.2113938\ttotal: 9.5s\tremaining: 7.9s\n",
      "546:\tlearn: 0.2113504\ttotal: 9.52s\tremaining: 7.88s\n",
      "547:\tlearn: 0.2113361\ttotal: 9.54s\tremaining: 7.87s\n",
      "548:\tlearn: 0.2113253\ttotal: 9.55s\tremaining: 7.85s\n",
      "549:\tlearn: 0.2113059\ttotal: 9.57s\tremaining: 7.83s\n",
      "550:\tlearn: 0.2112840\ttotal: 9.59s\tremaining: 7.82s\n",
      "551:\tlearn: 0.2112457\ttotal: 9.61s\tremaining: 7.8s\n",
      "552:\tlearn: 0.2112233\ttotal: 9.63s\tremaining: 7.78s\n",
      "553:\tlearn: 0.2111846\ttotal: 9.64s\tremaining: 7.76s\n",
      "554:\tlearn: 0.2111770\ttotal: 9.66s\tremaining: 7.75s\n",
      "555:\tlearn: 0.2111379\ttotal: 9.68s\tremaining: 7.73s\n",
      "556:\tlearn: 0.2111225\ttotal: 9.7s\tremaining: 7.71s\n",
      "557:\tlearn: 0.2111022\ttotal: 9.71s\tremaining: 7.69s\n",
      "558:\tlearn: 0.2110903\ttotal: 9.73s\tremaining: 7.68s\n",
      "559:\tlearn: 0.2110493\ttotal: 9.75s\tremaining: 7.66s\n",
      "560:\tlearn: 0.2110120\ttotal: 9.77s\tremaining: 7.64s\n",
      "561:\tlearn: 0.2109707\ttotal: 9.79s\tremaining: 7.63s\n",
      "562:\tlearn: 0.2109469\ttotal: 9.8s\tremaining: 7.61s\n",
      "563:\tlearn: 0.2109044\ttotal: 9.82s\tremaining: 7.59s\n",
      "564:\tlearn: 0.2108732\ttotal: 9.84s\tremaining: 7.57s\n",
      "565:\tlearn: 0.2108619\ttotal: 9.85s\tremaining: 7.55s\n",
      "566:\tlearn: 0.2108363\ttotal: 9.87s\tremaining: 7.54s\n",
      "567:\tlearn: 0.2108251\ttotal: 9.88s\tremaining: 7.52s\n",
      "568:\tlearn: 0.2107999\ttotal: 9.9s\tremaining: 7.5s\n",
      "569:\tlearn: 0.2107751\ttotal: 9.92s\tremaining: 7.48s\n",
      "570:\tlearn: 0.2107546\ttotal: 9.94s\tremaining: 7.46s\n",
      "571:\tlearn: 0.2107441\ttotal: 9.96s\tremaining: 7.45s\n",
      "572:\tlearn: 0.2107206\ttotal: 9.97s\tremaining: 7.43s\n",
      "573:\tlearn: 0.2106973\ttotal: 9.99s\tremaining: 7.42s\n",
      "574:\tlearn: 0.2106615\ttotal: 10s\tremaining: 7.4s\n",
      "575:\tlearn: 0.2106411\ttotal: 10s\tremaining: 7.38s\n",
      "576:\tlearn: 0.2106148\ttotal: 10s\tremaining: 7.36s\n",
      "577:\tlearn: 0.2105811\ttotal: 10.1s\tremaining: 7.34s\n",
      "578:\tlearn: 0.2105704\ttotal: 10.1s\tremaining: 7.33s\n",
      "579:\tlearn: 0.2105487\ttotal: 10.1s\tremaining: 7.31s\n",
      "580:\tlearn: 0.2105148\ttotal: 10.1s\tremaining: 7.29s\n",
      "581:\tlearn: 0.2104948\ttotal: 10.1s\tremaining: 7.27s\n",
      "582:\tlearn: 0.2104760\ttotal: 10.1s\tremaining: 7.26s\n",
      "583:\tlearn: 0.2104614\ttotal: 10.2s\tremaining: 7.24s\n",
      "584:\tlearn: 0.2104485\ttotal: 10.2s\tremaining: 7.22s\n",
      "585:\tlearn: 0.2104292\ttotal: 10.2s\tremaining: 7.21s\n",
      "586:\tlearn: 0.2104158\ttotal: 10.2s\tremaining: 7.19s\n",
      "587:\tlearn: 0.2103954\ttotal: 10.2s\tremaining: 7.17s\n",
      "588:\tlearn: 0.2103847\ttotal: 10.3s\tremaining: 7.15s\n",
      "589:\tlearn: 0.2103720\ttotal: 10.3s\tremaining: 7.13s\n",
      "590:\tlearn: 0.2103408\ttotal: 10.3s\tremaining: 7.12s\n",
      "591:\tlearn: 0.2103235\ttotal: 10.3s\tremaining: 7.1s\n",
      "592:\tlearn: 0.2103133\ttotal: 10.3s\tremaining: 7.08s\n",
      "593:\tlearn: 0.2103027\ttotal: 10.3s\tremaining: 7.07s\n",
      "594:\tlearn: 0.2102593\ttotal: 10.4s\tremaining: 7.05s\n",
      "595:\tlearn: 0.2102455\ttotal: 10.4s\tremaining: 7.03s\n",
      "596:\tlearn: 0.2102120\ttotal: 10.4s\tremaining: 7.01s\n",
      "597:\tlearn: 0.2101856\ttotal: 10.4s\tremaining: 7s\n",
      "598:\tlearn: 0.2101520\ttotal: 10.4s\tremaining: 6.98s\n",
      "599:\tlearn: 0.2101195\ttotal: 10.4s\tremaining: 6.96s\n",
      "600:\tlearn: 0.2100928\ttotal: 10.5s\tremaining: 6.94s\n",
      "601:\tlearn: 0.2100643\ttotal: 10.5s\tremaining: 6.92s\n",
      "602:\tlearn: 0.2100266\ttotal: 10.5s\tremaining: 6.91s\n",
      "603:\tlearn: 0.2100097\ttotal: 10.5s\tremaining: 6.89s\n",
      "604:\tlearn: 0.2099926\ttotal: 10.5s\tremaining: 6.87s\n",
      "605:\tlearn: 0.2099466\ttotal: 10.5s\tremaining: 6.86s\n",
      "606:\tlearn: 0.2099190\ttotal: 10.6s\tremaining: 6.84s\n",
      "607:\tlearn: 0.2098823\ttotal: 10.6s\tremaining: 6.82s\n",
      "608:\tlearn: 0.2098573\ttotal: 10.6s\tremaining: 6.8s\n",
      "609:\tlearn: 0.2098320\ttotal: 10.6s\tremaining: 6.79s\n",
      "610:\tlearn: 0.2098081\ttotal: 10.6s\tremaining: 6.77s\n",
      "611:\tlearn: 0.2097827\ttotal: 10.7s\tremaining: 6.75s\n",
      "612:\tlearn: 0.2097675\ttotal: 10.7s\tremaining: 6.74s\n",
      "613:\tlearn: 0.2097395\ttotal: 10.7s\tremaining: 6.72s\n",
      "614:\tlearn: 0.2097015\ttotal: 10.7s\tremaining: 6.7s\n",
      "615:\tlearn: 0.2096841\ttotal: 10.7s\tremaining: 6.68s\n",
      "616:\tlearn: 0.2096536\ttotal: 10.7s\tremaining: 6.67s\n",
      "617:\tlearn: 0.2096383\ttotal: 10.8s\tremaining: 6.65s\n",
      "618:\tlearn: 0.2096050\ttotal: 10.8s\tremaining: 6.63s\n",
      "619:\tlearn: 0.2095949\ttotal: 10.8s\tremaining: 6.62s\n",
      "620:\tlearn: 0.2095828\ttotal: 10.8s\tremaining: 6.6s\n",
      "621:\tlearn: 0.2095412\ttotal: 10.8s\tremaining: 6.58s\n",
      "622:\tlearn: 0.2095297\ttotal: 10.8s\tremaining: 6.56s\n",
      "623:\tlearn: 0.2095114\ttotal: 10.9s\tremaining: 6.54s\n",
      "624:\tlearn: 0.2094941\ttotal: 10.9s\tremaining: 6.53s\n",
      "625:\tlearn: 0.2094545\ttotal: 10.9s\tremaining: 6.51s\n",
      "626:\tlearn: 0.2094437\ttotal: 10.9s\tremaining: 6.49s\n",
      "627:\tlearn: 0.2094115\ttotal: 10.9s\tremaining: 6.47s\n",
      "628:\tlearn: 0.2093808\ttotal: 10.9s\tremaining: 6.46s\n",
      "629:\tlearn: 0.2093683\ttotal: 11s\tremaining: 6.44s\n",
      "630:\tlearn: 0.2093413\ttotal: 11s\tremaining: 6.42s\n",
      "631:\tlearn: 0.2093213\ttotal: 11s\tremaining: 6.41s\n",
      "632:\tlearn: 0.2093084\ttotal: 11s\tremaining: 6.39s\n",
      "633:\tlearn: 0.2092823\ttotal: 11s\tremaining: 6.37s\n",
      "634:\tlearn: 0.2092462\ttotal: 11.1s\tremaining: 6.35s\n",
      "635:\tlearn: 0.2092138\ttotal: 11.1s\tremaining: 6.34s\n",
      "636:\tlearn: 0.2091932\ttotal: 11.1s\tremaining: 6.32s\n",
      "637:\tlearn: 0.2091707\ttotal: 11.1s\tremaining: 6.3s\n",
      "638:\tlearn: 0.2091542\ttotal: 11.1s\tremaining: 6.28s\n",
      "639:\tlearn: 0.2091283\ttotal: 11.1s\tremaining: 6.26s\n",
      "640:\tlearn: 0.2091123\ttotal: 11.2s\tremaining: 6.25s\n",
      "641:\tlearn: 0.2090855\ttotal: 11.2s\tremaining: 6.23s\n",
      "642:\tlearn: 0.2090702\ttotal: 11.2s\tremaining: 6.21s\n",
      "643:\tlearn: 0.2090381\ttotal: 11.2s\tremaining: 6.2s\n",
      "644:\tlearn: 0.2090235\ttotal: 11.2s\tremaining: 6.18s\n",
      "645:\tlearn: 0.2090106\ttotal: 11.2s\tremaining: 6.16s\n",
      "646:\tlearn: 0.2089820\ttotal: 11.3s\tremaining: 6.14s\n",
      "647:\tlearn: 0.2089368\ttotal: 11.3s\tremaining: 6.13s\n",
      "648:\tlearn: 0.2089249\ttotal: 11.3s\tremaining: 6.11s\n",
      "649:\tlearn: 0.2088766\ttotal: 11.3s\tremaining: 6.09s\n",
      "650:\tlearn: 0.2088575\ttotal: 11.3s\tremaining: 6.07s\n",
      "651:\tlearn: 0.2088259\ttotal: 11.3s\tremaining: 6.06s\n",
      "652:\tlearn: 0.2088121\ttotal: 11.4s\tremaining: 6.04s\n",
      "653:\tlearn: 0.2087978\ttotal: 11.4s\tremaining: 6.02s\n",
      "654:\tlearn: 0.2087794\ttotal: 11.4s\tremaining: 6s\n",
      "655:\tlearn: 0.2087562\ttotal: 11.4s\tremaining: 5.99s\n",
      "656:\tlearn: 0.2087418\ttotal: 11.4s\tremaining: 5.97s\n",
      "657:\tlearn: 0.2087188\ttotal: 11.4s\tremaining: 5.95s\n",
      "658:\tlearn: 0.2087038\ttotal: 11.5s\tremaining: 5.93s\n",
      "659:\tlearn: 0.2086907\ttotal: 11.5s\tremaining: 5.92s\n",
      "660:\tlearn: 0.2086786\ttotal: 11.5s\tremaining: 5.9s\n",
      "661:\tlearn: 0.2086398\ttotal: 11.5s\tremaining: 5.88s\n",
      "662:\tlearn: 0.2086046\ttotal: 11.5s\tremaining: 5.86s\n",
      "663:\tlearn: 0.2085723\ttotal: 11.6s\tremaining: 5.84s\n",
      "664:\tlearn: 0.2085483\ttotal: 11.6s\tremaining: 5.83s\n",
      "665:\tlearn: 0.2085268\ttotal: 11.6s\tremaining: 5.81s\n",
      "666:\tlearn: 0.2085087\ttotal: 11.6s\tremaining: 5.79s\n",
      "667:\tlearn: 0.2084913\ttotal: 11.6s\tremaining: 5.78s\n",
      "668:\tlearn: 0.2084613\ttotal: 11.6s\tremaining: 5.76s\n",
      "669:\tlearn: 0.2084464\ttotal: 11.7s\tremaining: 5.74s\n",
      "670:\tlearn: 0.2084341\ttotal: 11.7s\tremaining: 5.72s\n",
      "671:\tlearn: 0.2084140\ttotal: 11.7s\tremaining: 5.71s\n",
      "672:\tlearn: 0.2083838\ttotal: 11.7s\tremaining: 5.69s\n",
      "673:\tlearn: 0.2083724\ttotal: 11.7s\tremaining: 5.67s\n",
      "674:\tlearn: 0.2083495\ttotal: 11.7s\tremaining: 5.66s\n",
      "675:\tlearn: 0.2083349\ttotal: 11.8s\tremaining: 5.64s\n",
      "676:\tlearn: 0.2083194\ttotal: 11.8s\tremaining: 5.62s\n",
      "677:\tlearn: 0.2083058\ttotal: 11.8s\tremaining: 5.6s\n",
      "678:\tlearn: 0.2082723\ttotal: 11.8s\tremaining: 5.58s\n",
      "679:\tlearn: 0.2082446\ttotal: 11.8s\tremaining: 5.57s\n",
      "680:\tlearn: 0.2082270\ttotal: 11.9s\tremaining: 5.55s\n",
      "681:\tlearn: 0.2082067\ttotal: 11.9s\tremaining: 5.53s\n",
      "682:\tlearn: 0.2081661\ttotal: 11.9s\tremaining: 5.52s\n",
      "683:\tlearn: 0.2081440\ttotal: 11.9s\tremaining: 5.5s\n",
      "684:\tlearn: 0.2081188\ttotal: 11.9s\tremaining: 5.48s\n",
      "685:\tlearn: 0.2080910\ttotal: 11.9s\tremaining: 5.46s\n",
      "686:\tlearn: 0.2080782\ttotal: 12s\tremaining: 5.45s\n",
      "687:\tlearn: 0.2080628\ttotal: 12s\tremaining: 5.43s\n",
      "688:\tlearn: 0.2080539\ttotal: 12s\tremaining: 5.41s\n",
      "689:\tlearn: 0.2080340\ttotal: 12s\tremaining: 5.39s\n",
      "690:\tlearn: 0.2079941\ttotal: 12s\tremaining: 5.38s\n",
      "691:\tlearn: 0.2079799\ttotal: 12s\tremaining: 5.36s\n",
      "692:\tlearn: 0.2079680\ttotal: 12.1s\tremaining: 5.34s\n",
      "693:\tlearn: 0.2079416\ttotal: 12.1s\tremaining: 5.33s\n",
      "694:\tlearn: 0.2079189\ttotal: 12.1s\tremaining: 5.31s\n",
      "695:\tlearn: 0.2078645\ttotal: 12.1s\tremaining: 5.29s\n",
      "696:\tlearn: 0.2078458\ttotal: 12.1s\tremaining: 5.27s\n",
      "697:\tlearn: 0.2078275\ttotal: 12.1s\tremaining: 5.26s\n",
      "698:\tlearn: 0.2078038\ttotal: 12.2s\tremaining: 5.24s\n",
      "699:\tlearn: 0.2077903\ttotal: 12.2s\tremaining: 5.22s\n",
      "700:\tlearn: 0.2077585\ttotal: 12.2s\tremaining: 5.21s\n",
      "701:\tlearn: 0.2077397\ttotal: 12.2s\tremaining: 5.19s\n",
      "702:\tlearn: 0.2077100\ttotal: 12.2s\tremaining: 5.17s\n",
      "703:\tlearn: 0.2076842\ttotal: 12.3s\tremaining: 5.15s\n",
      "704:\tlearn: 0.2076406\ttotal: 12.3s\tremaining: 5.14s\n",
      "705:\tlearn: 0.2076203\ttotal: 12.3s\tremaining: 5.12s\n",
      "706:\tlearn: 0.2075866\ttotal: 12.3s\tremaining: 5.1s\n",
      "707:\tlearn: 0.2075770\ttotal: 12.3s\tremaining: 5.08s\n",
      "708:\tlearn: 0.2075629\ttotal: 12.3s\tremaining: 5.07s\n",
      "709:\tlearn: 0.2075457\ttotal: 12.4s\tremaining: 5.05s\n",
      "710:\tlearn: 0.2075323\ttotal: 12.4s\tremaining: 5.03s\n",
      "711:\tlearn: 0.2075232\ttotal: 12.4s\tremaining: 5.01s\n",
      "712:\tlearn: 0.2075145\ttotal: 12.4s\tremaining: 5s\n",
      "713:\tlearn: 0.2075036\ttotal: 12.4s\tremaining: 4.98s\n",
      "714:\tlearn: 0.2074782\ttotal: 12.4s\tremaining: 4.96s\n",
      "715:\tlearn: 0.2074586\ttotal: 12.5s\tremaining: 4.94s\n",
      "716:\tlearn: 0.2074388\ttotal: 12.5s\tremaining: 4.93s\n",
      "717:\tlearn: 0.2074157\ttotal: 12.5s\tremaining: 4.91s\n",
      "718:\tlearn: 0.2074028\ttotal: 12.5s\tremaining: 4.89s\n",
      "719:\tlearn: 0.2073819\ttotal: 12.5s\tremaining: 4.88s\n",
      "720:\tlearn: 0.2073477\ttotal: 12.6s\tremaining: 4.86s\n",
      "721:\tlearn: 0.2073298\ttotal: 12.6s\tremaining: 4.84s\n",
      "722:\tlearn: 0.2073065\ttotal: 12.6s\tremaining: 4.82s\n",
      "723:\tlearn: 0.2072743\ttotal: 12.6s\tremaining: 4.8s\n",
      "724:\tlearn: 0.2072659\ttotal: 12.6s\tremaining: 4.79s\n",
      "725:\tlearn: 0.2072358\ttotal: 12.6s\tremaining: 4.77s\n",
      "726:\tlearn: 0.2072197\ttotal: 12.7s\tremaining: 4.75s\n",
      "727:\tlearn: 0.2071792\ttotal: 12.7s\tremaining: 4.74s\n",
      "728:\tlearn: 0.2071574\ttotal: 12.7s\tremaining: 4.72s\n",
      "729:\tlearn: 0.2071196\ttotal: 12.7s\tremaining: 4.7s\n",
      "730:\tlearn: 0.2070852\ttotal: 12.7s\tremaining: 4.68s\n",
      "731:\tlearn: 0.2070656\ttotal: 12.7s\tremaining: 4.67s\n",
      "732:\tlearn: 0.2070531\ttotal: 12.8s\tremaining: 4.65s\n",
      "733:\tlearn: 0.2070367\ttotal: 12.8s\tremaining: 4.63s\n",
      "734:\tlearn: 0.2070149\ttotal: 12.8s\tremaining: 4.61s\n",
      "735:\tlearn: 0.2069945\ttotal: 12.8s\tremaining: 4.6s\n",
      "736:\tlearn: 0.2069702\ttotal: 12.8s\tremaining: 4.58s\n",
      "737:\tlearn: 0.2069516\ttotal: 12.8s\tremaining: 4.56s\n",
      "738:\tlearn: 0.2069154\ttotal: 12.9s\tremaining: 4.54s\n",
      "739:\tlearn: 0.2069044\ttotal: 12.9s\tremaining: 4.53s\n",
      "740:\tlearn: 0.2068968\ttotal: 12.9s\tremaining: 4.51s\n",
      "741:\tlearn: 0.2068707\ttotal: 12.9s\tremaining: 4.49s\n",
      "742:\tlearn: 0.2068397\ttotal: 12.9s\tremaining: 4.47s\n",
      "743:\tlearn: 0.2068156\ttotal: 13s\tremaining: 4.46s\n",
      "744:\tlearn: 0.2067978\ttotal: 13s\tremaining: 4.44s\n",
      "745:\tlearn: 0.2067766\ttotal: 13s\tremaining: 4.42s\n",
      "746:\tlearn: 0.2067580\ttotal: 13s\tremaining: 4.41s\n",
      "747:\tlearn: 0.2067325\ttotal: 13s\tremaining: 4.39s\n",
      "748:\tlearn: 0.2067049\ttotal: 13s\tremaining: 4.37s\n",
      "749:\tlearn: 0.2066674\ttotal: 13.1s\tremaining: 4.35s\n",
      "750:\tlearn: 0.2066335\ttotal: 13.1s\tremaining: 4.33s\n",
      "751:\tlearn: 0.2066207\ttotal: 13.1s\tremaining: 4.32s\n",
      "752:\tlearn: 0.2066002\ttotal: 13.1s\tremaining: 4.3s\n",
      "753:\tlearn: 0.2065828\ttotal: 13.1s\tremaining: 4.28s\n",
      "754:\tlearn: 0.2065446\ttotal: 13.1s\tremaining: 4.27s\n",
      "755:\tlearn: 0.2065159\ttotal: 13.2s\tremaining: 4.25s\n",
      "756:\tlearn: 0.2065007\ttotal: 13.2s\tremaining: 4.23s\n",
      "757:\tlearn: 0.2064517\ttotal: 13.2s\tremaining: 4.21s\n",
      "758:\tlearn: 0.2064213\ttotal: 13.2s\tremaining: 4.2s\n",
      "759:\tlearn: 0.2063853\ttotal: 13.2s\tremaining: 4.18s\n",
      "760:\tlearn: 0.2063588\ttotal: 13.3s\tremaining: 4.16s\n",
      "761:\tlearn: 0.2063420\ttotal: 13.3s\tremaining: 4.14s\n",
      "762:\tlearn: 0.2062992\ttotal: 13.3s\tremaining: 4.13s\n",
      "763:\tlearn: 0.2062830\ttotal: 13.3s\tremaining: 4.11s\n",
      "764:\tlearn: 0.2062603\ttotal: 13.3s\tremaining: 4.09s\n",
      "765:\tlearn: 0.2062360\ttotal: 13.3s\tremaining: 4.08s\n",
      "766:\tlearn: 0.2061993\ttotal: 13.4s\tremaining: 4.06s\n",
      "767:\tlearn: 0.2061707\ttotal: 13.4s\tremaining: 4.04s\n",
      "768:\tlearn: 0.2061455\ttotal: 13.4s\tremaining: 4.02s\n",
      "769:\tlearn: 0.2061150\ttotal: 13.4s\tremaining: 4s\n",
      "770:\tlearn: 0.2060964\ttotal: 13.4s\tremaining: 3.99s\n",
      "771:\tlearn: 0.2060773\ttotal: 13.4s\tremaining: 3.97s\n",
      "772:\tlearn: 0.2060697\ttotal: 13.5s\tremaining: 3.95s\n",
      "773:\tlearn: 0.2060521\ttotal: 13.5s\tremaining: 3.94s\n",
      "774:\tlearn: 0.2060369\ttotal: 13.5s\tremaining: 3.92s\n",
      "775:\tlearn: 0.2060219\ttotal: 13.5s\tremaining: 3.9s\n",
      "776:\tlearn: 0.2060000\ttotal: 13.5s\tremaining: 3.88s\n",
      "777:\tlearn: 0.2059929\ttotal: 13.5s\tremaining: 3.87s\n",
      "778:\tlearn: 0.2059746\ttotal: 13.6s\tremaining: 3.85s\n",
      "779:\tlearn: 0.2059429\ttotal: 13.6s\tremaining: 3.83s\n",
      "780:\tlearn: 0.2059132\ttotal: 13.6s\tremaining: 3.81s\n",
      "781:\tlearn: 0.2058795\ttotal: 13.6s\tremaining: 3.8s\n",
      "782:\tlearn: 0.2058655\ttotal: 13.6s\tremaining: 3.78s\n",
      "783:\tlearn: 0.2058354\ttotal: 13.7s\tremaining: 3.76s\n",
      "784:\tlearn: 0.2058092\ttotal: 13.7s\tremaining: 3.74s\n",
      "785:\tlearn: 0.2057919\ttotal: 13.7s\tremaining: 3.73s\n",
      "786:\tlearn: 0.2057659\ttotal: 13.7s\tremaining: 3.71s\n",
      "787:\tlearn: 0.2057506\ttotal: 13.7s\tremaining: 3.69s\n",
      "788:\tlearn: 0.2057063\ttotal: 13.7s\tremaining: 3.67s\n",
      "789:\tlearn: 0.2056917\ttotal: 13.8s\tremaining: 3.66s\n",
      "790:\tlearn: 0.2056553\ttotal: 13.8s\tremaining: 3.64s\n",
      "791:\tlearn: 0.2056408\ttotal: 13.8s\tremaining: 3.62s\n",
      "792:\tlearn: 0.2056114\ttotal: 13.8s\tremaining: 3.6s\n",
      "793:\tlearn: 0.2055981\ttotal: 13.8s\tremaining: 3.59s\n",
      "794:\tlearn: 0.2055771\ttotal: 13.8s\tremaining: 3.57s\n",
      "795:\tlearn: 0.2055618\ttotal: 13.9s\tremaining: 3.55s\n",
      "796:\tlearn: 0.2055460\ttotal: 13.9s\tremaining: 3.54s\n",
      "797:\tlearn: 0.2055167\ttotal: 13.9s\tremaining: 3.52s\n",
      "798:\tlearn: 0.2054779\ttotal: 13.9s\tremaining: 3.5s\n",
      "799:\tlearn: 0.2054343\ttotal: 13.9s\tremaining: 3.48s\n",
      "800:\tlearn: 0.2054188\ttotal: 13.9s\tremaining: 3.46s\n",
      "801:\tlearn: 0.2053751\ttotal: 14s\tremaining: 3.45s\n",
      "802:\tlearn: 0.2053456\ttotal: 14s\tremaining: 3.43s\n",
      "803:\tlearn: 0.2053122\ttotal: 14s\tremaining: 3.41s\n",
      "804:\tlearn: 0.2052882\ttotal: 14s\tremaining: 3.4s\n",
      "805:\tlearn: 0.2052704\ttotal: 14s\tremaining: 3.38s\n",
      "806:\tlearn: 0.2052426\ttotal: 14.1s\tremaining: 3.36s\n",
      "807:\tlearn: 0.2052307\ttotal: 14.1s\tremaining: 3.34s\n",
      "808:\tlearn: 0.2052082\ttotal: 14.1s\tremaining: 3.33s\n",
      "809:\tlearn: 0.2051937\ttotal: 14.1s\tremaining: 3.31s\n",
      "810:\tlearn: 0.2051711\ttotal: 14.1s\tremaining: 3.29s\n",
      "811:\tlearn: 0.2051548\ttotal: 14.1s\tremaining: 3.27s\n",
      "812:\tlearn: 0.2051099\ttotal: 14.2s\tremaining: 3.26s\n",
      "813:\tlearn: 0.2050801\ttotal: 14.2s\tremaining: 3.24s\n",
      "814:\tlearn: 0.2050554\ttotal: 14.2s\tremaining: 3.22s\n",
      "815:\tlearn: 0.2050349\ttotal: 14.2s\tremaining: 3.21s\n",
      "816:\tlearn: 0.2050036\ttotal: 14.2s\tremaining: 3.19s\n",
      "817:\tlearn: 0.2049761\ttotal: 14.2s\tremaining: 3.17s\n",
      "818:\tlearn: 0.2049553\ttotal: 14.3s\tremaining: 3.15s\n",
      "819:\tlearn: 0.2049339\ttotal: 14.3s\tremaining: 3.13s\n",
      "820:\tlearn: 0.2049186\ttotal: 14.3s\tremaining: 3.12s\n",
      "821:\tlearn: 0.2049081\ttotal: 14.3s\tremaining: 3.1s\n",
      "822:\tlearn: 0.2048860\ttotal: 14.3s\tremaining: 3.08s\n",
      "823:\tlearn: 0.2048447\ttotal: 14.4s\tremaining: 3.06s\n",
      "824:\tlearn: 0.2048227\ttotal: 14.4s\tremaining: 3.05s\n",
      "825:\tlearn: 0.2048008\ttotal: 14.4s\tremaining: 3.03s\n",
      "826:\tlearn: 0.2047822\ttotal: 14.4s\tremaining: 3.01s\n",
      "827:\tlearn: 0.2047656\ttotal: 14.4s\tremaining: 3s\n",
      "828:\tlearn: 0.2047449\ttotal: 14.4s\tremaining: 2.98s\n",
      "829:\tlearn: 0.2047168\ttotal: 14.5s\tremaining: 2.96s\n",
      "830:\tlearn: 0.2046862\ttotal: 14.5s\tremaining: 2.94s\n",
      "831:\tlearn: 0.2046617\ttotal: 14.5s\tremaining: 2.93s\n",
      "832:\tlearn: 0.2046429\ttotal: 14.5s\tremaining: 2.91s\n",
      "833:\tlearn: 0.2046194\ttotal: 14.5s\tremaining: 2.89s\n",
      "834:\tlearn: 0.2046109\ttotal: 14.5s\tremaining: 2.87s\n",
      "835:\tlearn: 0.2045987\ttotal: 14.6s\tremaining: 2.86s\n",
      "836:\tlearn: 0.2045741\ttotal: 14.6s\tremaining: 2.84s\n",
      "837:\tlearn: 0.2045357\ttotal: 14.6s\tremaining: 2.82s\n",
      "838:\tlearn: 0.2045203\ttotal: 14.6s\tremaining: 2.8s\n",
      "839:\tlearn: 0.2045069\ttotal: 14.6s\tremaining: 2.79s\n",
      "840:\tlearn: 0.2044936\ttotal: 14.6s\tremaining: 2.77s\n",
      "841:\tlearn: 0.2044541\ttotal: 14.7s\tremaining: 2.75s\n",
      "842:\tlearn: 0.2044415\ttotal: 14.7s\tremaining: 2.73s\n",
      "843:\tlearn: 0.2044193\ttotal: 14.7s\tremaining: 2.72s\n",
      "844:\tlearn: 0.2044025\ttotal: 14.7s\tremaining: 2.7s\n",
      "845:\tlearn: 0.2043704\ttotal: 14.7s\tremaining: 2.68s\n",
      "846:\tlearn: 0.2043503\ttotal: 14.8s\tremaining: 2.66s\n",
      "847:\tlearn: 0.2043370\ttotal: 14.8s\tremaining: 2.65s\n",
      "848:\tlearn: 0.2043209\ttotal: 14.8s\tremaining: 2.63s\n",
      "849:\tlearn: 0.2042921\ttotal: 14.8s\tremaining: 2.61s\n",
      "850:\tlearn: 0.2042817\ttotal: 14.8s\tremaining: 2.6s\n",
      "851:\tlearn: 0.2042487\ttotal: 14.8s\tremaining: 2.58s\n",
      "852:\tlearn: 0.2042303\ttotal: 14.9s\tremaining: 2.56s\n",
      "853:\tlearn: 0.2042131\ttotal: 14.9s\tremaining: 2.54s\n",
      "854:\tlearn: 0.2041885\ttotal: 14.9s\tremaining: 2.53s\n",
      "855:\tlearn: 0.2041605\ttotal: 14.9s\tremaining: 2.51s\n",
      "856:\tlearn: 0.2041495\ttotal: 14.9s\tremaining: 2.49s\n",
      "857:\tlearn: 0.2041273\ttotal: 15s\tremaining: 2.47s\n",
      "858:\tlearn: 0.2041058\ttotal: 15s\tremaining: 2.46s\n",
      "859:\tlearn: 0.2040844\ttotal: 15s\tremaining: 2.44s\n",
      "860:\tlearn: 0.2040770\ttotal: 15s\tremaining: 2.42s\n",
      "861:\tlearn: 0.2040510\ttotal: 15s\tremaining: 2.4s\n",
      "862:\tlearn: 0.2040377\ttotal: 15s\tremaining: 2.39s\n",
      "863:\tlearn: 0.2039999\ttotal: 15.1s\tremaining: 2.37s\n",
      "864:\tlearn: 0.2039839\ttotal: 15.1s\tremaining: 2.35s\n",
      "865:\tlearn: 0.2039623\ttotal: 15.1s\tremaining: 2.33s\n",
      "866:\tlearn: 0.2039410\ttotal: 15.1s\tremaining: 2.32s\n",
      "867:\tlearn: 0.2039251\ttotal: 15.1s\tremaining: 2.3s\n",
      "868:\tlearn: 0.2039076\ttotal: 15.1s\tremaining: 2.28s\n",
      "869:\tlearn: 0.2038870\ttotal: 15.2s\tremaining: 2.27s\n",
      "870:\tlearn: 0.2038458\ttotal: 15.2s\tremaining: 2.25s\n",
      "871:\tlearn: 0.2038292\ttotal: 15.2s\tremaining: 2.23s\n",
      "872:\tlearn: 0.2038104\ttotal: 15.2s\tremaining: 2.21s\n",
      "873:\tlearn: 0.2037724\ttotal: 15.2s\tremaining: 2.19s\n",
      "874:\tlearn: 0.2037520\ttotal: 15.2s\tremaining: 2.18s\n",
      "875:\tlearn: 0.2037365\ttotal: 15.3s\tremaining: 2.16s\n",
      "876:\tlearn: 0.2037171\ttotal: 15.3s\tremaining: 2.14s\n",
      "877:\tlearn: 0.2037025\ttotal: 15.3s\tremaining: 2.13s\n",
      "878:\tlearn: 0.2036875\ttotal: 15.3s\tremaining: 2.11s\n",
      "879:\tlearn: 0.2036624\ttotal: 15.3s\tremaining: 2.09s\n",
      "880:\tlearn: 0.2036475\ttotal: 15.4s\tremaining: 2.07s\n",
      "881:\tlearn: 0.2036160\ttotal: 15.4s\tremaining: 2.06s\n",
      "882:\tlearn: 0.2036033\ttotal: 15.4s\tremaining: 2.04s\n",
      "883:\tlearn: 0.2035800\ttotal: 15.4s\tremaining: 2.02s\n",
      "884:\tlearn: 0.2035673\ttotal: 15.4s\tremaining: 2s\n",
      "885:\tlearn: 0.2035525\ttotal: 15.4s\tremaining: 1.99s\n",
      "886:\tlearn: 0.2035274\ttotal: 15.5s\tremaining: 1.97s\n",
      "887:\tlearn: 0.2035072\ttotal: 15.5s\tremaining: 1.95s\n",
      "888:\tlearn: 0.2034885\ttotal: 15.5s\tremaining: 1.93s\n",
      "889:\tlearn: 0.2034498\ttotal: 15.5s\tremaining: 1.92s\n",
      "890:\tlearn: 0.2034316\ttotal: 15.5s\tremaining: 1.9s\n",
      "891:\tlearn: 0.2034044\ttotal: 15.5s\tremaining: 1.88s\n",
      "892:\tlearn: 0.2033682\ttotal: 15.6s\tremaining: 1.86s\n",
      "893:\tlearn: 0.2033475\ttotal: 15.6s\tremaining: 1.85s\n",
      "894:\tlearn: 0.2033231\ttotal: 15.6s\tremaining: 1.83s\n",
      "895:\tlearn: 0.2033085\ttotal: 15.6s\tremaining: 1.81s\n",
      "896:\tlearn: 0.2032804\ttotal: 15.6s\tremaining: 1.79s\n",
      "897:\tlearn: 0.2032614\ttotal: 15.6s\tremaining: 1.78s\n",
      "898:\tlearn: 0.2032421\ttotal: 15.7s\tremaining: 1.76s\n",
      "899:\tlearn: 0.2032296\ttotal: 15.7s\tremaining: 1.74s\n",
      "900:\tlearn: 0.2032159\ttotal: 15.7s\tremaining: 1.73s\n",
      "901:\tlearn: 0.2031853\ttotal: 15.7s\tremaining: 1.71s\n",
      "902:\tlearn: 0.2031596\ttotal: 15.7s\tremaining: 1.69s\n",
      "903:\tlearn: 0.2031342\ttotal: 15.8s\tremaining: 1.67s\n",
      "904:\tlearn: 0.2031071\ttotal: 15.8s\tremaining: 1.66s\n",
      "905:\tlearn: 0.2030978\ttotal: 15.8s\tremaining: 1.64s\n",
      "906:\tlearn: 0.2030713\ttotal: 15.8s\tremaining: 1.62s\n",
      "907:\tlearn: 0.2030337\ttotal: 15.8s\tremaining: 1.6s\n",
      "908:\tlearn: 0.2029838\ttotal: 15.9s\tremaining: 1.59s\n",
      "909:\tlearn: 0.2029504\ttotal: 15.9s\tremaining: 1.57s\n",
      "910:\tlearn: 0.2029317\ttotal: 15.9s\tremaining: 1.55s\n",
      "911:\tlearn: 0.2029147\ttotal: 15.9s\tremaining: 1.53s\n",
      "912:\tlearn: 0.2028796\ttotal: 15.9s\tremaining: 1.52s\n",
      "913:\tlearn: 0.2028647\ttotal: 15.9s\tremaining: 1.5s\n",
      "914:\tlearn: 0.2028427\ttotal: 16s\tremaining: 1.48s\n",
      "915:\tlearn: 0.2028171\ttotal: 16s\tremaining: 1.47s\n",
      "916:\tlearn: 0.2027714\ttotal: 16s\tremaining: 1.45s\n",
      "917:\tlearn: 0.2027351\ttotal: 16s\tremaining: 1.43s\n",
      "918:\tlearn: 0.2027270\ttotal: 16s\tremaining: 1.41s\n",
      "919:\tlearn: 0.2027106\ttotal: 16s\tremaining: 1.4s\n",
      "920:\tlearn: 0.2026768\ttotal: 16.1s\tremaining: 1.38s\n",
      "921:\tlearn: 0.2026587\ttotal: 16.1s\tremaining: 1.36s\n",
      "922:\tlearn: 0.2026341\ttotal: 16.1s\tremaining: 1.34s\n",
      "923:\tlearn: 0.2026167\ttotal: 16.1s\tremaining: 1.32s\n",
      "924:\tlearn: 0.2025935\ttotal: 16.1s\tremaining: 1.31s\n",
      "925:\tlearn: 0.2025661\ttotal: 16.1s\tremaining: 1.29s\n",
      "926:\tlearn: 0.2025467\ttotal: 16.2s\tremaining: 1.27s\n",
      "927:\tlearn: 0.2025226\ttotal: 16.2s\tremaining: 1.25s\n",
      "928:\tlearn: 0.2024907\ttotal: 16.2s\tremaining: 1.24s\n",
      "929:\tlearn: 0.2024440\ttotal: 16.2s\tremaining: 1.22s\n",
      "930:\tlearn: 0.2024211\ttotal: 16.2s\tremaining: 1.2s\n",
      "931:\tlearn: 0.2023956\ttotal: 16.3s\tremaining: 1.19s\n",
      "932:\tlearn: 0.2023780\ttotal: 16.3s\tremaining: 1.17s\n",
      "933:\tlearn: 0.2023611\ttotal: 16.3s\tremaining: 1.15s\n",
      "934:\tlearn: 0.2023427\ttotal: 16.3s\tremaining: 1.13s\n",
      "935:\tlearn: 0.2023257\ttotal: 16.3s\tremaining: 1.12s\n",
      "936:\tlearn: 0.2022932\ttotal: 16.3s\tremaining: 1.1s\n",
      "937:\tlearn: 0.2022794\ttotal: 16.4s\tremaining: 1.08s\n",
      "938:\tlearn: 0.2022632\ttotal: 16.4s\tremaining: 1.06s\n",
      "939:\tlearn: 0.2022417\ttotal: 16.4s\tremaining: 1.05s\n",
      "940:\tlearn: 0.2022045\ttotal: 16.4s\tremaining: 1.03s\n",
      "941:\tlearn: 0.2021865\ttotal: 16.4s\tremaining: 1.01s\n",
      "942:\tlearn: 0.2021538\ttotal: 16.4s\tremaining: 994ms\n",
      "943:\tlearn: 0.2021411\ttotal: 16.5s\tremaining: 977ms\n",
      "944:\tlearn: 0.2021190\ttotal: 16.5s\tremaining: 959ms\n",
      "945:\tlearn: 0.2021037\ttotal: 16.5s\tremaining: 942ms\n",
      "946:\tlearn: 0.2020919\ttotal: 16.5s\tremaining: 924ms\n",
      "947:\tlearn: 0.2020810\ttotal: 16.5s\tremaining: 907ms\n",
      "948:\tlearn: 0.2020656\ttotal: 16.5s\tremaining: 889ms\n",
      "949:\tlearn: 0.2020578\ttotal: 16.6s\tremaining: 872ms\n",
      "950:\tlearn: 0.2020286\ttotal: 16.6s\tremaining: 854ms\n",
      "951:\tlearn: 0.2019847\ttotal: 16.6s\tremaining: 837ms\n",
      "952:\tlearn: 0.2019732\ttotal: 16.6s\tremaining: 820ms\n",
      "953:\tlearn: 0.2019464\ttotal: 16.6s\tremaining: 802ms\n",
      "954:\tlearn: 0.2019212\ttotal: 16.7s\tremaining: 785ms\n",
      "955:\tlearn: 0.2019001\ttotal: 16.7s\tremaining: 767ms\n",
      "956:\tlearn: 0.2018685\ttotal: 16.7s\tremaining: 750ms\n",
      "957:\tlearn: 0.2018357\ttotal: 16.7s\tremaining: 732ms\n",
      "958:\tlearn: 0.2017946\ttotal: 16.7s\tremaining: 715ms\n",
      "959:\tlearn: 0.2017775\ttotal: 16.7s\tremaining: 698ms\n",
      "960:\tlearn: 0.2017630\ttotal: 16.8s\tremaining: 680ms\n",
      "961:\tlearn: 0.2017284\ttotal: 16.8s\tremaining: 663ms\n",
      "962:\tlearn: 0.2016942\ttotal: 16.8s\tremaining: 645ms\n",
      "963:\tlearn: 0.2016534\ttotal: 16.8s\tremaining: 628ms\n",
      "964:\tlearn: 0.2016356\ttotal: 16.8s\tremaining: 610ms\n",
      "965:\tlearn: 0.2016108\ttotal: 16.8s\tremaining: 593ms\n",
      "966:\tlearn: 0.2015827\ttotal: 16.9s\tremaining: 575ms\n",
      "967:\tlearn: 0.2015603\ttotal: 16.9s\tremaining: 558ms\n",
      "968:\tlearn: 0.2015391\ttotal: 16.9s\tremaining: 541ms\n",
      "969:\tlearn: 0.2015049\ttotal: 16.9s\tremaining: 523ms\n",
      "970:\tlearn: 0.2014732\ttotal: 16.9s\tremaining: 506ms\n",
      "971:\tlearn: 0.2014435\ttotal: 16.9s\tremaining: 488ms\n",
      "972:\tlearn: 0.2014237\ttotal: 17s\tremaining: 471ms\n",
      "973:\tlearn: 0.2014115\ttotal: 17s\tremaining: 453ms\n",
      "974:\tlearn: 0.2013809\ttotal: 17s\tremaining: 436ms\n",
      "975:\tlearn: 0.2013548\ttotal: 17s\tremaining: 419ms\n",
      "976:\tlearn: 0.2013361\ttotal: 17s\tremaining: 401ms\n",
      "977:\tlearn: 0.2012937\ttotal: 17.1s\tremaining: 384ms\n",
      "978:\tlearn: 0.2012706\ttotal: 17.1s\tremaining: 366ms\n",
      "979:\tlearn: 0.2012329\ttotal: 17.1s\tremaining: 349ms\n",
      "980:\tlearn: 0.2012132\ttotal: 17.1s\tremaining: 331ms\n",
      "981:\tlearn: 0.2011975\ttotal: 17.1s\tremaining: 314ms\n",
      "982:\tlearn: 0.2011683\ttotal: 17.1s\tremaining: 296ms\n",
      "983:\tlearn: 0.2011465\ttotal: 17.2s\tremaining: 279ms\n",
      "984:\tlearn: 0.2011332\ttotal: 17.2s\tremaining: 262ms\n",
      "985:\tlearn: 0.2011201\ttotal: 17.2s\tremaining: 244ms\n",
      "986:\tlearn: 0.2011077\ttotal: 17.2s\tremaining: 227ms\n",
      "987:\tlearn: 0.2010885\ttotal: 17.2s\tremaining: 209ms\n",
      "988:\tlearn: 0.2010747\ttotal: 17.3s\tremaining: 192ms\n",
      "989:\tlearn: 0.2010543\ttotal: 17.3s\tremaining: 174ms\n",
      "990:\tlearn: 0.2010335\ttotal: 17.3s\tremaining: 157ms\n",
      "991:\tlearn: 0.2010075\ttotal: 17.3s\tremaining: 140ms\n",
      "992:\tlearn: 0.2009812\ttotal: 17.3s\tremaining: 122ms\n",
      "993:\tlearn: 0.2009667\ttotal: 17.3s\tremaining: 105ms\n",
      "994:\tlearn: 0.2009304\ttotal: 17.4s\tremaining: 87.2ms\n",
      "995:\tlearn: 0.2009010\ttotal: 17.4s\tremaining: 69.8ms\n",
      "996:\tlearn: 0.2008934\ttotal: 17.4s\tremaining: 52.3ms\n",
      "997:\tlearn: 0.2008653\ttotal: 17.4s\tremaining: 34.9ms\n",
      "998:\tlearn: 0.2008479\ttotal: 17.4s\tremaining: 17.4ms\n",
      "999:\tlearn: 0.2008256\ttotal: 17.4s\tremaining: 0us\n",
      "0:\tlearn: 0.9235724\ttotal: 20.3ms\tremaining: 20.3s\n",
      "1:\tlearn: 0.8560886\ttotal: 36.7ms\tremaining: 18.3s\n",
      "2:\tlearn: 0.7968378\ttotal: 52.5ms\tremaining: 17.4s\n",
      "3:\tlearn: 0.7438554\ttotal: 68.7ms\tremaining: 17.1s\n",
      "4:\tlearn: 0.6964301\ttotal: 84ms\tremaining: 16.7s\n",
      "5:\tlearn: 0.6536876\ttotal: 100ms\tremaining: 16.6s\n",
      "6:\tlearn: 0.6155779\ttotal: 116ms\tremaining: 16.5s\n",
      "7:\tlearn: 0.5813540\ttotal: 132ms\tremaining: 16.4s\n",
      "8:\tlearn: 0.5508226\ttotal: 148ms\tremaining: 16.3s\n",
      "9:\tlearn: 0.5229746\ttotal: 165ms\tremaining: 16.4s\n",
      "10:\tlearn: 0.4980055\ttotal: 182ms\tremaining: 16.3s\n",
      "11:\tlearn: 0.4753523\ttotal: 198ms\tremaining: 16.3s\n",
      "12:\tlearn: 0.4547288\ttotal: 214ms\tremaining: 16.2s\n",
      "13:\tlearn: 0.4362575\ttotal: 231ms\tremaining: 16.2s\n",
      "14:\tlearn: 0.4189553\ttotal: 247ms\tremaining: 16.2s\n",
      "15:\tlearn: 0.4037960\ttotal: 264ms\tremaining: 16.2s\n",
      "16:\tlearn: 0.3899277\ttotal: 283ms\tremaining: 16.4s\n",
      "17:\tlearn: 0.3772828\ttotal: 301ms\tremaining: 16.4s\n",
      "18:\tlearn: 0.3657035\ttotal: 317ms\tremaining: 16.4s\n",
      "19:\tlearn: 0.3547942\ttotal: 334ms\tremaining: 16.4s\n",
      "20:\tlearn: 0.3452078\ttotal: 350ms\tremaining: 16.3s\n",
      "21:\tlearn: 0.3364690\ttotal: 367ms\tremaining: 16.3s\n",
      "22:\tlearn: 0.3283340\ttotal: 384ms\tremaining: 16.3s\n",
      "23:\tlearn: 0.3209527\ttotal: 401ms\tremaining: 16.3s\n",
      "24:\tlearn: 0.3142178\ttotal: 419ms\tremaining: 16.4s\n",
      "25:\tlearn: 0.3077348\ttotal: 437ms\tremaining: 16.4s\n",
      "26:\tlearn: 0.3016476\ttotal: 454ms\tremaining: 16.4s\n",
      "27:\tlearn: 0.2964421\ttotal: 472ms\tremaining: 16.4s\n",
      "28:\tlearn: 0.2915585\ttotal: 488ms\tremaining: 16.4s\n",
      "29:\tlearn: 0.2870358\ttotal: 505ms\tremaining: 16.3s\n",
      "30:\tlearn: 0.2828251\ttotal: 522ms\tremaining: 16.3s\n",
      "31:\tlearn: 0.2789655\ttotal: 539ms\tremaining: 16.3s\n",
      "32:\tlearn: 0.2754045\ttotal: 555ms\tremaining: 16.3s\n",
      "33:\tlearn: 0.2718857\ttotal: 571ms\tremaining: 16.2s\n",
      "34:\tlearn: 0.2688830\ttotal: 588ms\tremaining: 16.2s\n",
      "35:\tlearn: 0.2661008\ttotal: 604ms\tremaining: 16.2s\n",
      "36:\tlearn: 0.2634201\ttotal: 620ms\tremaining: 16.1s\n",
      "37:\tlearn: 0.2607446\ttotal: 637ms\tremaining: 16.1s\n",
      "38:\tlearn: 0.2582179\ttotal: 655ms\tremaining: 16.1s\n",
      "39:\tlearn: 0.2561353\ttotal: 672ms\tremaining: 16.1s\n",
      "40:\tlearn: 0.2539317\ttotal: 690ms\tremaining: 16.1s\n",
      "41:\tlearn: 0.2521863\ttotal: 706ms\tremaining: 16.1s\n",
      "42:\tlearn: 0.2504174\ttotal: 723ms\tremaining: 16.1s\n",
      "43:\tlearn: 0.2488633\ttotal: 739ms\tremaining: 16.1s\n",
      "44:\tlearn: 0.2473985\ttotal: 756ms\tremaining: 16s\n",
      "45:\tlearn: 0.2458779\ttotal: 773ms\tremaining: 16s\n",
      "46:\tlearn: 0.2444509\ttotal: 789ms\tremaining: 16s\n",
      "47:\tlearn: 0.2432346\ttotal: 807ms\tremaining: 16s\n",
      "48:\tlearn: 0.2420211\ttotal: 823ms\tremaining: 16s\n",
      "49:\tlearn: 0.2410541\ttotal: 839ms\tremaining: 15.9s\n",
      "50:\tlearn: 0.2399969\ttotal: 859ms\tremaining: 16s\n",
      "51:\tlearn: 0.2390793\ttotal: 876ms\tremaining: 16s\n",
      "52:\tlearn: 0.2381597\ttotal: 893ms\tremaining: 16s\n",
      "53:\tlearn: 0.2371705\ttotal: 910ms\tremaining: 15.9s\n",
      "54:\tlearn: 0.2365122\ttotal: 928ms\tremaining: 15.9s\n",
      "55:\tlearn: 0.2356880\ttotal: 946ms\tremaining: 15.9s\n",
      "56:\tlearn: 0.2350136\ttotal: 962ms\tremaining: 15.9s\n",
      "57:\tlearn: 0.2342956\ttotal: 979ms\tremaining: 15.9s\n",
      "58:\tlearn: 0.2336616\ttotal: 996ms\tremaining: 15.9s\n",
      "59:\tlearn: 0.2330486\ttotal: 1.01s\tremaining: 15.9s\n",
      "60:\tlearn: 0.2325219\ttotal: 1.03s\tremaining: 15.8s\n",
      "61:\tlearn: 0.2319646\ttotal: 1.04s\tremaining: 15.8s\n",
      "62:\tlearn: 0.2315260\ttotal: 1.06s\tremaining: 15.8s\n",
      "63:\tlearn: 0.2310566\ttotal: 1.08s\tremaining: 15.8s\n",
      "64:\tlearn: 0.2306894\ttotal: 1.1s\tremaining: 15.8s\n",
      "65:\tlearn: 0.2303546\ttotal: 1.11s\tremaining: 15.8s\n",
      "66:\tlearn: 0.2299115\ttotal: 1.13s\tremaining: 15.8s\n",
      "67:\tlearn: 0.2296029\ttotal: 1.15s\tremaining: 15.7s\n",
      "68:\tlearn: 0.2291687\ttotal: 1.16s\tremaining: 15.7s\n",
      "69:\tlearn: 0.2287586\ttotal: 1.18s\tremaining: 15.7s\n",
      "70:\tlearn: 0.2284093\ttotal: 1.2s\tremaining: 15.7s\n",
      "71:\tlearn: 0.2281726\ttotal: 1.21s\tremaining: 15.6s\n",
      "72:\tlearn: 0.2279245\ttotal: 1.23s\tremaining: 15.6s\n",
      "73:\tlearn: 0.2276446\ttotal: 1.25s\tremaining: 15.6s\n",
      "74:\tlearn: 0.2273676\ttotal: 1.26s\tremaining: 15.6s\n",
      "75:\tlearn: 0.2271009\ttotal: 1.28s\tremaining: 15.6s\n",
      "76:\tlearn: 0.2268173\ttotal: 1.3s\tremaining: 15.6s\n",
      "77:\tlearn: 0.2265273\ttotal: 1.32s\tremaining: 15.6s\n",
      "78:\tlearn: 0.2263418\ttotal: 1.34s\tremaining: 15.6s\n",
      "79:\tlearn: 0.2260854\ttotal: 1.36s\tremaining: 15.6s\n",
      "80:\tlearn: 0.2258445\ttotal: 1.38s\tremaining: 15.6s\n",
      "81:\tlearn: 0.2256416\ttotal: 1.39s\tremaining: 15.6s\n",
      "82:\tlearn: 0.2254560\ttotal: 1.41s\tremaining: 15.6s\n",
      "83:\tlearn: 0.2253098\ttotal: 1.43s\tremaining: 15.6s\n",
      "84:\tlearn: 0.2250952\ttotal: 1.45s\tremaining: 15.6s\n",
      "85:\tlearn: 0.2249068\ttotal: 1.46s\tremaining: 15.6s\n",
      "86:\tlearn: 0.2247158\ttotal: 1.48s\tremaining: 15.5s\n",
      "87:\tlearn: 0.2245412\ttotal: 1.5s\tremaining: 15.5s\n",
      "88:\tlearn: 0.2244255\ttotal: 1.51s\tremaining: 15.5s\n",
      "89:\tlearn: 0.2243262\ttotal: 1.53s\tremaining: 15.5s\n",
      "90:\tlearn: 0.2242006\ttotal: 1.55s\tremaining: 15.5s\n",
      "91:\tlearn: 0.2240929\ttotal: 1.56s\tremaining: 15.4s\n",
      "92:\tlearn: 0.2239549\ttotal: 1.58s\tremaining: 15.4s\n",
      "93:\tlearn: 0.2237844\ttotal: 1.6s\tremaining: 15.4s\n",
      "94:\tlearn: 0.2236514\ttotal: 1.62s\tremaining: 15.4s\n",
      "95:\tlearn: 0.2235525\ttotal: 1.63s\tremaining: 15.4s\n",
      "96:\tlearn: 0.2234139\ttotal: 1.65s\tremaining: 15.4s\n",
      "97:\tlearn: 0.2233222\ttotal: 1.67s\tremaining: 15.4s\n",
      "98:\tlearn: 0.2231703\ttotal: 1.69s\tremaining: 15.3s\n",
      "99:\tlearn: 0.2230524\ttotal: 1.7s\tremaining: 15.3s\n",
      "100:\tlearn: 0.2229353\ttotal: 1.72s\tremaining: 15.3s\n",
      "101:\tlearn: 0.2228088\ttotal: 1.74s\tremaining: 15.3s\n",
      "102:\tlearn: 0.2227145\ttotal: 1.76s\tremaining: 15.3s\n",
      "103:\tlearn: 0.2226233\ttotal: 1.77s\tremaining: 15.3s\n",
      "104:\tlearn: 0.2225262\ttotal: 1.79s\tremaining: 15.3s\n",
      "105:\tlearn: 0.2224746\ttotal: 1.81s\tremaining: 15.2s\n",
      "106:\tlearn: 0.2224005\ttotal: 1.82s\tremaining: 15.2s\n",
      "107:\tlearn: 0.2223192\ttotal: 1.84s\tremaining: 15.2s\n",
      "108:\tlearn: 0.2222757\ttotal: 1.86s\tremaining: 15.2s\n",
      "109:\tlearn: 0.2221904\ttotal: 1.87s\tremaining: 15.2s\n",
      "110:\tlearn: 0.2220953\ttotal: 1.89s\tremaining: 15.2s\n",
      "111:\tlearn: 0.2220303\ttotal: 1.91s\tremaining: 15.1s\n",
      "112:\tlearn: 0.2219570\ttotal: 1.93s\tremaining: 15.1s\n",
      "113:\tlearn: 0.2219060\ttotal: 1.95s\tremaining: 15.1s\n",
      "114:\tlearn: 0.2218714\ttotal: 1.96s\tremaining: 15.1s\n",
      "115:\tlearn: 0.2218163\ttotal: 1.98s\tremaining: 15.1s\n",
      "116:\tlearn: 0.2217220\ttotal: 2s\tremaining: 15.1s\n",
      "117:\tlearn: 0.2216188\ttotal: 2.01s\tremaining: 15.1s\n",
      "118:\tlearn: 0.2215594\ttotal: 2.03s\tremaining: 15s\n",
      "119:\tlearn: 0.2214827\ttotal: 2.05s\tremaining: 15s\n",
      "120:\tlearn: 0.2213730\ttotal: 2.06s\tremaining: 15s\n",
      "121:\tlearn: 0.2212908\ttotal: 2.08s\tremaining: 15s\n",
      "122:\tlearn: 0.2212195\ttotal: 2.1s\tremaining: 15s\n",
      "123:\tlearn: 0.2211515\ttotal: 2.12s\tremaining: 15s\n",
      "124:\tlearn: 0.2211022\ttotal: 2.13s\tremaining: 14.9s\n",
      "125:\tlearn: 0.2210490\ttotal: 2.15s\tremaining: 14.9s\n",
      "126:\tlearn: 0.2210156\ttotal: 2.17s\tremaining: 14.9s\n",
      "127:\tlearn: 0.2209726\ttotal: 2.19s\tremaining: 14.9s\n",
      "128:\tlearn: 0.2209275\ttotal: 2.2s\tremaining: 14.9s\n",
      "129:\tlearn: 0.2208805\ttotal: 2.22s\tremaining: 14.9s\n",
      "130:\tlearn: 0.2208197\ttotal: 2.24s\tremaining: 14.8s\n",
      "131:\tlearn: 0.2207739\ttotal: 2.25s\tremaining: 14.8s\n",
      "132:\tlearn: 0.2207010\ttotal: 2.27s\tremaining: 14.8s\n",
      "133:\tlearn: 0.2206604\ttotal: 2.29s\tremaining: 14.8s\n",
      "134:\tlearn: 0.2206086\ttotal: 2.3s\tremaining: 14.8s\n",
      "135:\tlearn: 0.2205649\ttotal: 2.32s\tremaining: 14.7s\n",
      "136:\tlearn: 0.2205054\ttotal: 2.34s\tremaining: 14.7s\n",
      "137:\tlearn: 0.2204660\ttotal: 2.35s\tremaining: 14.7s\n",
      "138:\tlearn: 0.2203676\ttotal: 2.37s\tremaining: 14.7s\n",
      "139:\tlearn: 0.2202907\ttotal: 2.39s\tremaining: 14.7s\n",
      "140:\tlearn: 0.2202517\ttotal: 2.41s\tremaining: 14.7s\n",
      "141:\tlearn: 0.2201856\ttotal: 2.42s\tremaining: 14.7s\n",
      "142:\tlearn: 0.2201342\ttotal: 2.44s\tremaining: 14.6s\n",
      "143:\tlearn: 0.2200962\ttotal: 2.46s\tremaining: 14.6s\n",
      "144:\tlearn: 0.2200345\ttotal: 2.48s\tremaining: 14.6s\n",
      "145:\tlearn: 0.2200097\ttotal: 2.49s\tremaining: 14.6s\n",
      "146:\tlearn: 0.2199339\ttotal: 2.51s\tremaining: 14.6s\n",
      "147:\tlearn: 0.2198836\ttotal: 2.53s\tremaining: 14.6s\n",
      "148:\tlearn: 0.2198402\ttotal: 2.55s\tremaining: 14.6s\n",
      "149:\tlearn: 0.2198009\ttotal: 2.57s\tremaining: 14.6s\n",
      "150:\tlearn: 0.2197736\ttotal: 2.58s\tremaining: 14.5s\n",
      "151:\tlearn: 0.2197433\ttotal: 2.6s\tremaining: 14.5s\n",
      "152:\tlearn: 0.2197078\ttotal: 2.62s\tremaining: 14.5s\n",
      "153:\tlearn: 0.2196751\ttotal: 2.63s\tremaining: 14.5s\n",
      "154:\tlearn: 0.2196433\ttotal: 2.65s\tremaining: 14.5s\n",
      "155:\tlearn: 0.2195495\ttotal: 2.67s\tremaining: 14.4s\n",
      "156:\tlearn: 0.2195050\ttotal: 2.68s\tremaining: 14.4s\n",
      "157:\tlearn: 0.2194787\ttotal: 2.7s\tremaining: 14.4s\n",
      "158:\tlearn: 0.2194400\ttotal: 2.72s\tremaining: 14.4s\n",
      "159:\tlearn: 0.2193938\ttotal: 2.74s\tremaining: 14.4s\n",
      "160:\tlearn: 0.2193186\ttotal: 2.75s\tremaining: 14.4s\n",
      "161:\tlearn: 0.2192692\ttotal: 2.77s\tremaining: 14.3s\n",
      "162:\tlearn: 0.2192323\ttotal: 2.79s\tremaining: 14.3s\n",
      "163:\tlearn: 0.2191955\ttotal: 2.81s\tremaining: 14.3s\n",
      "164:\tlearn: 0.2191608\ttotal: 2.82s\tremaining: 14.3s\n",
      "165:\tlearn: 0.2191368\ttotal: 2.84s\tremaining: 14.3s\n",
      "166:\tlearn: 0.2191056\ttotal: 2.85s\tremaining: 14.2s\n",
      "167:\tlearn: 0.2190583\ttotal: 2.87s\tremaining: 14.2s\n",
      "168:\tlearn: 0.2190251\ttotal: 2.89s\tremaining: 14.2s\n",
      "169:\tlearn: 0.2189903\ttotal: 2.9s\tremaining: 14.2s\n",
      "170:\tlearn: 0.2189436\ttotal: 2.92s\tremaining: 14.2s\n",
      "171:\tlearn: 0.2188646\ttotal: 2.94s\tremaining: 14.2s\n",
      "172:\tlearn: 0.2187963\ttotal: 2.96s\tremaining: 14.1s\n",
      "173:\tlearn: 0.2187370\ttotal: 2.98s\tremaining: 14.1s\n",
      "174:\tlearn: 0.2186765\ttotal: 3s\tremaining: 14.1s\n",
      "175:\tlearn: 0.2186186\ttotal: 3.01s\tremaining: 14.1s\n",
      "176:\tlearn: 0.2185753\ttotal: 3.03s\tremaining: 14.1s\n",
      "177:\tlearn: 0.2185536\ttotal: 3.05s\tremaining: 14.1s\n",
      "178:\tlearn: 0.2185053\ttotal: 3.06s\tremaining: 14s\n",
      "179:\tlearn: 0.2184574\ttotal: 3.08s\tremaining: 14s\n",
      "180:\tlearn: 0.2184055\ttotal: 3.1s\tremaining: 14s\n",
      "181:\tlearn: 0.2183598\ttotal: 3.11s\tremaining: 14s\n",
      "182:\tlearn: 0.2183380\ttotal: 3.13s\tremaining: 14s\n",
      "183:\tlearn: 0.2182938\ttotal: 3.15s\tremaining: 14s\n",
      "184:\tlearn: 0.2182595\ttotal: 3.17s\tremaining: 13.9s\n",
      "185:\tlearn: 0.2182044\ttotal: 3.18s\tremaining: 13.9s\n",
      "186:\tlearn: 0.2181659\ttotal: 3.2s\tremaining: 13.9s\n",
      "187:\tlearn: 0.2181312\ttotal: 3.22s\tremaining: 13.9s\n",
      "188:\tlearn: 0.2180992\ttotal: 3.24s\tremaining: 13.9s\n",
      "189:\tlearn: 0.2180763\ttotal: 3.25s\tremaining: 13.9s\n",
      "190:\tlearn: 0.2180480\ttotal: 3.27s\tremaining: 13.8s\n",
      "191:\tlearn: 0.2180125\ttotal: 3.29s\tremaining: 13.8s\n",
      "192:\tlearn: 0.2179870\ttotal: 3.3s\tremaining: 13.8s\n",
      "193:\tlearn: 0.2179604\ttotal: 3.32s\tremaining: 13.8s\n",
      "194:\tlearn: 0.2179177\ttotal: 3.34s\tremaining: 13.8s\n",
      "195:\tlearn: 0.2178439\ttotal: 3.35s\tremaining: 13.8s\n",
      "196:\tlearn: 0.2178039\ttotal: 3.37s\tremaining: 13.7s\n",
      "197:\tlearn: 0.2177639\ttotal: 3.39s\tremaining: 13.7s\n",
      "198:\tlearn: 0.2177304\ttotal: 3.41s\tremaining: 13.7s\n",
      "199:\tlearn: 0.2177008\ttotal: 3.42s\tremaining: 13.7s\n",
      "200:\tlearn: 0.2176744\ttotal: 3.44s\tremaining: 13.7s\n",
      "201:\tlearn: 0.2176502\ttotal: 3.46s\tremaining: 13.7s\n",
      "202:\tlearn: 0.2176207\ttotal: 3.48s\tremaining: 13.7s\n",
      "203:\tlearn: 0.2175995\ttotal: 3.49s\tremaining: 13.6s\n",
      "204:\tlearn: 0.2175592\ttotal: 3.51s\tremaining: 13.6s\n",
      "205:\tlearn: 0.2175381\ttotal: 3.53s\tremaining: 13.6s\n",
      "206:\tlearn: 0.2175070\ttotal: 3.54s\tremaining: 13.6s\n",
      "207:\tlearn: 0.2174377\ttotal: 3.56s\tremaining: 13.6s\n",
      "208:\tlearn: 0.2174072\ttotal: 3.58s\tremaining: 13.5s\n",
      "209:\tlearn: 0.2173823\ttotal: 3.6s\tremaining: 13.5s\n",
      "210:\tlearn: 0.2173278\ttotal: 3.61s\tremaining: 13.5s\n",
      "211:\tlearn: 0.2172992\ttotal: 3.63s\tremaining: 13.5s\n",
      "212:\tlearn: 0.2172291\ttotal: 3.65s\tremaining: 13.5s\n",
      "213:\tlearn: 0.2172058\ttotal: 3.66s\tremaining: 13.5s\n",
      "214:\tlearn: 0.2171712\ttotal: 3.68s\tremaining: 13.4s\n",
      "215:\tlearn: 0.2171381\ttotal: 3.7s\tremaining: 13.4s\n",
      "216:\tlearn: 0.2170858\ttotal: 3.71s\tremaining: 13.4s\n",
      "217:\tlearn: 0.2170626\ttotal: 3.73s\tremaining: 13.4s\n",
      "218:\tlearn: 0.2170210\ttotal: 3.75s\tremaining: 13.4s\n",
      "219:\tlearn: 0.2169616\ttotal: 3.77s\tremaining: 13.4s\n",
      "220:\tlearn: 0.2169454\ttotal: 3.78s\tremaining: 13.3s\n",
      "221:\tlearn: 0.2169127\ttotal: 3.8s\tremaining: 13.3s\n",
      "222:\tlearn: 0.2168809\ttotal: 3.82s\tremaining: 13.3s\n",
      "223:\tlearn: 0.2168281\ttotal: 3.83s\tremaining: 13.3s\n",
      "224:\tlearn: 0.2168101\ttotal: 3.85s\tremaining: 13.3s\n",
      "225:\tlearn: 0.2167559\ttotal: 3.87s\tremaining: 13.3s\n",
      "226:\tlearn: 0.2167228\ttotal: 3.89s\tremaining: 13.2s\n",
      "227:\tlearn: 0.2167012\ttotal: 3.9s\tremaining: 13.2s\n",
      "228:\tlearn: 0.2166767\ttotal: 3.92s\tremaining: 13.2s\n",
      "229:\tlearn: 0.2166533\ttotal: 3.94s\tremaining: 13.2s\n",
      "230:\tlearn: 0.2166283\ttotal: 3.95s\tremaining: 13.2s\n",
      "231:\tlearn: 0.2165962\ttotal: 3.97s\tremaining: 13.1s\n",
      "232:\tlearn: 0.2165674\ttotal: 3.99s\tremaining: 13.1s\n",
      "233:\tlearn: 0.2165388\ttotal: 4s\tremaining: 13.1s\n",
      "234:\tlearn: 0.2165048\ttotal: 4.02s\tremaining: 13.1s\n",
      "235:\tlearn: 0.2164806\ttotal: 4.04s\tremaining: 13.1s\n",
      "236:\tlearn: 0.2164363\ttotal: 4.06s\tremaining: 13.1s\n",
      "237:\tlearn: 0.2164189\ttotal: 4.08s\tremaining: 13s\n",
      "238:\tlearn: 0.2163837\ttotal: 4.09s\tremaining: 13s\n",
      "239:\tlearn: 0.2163286\ttotal: 4.11s\tremaining: 13s\n",
      "240:\tlearn: 0.2162734\ttotal: 4.13s\tremaining: 13s\n",
      "241:\tlearn: 0.2162449\ttotal: 4.14s\tremaining: 13s\n",
      "242:\tlearn: 0.2161931\ttotal: 4.16s\tremaining: 13s\n",
      "243:\tlearn: 0.2161669\ttotal: 4.18s\tremaining: 12.9s\n",
      "244:\tlearn: 0.2161406\ttotal: 4.19s\tremaining: 12.9s\n",
      "245:\tlearn: 0.2161087\ttotal: 4.21s\tremaining: 12.9s\n",
      "246:\tlearn: 0.2160879\ttotal: 4.23s\tremaining: 12.9s\n",
      "247:\tlearn: 0.2160632\ttotal: 4.25s\tremaining: 12.9s\n",
      "248:\tlearn: 0.2160381\ttotal: 4.26s\tremaining: 12.9s\n",
      "249:\tlearn: 0.2160000\ttotal: 4.28s\tremaining: 12.8s\n",
      "250:\tlearn: 0.2159747\ttotal: 4.3s\tremaining: 12.8s\n",
      "251:\tlearn: 0.2159561\ttotal: 4.32s\tremaining: 12.8s\n",
      "252:\tlearn: 0.2159229\ttotal: 4.33s\tremaining: 12.8s\n",
      "253:\tlearn: 0.2159030\ttotal: 4.35s\tremaining: 12.8s\n",
      "254:\tlearn: 0.2158839\ttotal: 4.37s\tremaining: 12.8s\n",
      "255:\tlearn: 0.2158458\ttotal: 4.38s\tremaining: 12.7s\n",
      "256:\tlearn: 0.2158142\ttotal: 4.4s\tremaining: 12.7s\n",
      "257:\tlearn: 0.2157944\ttotal: 4.42s\tremaining: 12.7s\n",
      "258:\tlearn: 0.2157605\ttotal: 4.44s\tremaining: 12.7s\n",
      "259:\tlearn: 0.2157324\ttotal: 4.45s\tremaining: 12.7s\n",
      "260:\tlearn: 0.2157052\ttotal: 4.47s\tremaining: 12.7s\n",
      "261:\tlearn: 0.2156739\ttotal: 4.49s\tremaining: 12.6s\n",
      "262:\tlearn: 0.2156587\ttotal: 4.51s\tremaining: 12.6s\n",
      "263:\tlearn: 0.2156322\ttotal: 4.52s\tremaining: 12.6s\n",
      "264:\tlearn: 0.2156003\ttotal: 4.54s\tremaining: 12.6s\n",
      "265:\tlearn: 0.2155507\ttotal: 4.56s\tremaining: 12.6s\n",
      "266:\tlearn: 0.2155063\ttotal: 4.57s\tremaining: 12.6s\n",
      "267:\tlearn: 0.2154673\ttotal: 4.59s\tremaining: 12.5s\n",
      "268:\tlearn: 0.2154411\ttotal: 4.61s\tremaining: 12.5s\n",
      "269:\tlearn: 0.2153925\ttotal: 4.62s\tremaining: 12.5s\n",
      "270:\tlearn: 0.2153589\ttotal: 4.64s\tremaining: 12.5s\n",
      "271:\tlearn: 0.2153297\ttotal: 4.66s\tremaining: 12.5s\n",
      "272:\tlearn: 0.2152940\ttotal: 4.67s\tremaining: 12.4s\n",
      "273:\tlearn: 0.2152763\ttotal: 4.69s\tremaining: 12.4s\n",
      "274:\tlearn: 0.2152537\ttotal: 4.71s\tremaining: 12.4s\n",
      "275:\tlearn: 0.2152399\ttotal: 4.73s\tremaining: 12.4s\n",
      "276:\tlearn: 0.2151757\ttotal: 4.74s\tremaining: 12.4s\n",
      "277:\tlearn: 0.2151473\ttotal: 4.76s\tremaining: 12.4s\n",
      "278:\tlearn: 0.2151191\ttotal: 4.78s\tremaining: 12.3s\n",
      "279:\tlearn: 0.2150784\ttotal: 4.8s\tremaining: 12.3s\n",
      "280:\tlearn: 0.2150459\ttotal: 4.81s\tremaining: 12.3s\n",
      "281:\tlearn: 0.2150322\ttotal: 4.83s\tremaining: 12.3s\n",
      "282:\tlearn: 0.2149741\ttotal: 4.85s\tremaining: 12.3s\n",
      "283:\tlearn: 0.2149277\ttotal: 4.86s\tremaining: 12.3s\n",
      "284:\tlearn: 0.2148941\ttotal: 4.88s\tremaining: 12.2s\n",
      "285:\tlearn: 0.2148762\ttotal: 4.9s\tremaining: 12.2s\n",
      "286:\tlearn: 0.2148539\ttotal: 4.92s\tremaining: 12.2s\n",
      "287:\tlearn: 0.2148198\ttotal: 4.94s\tremaining: 12.2s\n",
      "288:\tlearn: 0.2147982\ttotal: 4.95s\tremaining: 12.2s\n",
      "289:\tlearn: 0.2147854\ttotal: 4.97s\tremaining: 12.2s\n",
      "290:\tlearn: 0.2147479\ttotal: 4.99s\tremaining: 12.2s\n",
      "291:\tlearn: 0.2147162\ttotal: 5s\tremaining: 12.1s\n",
      "292:\tlearn: 0.2146855\ttotal: 5.02s\tremaining: 12.1s\n",
      "293:\tlearn: 0.2146589\ttotal: 5.04s\tremaining: 12.1s\n",
      "294:\tlearn: 0.2146347\ttotal: 5.05s\tremaining: 12.1s\n",
      "295:\tlearn: 0.2146092\ttotal: 5.07s\tremaining: 12.1s\n",
      "296:\tlearn: 0.2145618\ttotal: 5.09s\tremaining: 12s\n",
      "297:\tlearn: 0.2145441\ttotal: 5.11s\tremaining: 12s\n",
      "298:\tlearn: 0.2145103\ttotal: 5.12s\tremaining: 12s\n",
      "299:\tlearn: 0.2144729\ttotal: 5.14s\tremaining: 12s\n",
      "300:\tlearn: 0.2144495\ttotal: 5.16s\tremaining: 12s\n",
      "301:\tlearn: 0.2144135\ttotal: 5.17s\tremaining: 12s\n",
      "302:\tlearn: 0.2143648\ttotal: 5.19s\tremaining: 11.9s\n",
      "303:\tlearn: 0.2143355\ttotal: 5.21s\tremaining: 11.9s\n",
      "304:\tlearn: 0.2142727\ttotal: 5.22s\tremaining: 11.9s\n",
      "305:\tlearn: 0.2142369\ttotal: 5.24s\tremaining: 11.9s\n",
      "306:\tlearn: 0.2142169\ttotal: 5.26s\tremaining: 11.9s\n",
      "307:\tlearn: 0.2141953\ttotal: 5.27s\tremaining: 11.8s\n",
      "308:\tlearn: 0.2141735\ttotal: 5.29s\tremaining: 11.8s\n",
      "309:\tlearn: 0.2141416\ttotal: 5.31s\tremaining: 11.8s\n",
      "310:\tlearn: 0.2140927\ttotal: 5.33s\tremaining: 11.8s\n",
      "311:\tlearn: 0.2140678\ttotal: 5.34s\tremaining: 11.8s\n",
      "312:\tlearn: 0.2140428\ttotal: 5.36s\tremaining: 11.8s\n",
      "313:\tlearn: 0.2140078\ttotal: 5.38s\tremaining: 11.8s\n",
      "314:\tlearn: 0.2139787\ttotal: 5.4s\tremaining: 11.7s\n",
      "315:\tlearn: 0.2139294\ttotal: 5.41s\tremaining: 11.7s\n",
      "316:\tlearn: 0.2139105\ttotal: 5.43s\tremaining: 11.7s\n",
      "317:\tlearn: 0.2138913\ttotal: 5.45s\tremaining: 11.7s\n",
      "318:\tlearn: 0.2138709\ttotal: 5.47s\tremaining: 11.7s\n",
      "319:\tlearn: 0.2138489\ttotal: 5.48s\tremaining: 11.7s\n",
      "320:\tlearn: 0.2138180\ttotal: 5.5s\tremaining: 11.6s\n",
      "321:\tlearn: 0.2137922\ttotal: 5.52s\tremaining: 11.6s\n",
      "322:\tlearn: 0.2137562\ttotal: 5.54s\tremaining: 11.6s\n",
      "323:\tlearn: 0.2137148\ttotal: 5.55s\tremaining: 11.6s\n",
      "324:\tlearn: 0.2136748\ttotal: 5.57s\tremaining: 11.6s\n",
      "325:\tlearn: 0.2136361\ttotal: 5.59s\tremaining: 11.6s\n",
      "326:\tlearn: 0.2136093\ttotal: 5.61s\tremaining: 11.5s\n",
      "327:\tlearn: 0.2135874\ttotal: 5.62s\tremaining: 11.5s\n",
      "328:\tlearn: 0.2135633\ttotal: 5.64s\tremaining: 11.5s\n",
      "329:\tlearn: 0.2135379\ttotal: 5.66s\tremaining: 11.5s\n",
      "330:\tlearn: 0.2135181\ttotal: 5.67s\tremaining: 11.5s\n",
      "331:\tlearn: 0.2134740\ttotal: 5.69s\tremaining: 11.4s\n",
      "332:\tlearn: 0.2134580\ttotal: 5.71s\tremaining: 11.4s\n",
      "333:\tlearn: 0.2134293\ttotal: 5.72s\tremaining: 11.4s\n",
      "334:\tlearn: 0.2133736\ttotal: 5.74s\tremaining: 11.4s\n",
      "335:\tlearn: 0.2133351\ttotal: 5.76s\tremaining: 11.4s\n",
      "336:\tlearn: 0.2132990\ttotal: 5.78s\tremaining: 11.4s\n",
      "337:\tlearn: 0.2132677\ttotal: 5.79s\tremaining: 11.4s\n",
      "338:\tlearn: 0.2132264\ttotal: 5.81s\tremaining: 11.3s\n",
      "339:\tlearn: 0.2131709\ttotal: 5.83s\tremaining: 11.3s\n",
      "340:\tlearn: 0.2131484\ttotal: 5.85s\tremaining: 11.3s\n",
      "341:\tlearn: 0.2131256\ttotal: 5.86s\tremaining: 11.3s\n",
      "342:\tlearn: 0.2130966\ttotal: 5.88s\tremaining: 11.3s\n",
      "343:\tlearn: 0.2130422\ttotal: 5.9s\tremaining: 11.2s\n",
      "344:\tlearn: 0.2130100\ttotal: 5.91s\tremaining: 11.2s\n",
      "345:\tlearn: 0.2129822\ttotal: 5.93s\tremaining: 11.2s\n",
      "346:\tlearn: 0.2129517\ttotal: 5.95s\tremaining: 11.2s\n",
      "347:\tlearn: 0.2129140\ttotal: 5.97s\tremaining: 11.2s\n",
      "348:\tlearn: 0.2128771\ttotal: 5.99s\tremaining: 11.2s\n",
      "349:\tlearn: 0.2128417\ttotal: 6.01s\tremaining: 11.2s\n",
      "350:\tlearn: 0.2128073\ttotal: 6.03s\tremaining: 11.2s\n",
      "351:\tlearn: 0.2127811\ttotal: 6.05s\tremaining: 11.1s\n",
      "352:\tlearn: 0.2127543\ttotal: 6.07s\tremaining: 11.1s\n",
      "353:\tlearn: 0.2127195\ttotal: 6.09s\tremaining: 11.1s\n",
      "354:\tlearn: 0.2126983\ttotal: 6.11s\tremaining: 11.1s\n",
      "355:\tlearn: 0.2126826\ttotal: 6.13s\tremaining: 11.1s\n",
      "356:\tlearn: 0.2126538\ttotal: 6.14s\tremaining: 11.1s\n",
      "357:\tlearn: 0.2126112\ttotal: 6.16s\tremaining: 11.1s\n",
      "358:\tlearn: 0.2125809\ttotal: 6.18s\tremaining: 11s\n",
      "359:\tlearn: 0.2125465\ttotal: 6.2s\tremaining: 11s\n",
      "360:\tlearn: 0.2124948\ttotal: 6.22s\tremaining: 11s\n",
      "361:\tlearn: 0.2124658\ttotal: 6.24s\tremaining: 11s\n",
      "362:\tlearn: 0.2124305\ttotal: 6.26s\tremaining: 11s\n",
      "363:\tlearn: 0.2123935\ttotal: 6.28s\tremaining: 11s\n",
      "364:\tlearn: 0.2123687\ttotal: 6.3s\tremaining: 11s\n",
      "365:\tlearn: 0.2123378\ttotal: 6.31s\tremaining: 10.9s\n",
      "366:\tlearn: 0.2123184\ttotal: 6.33s\tremaining: 10.9s\n",
      "367:\tlearn: 0.2122633\ttotal: 6.35s\tremaining: 10.9s\n",
      "368:\tlearn: 0.2122220\ttotal: 6.36s\tremaining: 10.9s\n",
      "369:\tlearn: 0.2121809\ttotal: 6.38s\tremaining: 10.9s\n",
      "370:\tlearn: 0.2121500\ttotal: 6.4s\tremaining: 10.8s\n",
      "371:\tlearn: 0.2121277\ttotal: 6.42s\tremaining: 10.8s\n",
      "372:\tlearn: 0.2121002\ttotal: 6.43s\tremaining: 10.8s\n",
      "373:\tlearn: 0.2120753\ttotal: 6.45s\tremaining: 10.8s\n",
      "374:\tlearn: 0.2120502\ttotal: 6.47s\tremaining: 10.8s\n",
      "375:\tlearn: 0.2120213\ttotal: 6.49s\tremaining: 10.8s\n",
      "376:\tlearn: 0.2119873\ttotal: 6.5s\tremaining: 10.7s\n",
      "377:\tlearn: 0.2119529\ttotal: 6.52s\tremaining: 10.7s\n",
      "378:\tlearn: 0.2119103\ttotal: 6.54s\tremaining: 10.7s\n",
      "379:\tlearn: 0.2118825\ttotal: 6.55s\tremaining: 10.7s\n",
      "380:\tlearn: 0.2118545\ttotal: 6.57s\tremaining: 10.7s\n",
      "381:\tlearn: 0.2118258\ttotal: 6.59s\tremaining: 10.7s\n",
      "382:\tlearn: 0.2117869\ttotal: 6.61s\tremaining: 10.6s\n",
      "383:\tlearn: 0.2117606\ttotal: 6.62s\tremaining: 10.6s\n",
      "384:\tlearn: 0.2117383\ttotal: 6.64s\tremaining: 10.6s\n",
      "385:\tlearn: 0.2117041\ttotal: 6.66s\tremaining: 10.6s\n",
      "386:\tlearn: 0.2116770\ttotal: 6.67s\tremaining: 10.6s\n",
      "387:\tlearn: 0.2116552\ttotal: 6.69s\tremaining: 10.6s\n",
      "388:\tlearn: 0.2116246\ttotal: 6.71s\tremaining: 10.5s\n",
      "389:\tlearn: 0.2116002\ttotal: 6.73s\tremaining: 10.5s\n",
      "390:\tlearn: 0.2115686\ttotal: 6.74s\tremaining: 10.5s\n",
      "391:\tlearn: 0.2115302\ttotal: 6.76s\tremaining: 10.5s\n",
      "392:\tlearn: 0.2115104\ttotal: 6.78s\tremaining: 10.5s\n",
      "393:\tlearn: 0.2114878\ttotal: 6.79s\tremaining: 10.4s\n",
      "394:\tlearn: 0.2114707\ttotal: 6.81s\tremaining: 10.4s\n",
      "395:\tlearn: 0.2114472\ttotal: 6.83s\tremaining: 10.4s\n",
      "396:\tlearn: 0.2113930\ttotal: 6.85s\tremaining: 10.4s\n",
      "397:\tlearn: 0.2113739\ttotal: 6.86s\tremaining: 10.4s\n",
      "398:\tlearn: 0.2113496\ttotal: 6.88s\tremaining: 10.4s\n",
      "399:\tlearn: 0.2113338\ttotal: 6.9s\tremaining: 10.3s\n",
      "400:\tlearn: 0.2113010\ttotal: 6.91s\tremaining: 10.3s\n",
      "401:\tlearn: 0.2112800\ttotal: 6.93s\tremaining: 10.3s\n",
      "402:\tlearn: 0.2112577\ttotal: 6.95s\tremaining: 10.3s\n",
      "403:\tlearn: 0.2112447\ttotal: 6.97s\tremaining: 10.3s\n",
      "404:\tlearn: 0.2112270\ttotal: 6.98s\tremaining: 10.3s\n",
      "405:\tlearn: 0.2111927\ttotal: 7s\tremaining: 10.2s\n",
      "406:\tlearn: 0.2111577\ttotal: 7.02s\tremaining: 10.2s\n",
      "407:\tlearn: 0.2111358\ttotal: 7.04s\tremaining: 10.2s\n",
      "408:\tlearn: 0.2111237\ttotal: 7.05s\tremaining: 10.2s\n",
      "409:\tlearn: 0.2111115\ttotal: 7.07s\tremaining: 10.2s\n",
      "410:\tlearn: 0.2110964\ttotal: 7.09s\tremaining: 10.2s\n",
      "411:\tlearn: 0.2110640\ttotal: 7.1s\tremaining: 10.1s\n",
      "412:\tlearn: 0.2110289\ttotal: 7.12s\tremaining: 10.1s\n",
      "413:\tlearn: 0.2109892\ttotal: 7.14s\tremaining: 10.1s\n",
      "414:\tlearn: 0.2109744\ttotal: 7.15s\tremaining: 10.1s\n",
      "415:\tlearn: 0.2109492\ttotal: 7.17s\tremaining: 10.1s\n",
      "416:\tlearn: 0.2109340\ttotal: 7.19s\tremaining: 10s\n",
      "417:\tlearn: 0.2109029\ttotal: 7.2s\tremaining: 10s\n",
      "418:\tlearn: 0.2108736\ttotal: 7.22s\tremaining: 10s\n",
      "419:\tlearn: 0.2108471\ttotal: 7.24s\tremaining: 10s\n",
      "420:\tlearn: 0.2108142\ttotal: 7.26s\tremaining: 9.98s\n",
      "421:\tlearn: 0.2107998\ttotal: 7.27s\tremaining: 9.96s\n",
      "422:\tlearn: 0.2107509\ttotal: 7.29s\tremaining: 9.95s\n",
      "423:\tlearn: 0.2107235\ttotal: 7.31s\tremaining: 9.93s\n",
      "424:\tlearn: 0.2106924\ttotal: 7.33s\tremaining: 9.91s\n",
      "425:\tlearn: 0.2106682\ttotal: 7.34s\tremaining: 9.89s\n",
      "426:\tlearn: 0.2106491\ttotal: 7.36s\tremaining: 9.88s\n",
      "427:\tlearn: 0.2106279\ttotal: 7.38s\tremaining: 9.86s\n",
      "428:\tlearn: 0.2106137\ttotal: 7.4s\tremaining: 9.85s\n",
      "429:\tlearn: 0.2105961\ttotal: 7.42s\tremaining: 9.83s\n",
      "430:\tlearn: 0.2105719\ttotal: 7.43s\tremaining: 9.81s\n",
      "431:\tlearn: 0.2105574\ttotal: 7.45s\tremaining: 9.8s\n",
      "432:\tlearn: 0.2105291\ttotal: 7.47s\tremaining: 9.79s\n",
      "433:\tlearn: 0.2105088\ttotal: 7.49s\tremaining: 9.77s\n",
      "434:\tlearn: 0.2104917\ttotal: 7.51s\tremaining: 9.75s\n",
      "435:\tlearn: 0.2104738\ttotal: 7.53s\tremaining: 9.73s\n",
      "436:\tlearn: 0.2104427\ttotal: 7.54s\tremaining: 9.72s\n",
      "437:\tlearn: 0.2104146\ttotal: 7.56s\tremaining: 9.7s\n",
      "438:\tlearn: 0.2104008\ttotal: 7.58s\tremaining: 9.68s\n",
      "439:\tlearn: 0.2103719\ttotal: 7.59s\tremaining: 9.66s\n",
      "440:\tlearn: 0.2103552\ttotal: 7.61s\tremaining: 9.64s\n",
      "441:\tlearn: 0.2103090\ttotal: 7.63s\tremaining: 9.63s\n",
      "442:\tlearn: 0.2102795\ttotal: 7.64s\tremaining: 9.61s\n",
      "443:\tlearn: 0.2102510\ttotal: 7.66s\tremaining: 9.59s\n",
      "444:\tlearn: 0.2102144\ttotal: 7.68s\tremaining: 9.58s\n",
      "445:\tlearn: 0.2101962\ttotal: 7.69s\tremaining: 9.56s\n",
      "446:\tlearn: 0.2101679\ttotal: 7.71s\tremaining: 9.54s\n",
      "447:\tlearn: 0.2101421\ttotal: 7.73s\tremaining: 9.52s\n",
      "448:\tlearn: 0.2101215\ttotal: 7.75s\tremaining: 9.51s\n",
      "449:\tlearn: 0.2100993\ttotal: 7.76s\tremaining: 9.49s\n",
      "450:\tlearn: 0.2100530\ttotal: 7.78s\tremaining: 9.47s\n",
      "451:\tlearn: 0.2100351\ttotal: 7.8s\tremaining: 9.45s\n",
      "452:\tlearn: 0.2100170\ttotal: 7.81s\tremaining: 9.44s\n",
      "453:\tlearn: 0.2099874\ttotal: 7.83s\tremaining: 9.42s\n",
      "454:\tlearn: 0.2099717\ttotal: 7.85s\tremaining: 9.4s\n",
      "455:\tlearn: 0.2099585\ttotal: 7.87s\tremaining: 9.38s\n",
      "456:\tlearn: 0.2099130\ttotal: 7.88s\tremaining: 9.37s\n",
      "457:\tlearn: 0.2098725\ttotal: 7.9s\tremaining: 9.35s\n",
      "458:\tlearn: 0.2098530\ttotal: 7.92s\tremaining: 9.33s\n",
      "459:\tlearn: 0.2098261\ttotal: 7.94s\tremaining: 9.32s\n",
      "460:\tlearn: 0.2097990\ttotal: 7.95s\tremaining: 9.3s\n",
      "461:\tlearn: 0.2097646\ttotal: 7.97s\tremaining: 9.28s\n",
      "462:\tlearn: 0.2097480\ttotal: 7.99s\tremaining: 9.26s\n",
      "463:\tlearn: 0.2097290\ttotal: 8s\tremaining: 9.25s\n",
      "464:\tlearn: 0.2097001\ttotal: 8.02s\tremaining: 9.23s\n",
      "465:\tlearn: 0.2096852\ttotal: 8.04s\tremaining: 9.21s\n",
      "466:\tlearn: 0.2096709\ttotal: 8.05s\tremaining: 9.19s\n",
      "467:\tlearn: 0.2096317\ttotal: 8.07s\tremaining: 9.18s\n",
      "468:\tlearn: 0.2095970\ttotal: 8.09s\tremaining: 9.16s\n",
      "469:\tlearn: 0.2095721\ttotal: 8.11s\tremaining: 9.14s\n",
      "470:\tlearn: 0.2095518\ttotal: 8.13s\tremaining: 9.13s\n",
      "471:\tlearn: 0.2095345\ttotal: 8.14s\tremaining: 9.11s\n",
      "472:\tlearn: 0.2095073\ttotal: 8.16s\tremaining: 9.09s\n",
      "473:\tlearn: 0.2094841\ttotal: 8.18s\tremaining: 9.07s\n",
      "474:\tlearn: 0.2094642\ttotal: 8.19s\tremaining: 9.06s\n",
      "475:\tlearn: 0.2094542\ttotal: 8.21s\tremaining: 9.04s\n",
      "476:\tlearn: 0.2094366\ttotal: 8.23s\tremaining: 9.02s\n",
      "477:\tlearn: 0.2094203\ttotal: 8.24s\tremaining: 9s\n",
      "478:\tlearn: 0.2093962\ttotal: 8.26s\tremaining: 8.98s\n",
      "479:\tlearn: 0.2093535\ttotal: 8.28s\tremaining: 8.97s\n",
      "480:\tlearn: 0.2093300\ttotal: 8.3s\tremaining: 8.95s\n",
      "481:\tlearn: 0.2092989\ttotal: 8.31s\tremaining: 8.93s\n",
      "482:\tlearn: 0.2092670\ttotal: 8.33s\tremaining: 8.92s\n",
      "483:\tlearn: 0.2092406\ttotal: 8.35s\tremaining: 8.9s\n",
      "484:\tlearn: 0.2092112\ttotal: 8.37s\tremaining: 8.88s\n",
      "485:\tlearn: 0.2091903\ttotal: 8.38s\tremaining: 8.87s\n",
      "486:\tlearn: 0.2091620\ttotal: 8.4s\tremaining: 8.85s\n",
      "487:\tlearn: 0.2091349\ttotal: 8.42s\tremaining: 8.83s\n",
      "488:\tlearn: 0.2091198\ttotal: 8.43s\tremaining: 8.81s\n",
      "489:\tlearn: 0.2090863\ttotal: 8.45s\tremaining: 8.8s\n",
      "490:\tlearn: 0.2090656\ttotal: 8.47s\tremaining: 8.78s\n",
      "491:\tlearn: 0.2090262\ttotal: 8.49s\tremaining: 8.76s\n",
      "492:\tlearn: 0.2090007\ttotal: 8.5s\tremaining: 8.74s\n",
      "493:\tlearn: 0.2089832\ttotal: 8.52s\tremaining: 8.73s\n",
      "494:\tlearn: 0.2089602\ttotal: 8.54s\tremaining: 8.71s\n",
      "495:\tlearn: 0.2089302\ttotal: 8.56s\tremaining: 8.69s\n",
      "496:\tlearn: 0.2089107\ttotal: 8.57s\tremaining: 8.68s\n",
      "497:\tlearn: 0.2088919\ttotal: 8.59s\tremaining: 8.66s\n",
      "498:\tlearn: 0.2088456\ttotal: 8.61s\tremaining: 8.64s\n",
      "499:\tlearn: 0.2088134\ttotal: 8.63s\tremaining: 8.63s\n",
      "500:\tlearn: 0.2087796\ttotal: 8.64s\tremaining: 8.61s\n",
      "501:\tlearn: 0.2087492\ttotal: 8.66s\tremaining: 8.59s\n",
      "502:\tlearn: 0.2087189\ttotal: 8.68s\tremaining: 8.57s\n",
      "503:\tlearn: 0.2086873\ttotal: 8.7s\tremaining: 8.56s\n",
      "504:\tlearn: 0.2086639\ttotal: 8.71s\tremaining: 8.54s\n",
      "505:\tlearn: 0.2086494\ttotal: 8.73s\tremaining: 8.52s\n",
      "506:\tlearn: 0.2086262\ttotal: 8.75s\tremaining: 8.51s\n",
      "507:\tlearn: 0.2085992\ttotal: 8.76s\tremaining: 8.49s\n",
      "508:\tlearn: 0.2085721\ttotal: 8.78s\tremaining: 8.47s\n",
      "509:\tlearn: 0.2085424\ttotal: 8.8s\tremaining: 8.45s\n",
      "510:\tlearn: 0.2085157\ttotal: 8.81s\tremaining: 8.43s\n",
      "511:\tlearn: 0.2084849\ttotal: 8.83s\tremaining: 8.42s\n",
      "512:\tlearn: 0.2084607\ttotal: 8.85s\tremaining: 8.4s\n",
      "513:\tlearn: 0.2084445\ttotal: 8.86s\tremaining: 8.38s\n",
      "514:\tlearn: 0.2084304\ttotal: 8.88s\tremaining: 8.36s\n",
      "515:\tlearn: 0.2084093\ttotal: 8.9s\tremaining: 8.35s\n",
      "516:\tlearn: 0.2083797\ttotal: 8.91s\tremaining: 8.33s\n",
      "517:\tlearn: 0.2083474\ttotal: 8.93s\tremaining: 8.31s\n",
      "518:\tlearn: 0.2083202\ttotal: 8.95s\tremaining: 8.3s\n",
      "519:\tlearn: 0.2082951\ttotal: 8.97s\tremaining: 8.28s\n",
      "520:\tlearn: 0.2082665\ttotal: 8.99s\tremaining: 8.26s\n",
      "521:\tlearn: 0.2082538\ttotal: 9s\tremaining: 8.24s\n",
      "522:\tlearn: 0.2082152\ttotal: 9.02s\tremaining: 8.23s\n",
      "523:\tlearn: 0.2082009\ttotal: 9.04s\tremaining: 8.21s\n",
      "524:\tlearn: 0.2081733\ttotal: 9.05s\tremaining: 8.19s\n",
      "525:\tlearn: 0.2081546\ttotal: 9.07s\tremaining: 8.17s\n",
      "526:\tlearn: 0.2081366\ttotal: 9.09s\tremaining: 8.15s\n",
      "527:\tlearn: 0.2081087\ttotal: 9.1s\tremaining: 8.14s\n",
      "528:\tlearn: 0.2080909\ttotal: 9.12s\tremaining: 8.12s\n",
      "529:\tlearn: 0.2080672\ttotal: 9.14s\tremaining: 8.1s\n",
      "530:\tlearn: 0.2080492\ttotal: 9.16s\tremaining: 8.09s\n",
      "531:\tlearn: 0.2080243\ttotal: 9.18s\tremaining: 8.07s\n",
      "532:\tlearn: 0.2080108\ttotal: 9.19s\tremaining: 8.05s\n",
      "533:\tlearn: 0.2079809\ttotal: 9.21s\tremaining: 8.04s\n",
      "534:\tlearn: 0.2079450\ttotal: 9.23s\tremaining: 8.02s\n",
      "535:\tlearn: 0.2079356\ttotal: 9.24s\tremaining: 8s\n",
      "536:\tlearn: 0.2079155\ttotal: 9.26s\tremaining: 7.98s\n",
      "537:\tlearn: 0.2078811\ttotal: 9.28s\tremaining: 7.97s\n",
      "538:\tlearn: 0.2078480\ttotal: 9.29s\tremaining: 7.95s\n",
      "539:\tlearn: 0.2078390\ttotal: 9.31s\tremaining: 7.93s\n",
      "540:\tlearn: 0.2078228\ttotal: 9.33s\tremaining: 7.91s\n",
      "541:\tlearn: 0.2078018\ttotal: 9.34s\tremaining: 7.9s\n",
      "542:\tlearn: 0.2077867\ttotal: 9.36s\tremaining: 7.88s\n",
      "543:\tlearn: 0.2077572\ttotal: 9.38s\tremaining: 7.86s\n",
      "544:\tlearn: 0.2077428\ttotal: 9.4s\tremaining: 7.84s\n",
      "545:\tlearn: 0.2077171\ttotal: 9.41s\tremaining: 7.83s\n",
      "546:\tlearn: 0.2076839\ttotal: 9.43s\tremaining: 7.81s\n",
      "547:\tlearn: 0.2076732\ttotal: 9.45s\tremaining: 7.79s\n",
      "548:\tlearn: 0.2076600\ttotal: 9.47s\tremaining: 7.78s\n",
      "549:\tlearn: 0.2076261\ttotal: 9.48s\tremaining: 7.76s\n",
      "550:\tlearn: 0.2076006\ttotal: 9.5s\tremaining: 7.74s\n",
      "551:\tlearn: 0.2075881\ttotal: 9.52s\tremaining: 7.72s\n",
      "552:\tlearn: 0.2075513\ttotal: 9.54s\tremaining: 7.71s\n",
      "553:\tlearn: 0.2075395\ttotal: 9.55s\tremaining: 7.69s\n",
      "554:\tlearn: 0.2075274\ttotal: 9.57s\tremaining: 7.67s\n",
      "555:\tlearn: 0.2075166\ttotal: 9.59s\tremaining: 7.66s\n",
      "556:\tlearn: 0.2074911\ttotal: 9.6s\tremaining: 7.64s\n",
      "557:\tlearn: 0.2074797\ttotal: 9.62s\tremaining: 7.62s\n",
      "558:\tlearn: 0.2074543\ttotal: 9.64s\tremaining: 7.6s\n",
      "559:\tlearn: 0.2074106\ttotal: 9.65s\tremaining: 7.59s\n",
      "560:\tlearn: 0.2073791\ttotal: 9.67s\tremaining: 7.57s\n",
      "561:\tlearn: 0.2073486\ttotal: 9.69s\tremaining: 7.55s\n",
      "562:\tlearn: 0.2073378\ttotal: 9.71s\tremaining: 7.53s\n",
      "563:\tlearn: 0.2073143\ttotal: 9.72s\tremaining: 7.51s\n",
      "564:\tlearn: 0.2072797\ttotal: 9.74s\tremaining: 7.5s\n",
      "565:\tlearn: 0.2072546\ttotal: 9.76s\tremaining: 7.48s\n",
      "566:\tlearn: 0.2072265\ttotal: 9.77s\tremaining: 7.46s\n",
      "567:\tlearn: 0.2071964\ttotal: 9.79s\tremaining: 7.45s\n",
      "568:\tlearn: 0.2071859\ttotal: 9.81s\tremaining: 7.43s\n",
      "569:\tlearn: 0.2071687\ttotal: 9.83s\tremaining: 7.41s\n",
      "570:\tlearn: 0.2071504\ttotal: 9.84s\tremaining: 7.39s\n",
      "571:\tlearn: 0.2071402\ttotal: 9.86s\tremaining: 7.38s\n",
      "572:\tlearn: 0.2071175\ttotal: 9.88s\tremaining: 7.36s\n",
      "573:\tlearn: 0.2070735\ttotal: 9.89s\tremaining: 7.34s\n",
      "574:\tlearn: 0.2070515\ttotal: 9.91s\tremaining: 7.33s\n",
      "575:\tlearn: 0.2070416\ttotal: 9.93s\tremaining: 7.31s\n",
      "576:\tlearn: 0.2070297\ttotal: 9.94s\tremaining: 7.29s\n",
      "577:\tlearn: 0.2069912\ttotal: 9.96s\tremaining: 7.27s\n",
      "578:\tlearn: 0.2069698\ttotal: 9.98s\tremaining: 7.26s\n",
      "579:\tlearn: 0.2069421\ttotal: 10s\tremaining: 7.24s\n",
      "580:\tlearn: 0.2069208\ttotal: 10s\tremaining: 7.22s\n",
      "581:\tlearn: 0.2068848\ttotal: 10s\tremaining: 7.21s\n",
      "582:\tlearn: 0.2068617\ttotal: 10.1s\tremaining: 7.19s\n",
      "583:\tlearn: 0.2068502\ttotal: 10.1s\tremaining: 7.17s\n",
      "584:\tlearn: 0.2068405\ttotal: 10.1s\tremaining: 7.16s\n",
      "585:\tlearn: 0.2068088\ttotal: 10.1s\tremaining: 7.14s\n",
      "586:\tlearn: 0.2067913\ttotal: 10.1s\tremaining: 7.12s\n",
      "587:\tlearn: 0.2067722\ttotal: 10.1s\tremaining: 7.1s\n",
      "588:\tlearn: 0.2067421\ttotal: 10.2s\tremaining: 7.08s\n",
      "589:\tlearn: 0.2067166\ttotal: 10.2s\tremaining: 7.07s\n",
      "590:\tlearn: 0.2066937\ttotal: 10.2s\tremaining: 7.05s\n",
      "591:\tlearn: 0.2066509\ttotal: 10.2s\tremaining: 7.03s\n",
      "592:\tlearn: 0.2066323\ttotal: 10.2s\tremaining: 7.01s\n",
      "593:\tlearn: 0.2066148\ttotal: 10.2s\tremaining: 7s\n",
      "594:\tlearn: 0.2065884\ttotal: 10.3s\tremaining: 6.98s\n",
      "595:\tlearn: 0.2065678\ttotal: 10.3s\tremaining: 6.96s\n",
      "596:\tlearn: 0.2065276\ttotal: 10.3s\tremaining: 6.95s\n",
      "597:\tlearn: 0.2065017\ttotal: 10.3s\tremaining: 6.93s\n",
      "598:\tlearn: 0.2064846\ttotal: 10.3s\tremaining: 6.91s\n",
      "599:\tlearn: 0.2064757\ttotal: 10.3s\tremaining: 6.89s\n",
      "600:\tlearn: 0.2064541\ttotal: 10.4s\tremaining: 6.88s\n",
      "601:\tlearn: 0.2064438\ttotal: 10.4s\tremaining: 6.86s\n",
      "602:\tlearn: 0.2064276\ttotal: 10.4s\tremaining: 6.84s\n",
      "603:\tlearn: 0.2064087\ttotal: 10.4s\tremaining: 6.83s\n",
      "604:\tlearn: 0.2063743\ttotal: 10.4s\tremaining: 6.81s\n",
      "605:\tlearn: 0.2063526\ttotal: 10.4s\tremaining: 6.79s\n",
      "606:\tlearn: 0.2063353\ttotal: 10.5s\tremaining: 6.77s\n",
      "607:\tlearn: 0.2062996\ttotal: 10.5s\tremaining: 6.75s\n",
      "608:\tlearn: 0.2062743\ttotal: 10.5s\tremaining: 6.74s\n",
      "609:\tlearn: 0.2062536\ttotal: 10.5s\tremaining: 6.72s\n",
      "610:\tlearn: 0.2062369\ttotal: 10.5s\tremaining: 6.7s\n",
      "611:\tlearn: 0.2062129\ttotal: 10.5s\tremaining: 6.68s\n",
      "612:\tlearn: 0.2061932\ttotal: 10.6s\tremaining: 6.67s\n",
      "613:\tlearn: 0.2061828\ttotal: 10.6s\tremaining: 6.65s\n",
      "614:\tlearn: 0.2061476\ttotal: 10.6s\tremaining: 6.63s\n",
      "615:\tlearn: 0.2061305\ttotal: 10.6s\tremaining: 6.62s\n",
      "616:\tlearn: 0.2060864\ttotal: 10.6s\tremaining: 6.6s\n",
      "617:\tlearn: 0.2060673\ttotal: 10.7s\tremaining: 6.58s\n",
      "618:\tlearn: 0.2060448\ttotal: 10.7s\tremaining: 6.57s\n",
      "619:\tlearn: 0.2060298\ttotal: 10.7s\tremaining: 6.55s\n",
      "620:\tlearn: 0.2059979\ttotal: 10.7s\tremaining: 6.53s\n",
      "621:\tlearn: 0.2059612\ttotal: 10.7s\tremaining: 6.51s\n",
      "622:\tlearn: 0.2059338\ttotal: 10.7s\tremaining: 6.5s\n",
      "623:\tlearn: 0.2059107\ttotal: 10.8s\tremaining: 6.48s\n",
      "624:\tlearn: 0.2058797\ttotal: 10.8s\tremaining: 6.46s\n",
      "625:\tlearn: 0.2058703\ttotal: 10.8s\tremaining: 6.45s\n",
      "626:\tlearn: 0.2058605\ttotal: 10.8s\tremaining: 6.43s\n",
      "627:\tlearn: 0.2058436\ttotal: 10.8s\tremaining: 6.41s\n",
      "628:\tlearn: 0.2058160\ttotal: 10.8s\tremaining: 6.39s\n",
      "629:\tlearn: 0.2057895\ttotal: 10.9s\tremaining: 6.38s\n",
      "630:\tlearn: 0.2057497\ttotal: 10.9s\tremaining: 6.36s\n",
      "631:\tlearn: 0.2057189\ttotal: 10.9s\tremaining: 6.34s\n",
      "632:\tlearn: 0.2057048\ttotal: 10.9s\tremaining: 6.33s\n",
      "633:\tlearn: 0.2056904\ttotal: 10.9s\tremaining: 6.31s\n",
      "634:\tlearn: 0.2056490\ttotal: 10.9s\tremaining: 6.29s\n",
      "635:\tlearn: 0.2056326\ttotal: 11s\tremaining: 6.27s\n",
      "636:\tlearn: 0.2055928\ttotal: 11s\tremaining: 6.25s\n",
      "637:\tlearn: 0.2055610\ttotal: 11s\tremaining: 6.24s\n",
      "638:\tlearn: 0.2055451\ttotal: 11s\tremaining: 6.22s\n",
      "639:\tlearn: 0.2055228\ttotal: 11s\tremaining: 6.2s\n",
      "640:\tlearn: 0.2054933\ttotal: 11s\tremaining: 6.18s\n",
      "641:\tlearn: 0.2054528\ttotal: 11.1s\tremaining: 6.17s\n",
      "642:\tlearn: 0.2054202\ttotal: 11.1s\tremaining: 6.15s\n",
      "643:\tlearn: 0.2053877\ttotal: 11.1s\tremaining: 6.13s\n",
      "644:\tlearn: 0.2053640\ttotal: 11.1s\tremaining: 6.12s\n",
      "645:\tlearn: 0.2053413\ttotal: 11.1s\tremaining: 6.1s\n",
      "646:\tlearn: 0.2053006\ttotal: 11.1s\tremaining: 6.08s\n",
      "647:\tlearn: 0.2052924\ttotal: 11.2s\tremaining: 6.07s\n",
      "648:\tlearn: 0.2052556\ttotal: 11.2s\tremaining: 6.05s\n",
      "649:\tlearn: 0.2052068\ttotal: 11.2s\tremaining: 6.03s\n",
      "650:\tlearn: 0.2051899\ttotal: 11.2s\tremaining: 6.01s\n",
      "651:\tlearn: 0.2051746\ttotal: 11.2s\tremaining: 6s\n",
      "652:\tlearn: 0.2051359\ttotal: 11.3s\tremaining: 5.98s\n",
      "653:\tlearn: 0.2050904\ttotal: 11.3s\tremaining: 5.96s\n",
      "654:\tlearn: 0.2050758\ttotal: 11.3s\tremaining: 5.95s\n",
      "655:\tlearn: 0.2050538\ttotal: 11.3s\tremaining: 5.93s\n",
      "656:\tlearn: 0.2050189\ttotal: 11.3s\tremaining: 5.91s\n",
      "657:\tlearn: 0.2049915\ttotal: 11.3s\tremaining: 5.89s\n",
      "658:\tlearn: 0.2049726\ttotal: 11.4s\tremaining: 5.88s\n",
      "659:\tlearn: 0.2049442\ttotal: 11.4s\tremaining: 5.86s\n",
      "660:\tlearn: 0.2049178\ttotal: 11.4s\tremaining: 5.84s\n",
      "661:\tlearn: 0.2048983\ttotal: 11.4s\tremaining: 5.83s\n",
      "662:\tlearn: 0.2048752\ttotal: 11.4s\tremaining: 5.81s\n",
      "663:\tlearn: 0.2048599\ttotal: 11.4s\tremaining: 5.79s\n",
      "664:\tlearn: 0.2048454\ttotal: 11.5s\tremaining: 5.77s\n",
      "665:\tlearn: 0.2048064\ttotal: 11.5s\tremaining: 5.76s\n",
      "666:\tlearn: 0.2048005\ttotal: 11.5s\tremaining: 5.74s\n",
      "667:\tlearn: 0.2047686\ttotal: 11.5s\tremaining: 5.72s\n",
      "668:\tlearn: 0.2047491\ttotal: 11.5s\tremaining: 5.71s\n",
      "669:\tlearn: 0.2047355\ttotal: 11.5s\tremaining: 5.69s\n",
      "670:\tlearn: 0.2046978\ttotal: 11.6s\tremaining: 5.67s\n",
      "671:\tlearn: 0.2046626\ttotal: 11.6s\tremaining: 5.65s\n",
      "672:\tlearn: 0.2046388\ttotal: 11.6s\tremaining: 5.64s\n",
      "673:\tlearn: 0.2046152\ttotal: 11.6s\tremaining: 5.62s\n",
      "674:\tlearn: 0.2045897\ttotal: 11.6s\tremaining: 5.6s\n",
      "675:\tlearn: 0.2045771\ttotal: 11.7s\tremaining: 5.58s\n",
      "676:\tlearn: 0.2045460\ttotal: 11.7s\tremaining: 5.57s\n",
      "677:\tlearn: 0.2045210\ttotal: 11.7s\tremaining: 5.55s\n",
      "678:\tlearn: 0.2044972\ttotal: 11.7s\tremaining: 5.54s\n",
      "679:\tlearn: 0.2044573\ttotal: 11.7s\tremaining: 5.52s\n",
      "680:\tlearn: 0.2044269\ttotal: 11.7s\tremaining: 5.5s\n",
      "681:\tlearn: 0.2044085\ttotal: 11.8s\tremaining: 5.49s\n",
      "682:\tlearn: 0.2043972\ttotal: 11.8s\tremaining: 5.47s\n",
      "683:\tlearn: 0.2043661\ttotal: 11.8s\tremaining: 5.45s\n",
      "684:\tlearn: 0.2043476\ttotal: 11.8s\tremaining: 5.43s\n",
      "685:\tlearn: 0.2043310\ttotal: 11.8s\tremaining: 5.42s\n",
      "686:\tlearn: 0.2043031\ttotal: 11.8s\tremaining: 5.4s\n",
      "687:\tlearn: 0.2042766\ttotal: 11.9s\tremaining: 5.38s\n",
      "688:\tlearn: 0.2042568\ttotal: 11.9s\tremaining: 5.37s\n",
      "689:\tlearn: 0.2042338\ttotal: 11.9s\tremaining: 5.35s\n",
      "690:\tlearn: 0.2042283\ttotal: 11.9s\tremaining: 5.33s\n",
      "691:\tlearn: 0.2042110\ttotal: 11.9s\tremaining: 5.31s\n",
      "692:\tlearn: 0.2041927\ttotal: 12s\tremaining: 5.3s\n",
      "693:\tlearn: 0.2041750\ttotal: 12s\tremaining: 5.28s\n",
      "694:\tlearn: 0.2041584\ttotal: 12s\tremaining: 5.26s\n",
      "695:\tlearn: 0.2041252\ttotal: 12s\tremaining: 5.24s\n",
      "696:\tlearn: 0.2041025\ttotal: 12s\tremaining: 5.23s\n",
      "697:\tlearn: 0.2040814\ttotal: 12s\tremaining: 5.21s\n",
      "698:\tlearn: 0.2040619\ttotal: 12.1s\tremaining: 5.19s\n",
      "699:\tlearn: 0.2040391\ttotal: 12.1s\tremaining: 5.17s\n",
      "700:\tlearn: 0.2040274\ttotal: 12.1s\tremaining: 5.16s\n",
      "701:\tlearn: 0.2040010\ttotal: 12.1s\tremaining: 5.14s\n",
      "702:\tlearn: 0.2039663\ttotal: 12.1s\tremaining: 5.12s\n",
      "703:\tlearn: 0.2039566\ttotal: 12.1s\tremaining: 5.1s\n",
      "704:\tlearn: 0.2039514\ttotal: 12.2s\tremaining: 5.09s\n",
      "705:\tlearn: 0.2039389\ttotal: 12.2s\tremaining: 5.07s\n",
      "706:\tlearn: 0.2039145\ttotal: 12.2s\tremaining: 5.05s\n",
      "707:\tlearn: 0.2038798\ttotal: 12.2s\tremaining: 5.04s\n",
      "708:\tlearn: 0.2038440\ttotal: 12.2s\tremaining: 5.02s\n",
      "709:\tlearn: 0.2038235\ttotal: 12.2s\tremaining: 5s\n",
      "710:\tlearn: 0.2037938\ttotal: 12.3s\tremaining: 4.98s\n",
      "711:\tlearn: 0.2037782\ttotal: 12.3s\tremaining: 4.96s\n",
      "712:\tlearn: 0.2037555\ttotal: 12.3s\tremaining: 4.95s\n",
      "713:\tlearn: 0.2037230\ttotal: 12.3s\tremaining: 4.93s\n",
      "714:\tlearn: 0.2036804\ttotal: 12.3s\tremaining: 4.91s\n",
      "715:\tlearn: 0.2036636\ttotal: 12.3s\tremaining: 4.9s\n",
      "716:\tlearn: 0.2036478\ttotal: 12.4s\tremaining: 4.88s\n",
      "717:\tlearn: 0.2036213\ttotal: 12.4s\tremaining: 4.86s\n",
      "718:\tlearn: 0.2035932\ttotal: 12.4s\tremaining: 4.84s\n",
      "719:\tlearn: 0.2035619\ttotal: 12.4s\tremaining: 4.83s\n",
      "720:\tlearn: 0.2035283\ttotal: 12.4s\tremaining: 4.81s\n",
      "721:\tlearn: 0.2035163\ttotal: 12.4s\tremaining: 4.79s\n",
      "722:\tlearn: 0.2034963\ttotal: 12.5s\tremaining: 4.78s\n",
      "723:\tlearn: 0.2034580\ttotal: 12.5s\tremaining: 4.76s\n",
      "724:\tlearn: 0.2034302\ttotal: 12.5s\tremaining: 4.74s\n",
      "725:\tlearn: 0.2033962\ttotal: 12.5s\tremaining: 4.72s\n",
      "726:\tlearn: 0.2033779\ttotal: 12.5s\tremaining: 4.71s\n",
      "727:\tlearn: 0.2033615\ttotal: 12.5s\tremaining: 4.69s\n",
      "728:\tlearn: 0.2033362\ttotal: 12.6s\tremaining: 4.67s\n",
      "729:\tlearn: 0.2033162\ttotal: 12.6s\tremaining: 4.65s\n",
      "730:\tlearn: 0.2032848\ttotal: 12.6s\tremaining: 4.64s\n",
      "731:\tlearn: 0.2032642\ttotal: 12.6s\tremaining: 4.62s\n",
      "732:\tlearn: 0.2032435\ttotal: 12.6s\tremaining: 4.6s\n",
      "733:\tlearn: 0.2032285\ttotal: 12.7s\tremaining: 4.58s\n",
      "734:\tlearn: 0.2032082\ttotal: 12.7s\tremaining: 4.57s\n",
      "735:\tlearn: 0.2031878\ttotal: 12.7s\tremaining: 4.55s\n",
      "736:\tlearn: 0.2031617\ttotal: 12.7s\tremaining: 4.53s\n",
      "737:\tlearn: 0.2031385\ttotal: 12.7s\tremaining: 4.51s\n",
      "738:\tlearn: 0.2031257\ttotal: 12.7s\tremaining: 4.5s\n",
      "739:\tlearn: 0.2030898\ttotal: 12.8s\tremaining: 4.48s\n",
      "740:\tlearn: 0.2030682\ttotal: 12.8s\tremaining: 4.46s\n",
      "741:\tlearn: 0.2030507\ttotal: 12.8s\tremaining: 4.45s\n",
      "742:\tlearn: 0.2030257\ttotal: 12.8s\tremaining: 4.43s\n",
      "743:\tlearn: 0.2029925\ttotal: 12.8s\tremaining: 4.41s\n",
      "744:\tlearn: 0.2029791\ttotal: 12.8s\tremaining: 4.4s\n",
      "745:\tlearn: 0.2029490\ttotal: 12.9s\tremaining: 4.38s\n",
      "746:\tlearn: 0.2029301\ttotal: 12.9s\tremaining: 4.36s\n",
      "747:\tlearn: 0.2029141\ttotal: 12.9s\tremaining: 4.34s\n",
      "748:\tlearn: 0.2029024\ttotal: 12.9s\tremaining: 4.33s\n",
      "749:\tlearn: 0.2028672\ttotal: 12.9s\tremaining: 4.31s\n",
      "750:\tlearn: 0.2028525\ttotal: 12.9s\tremaining: 4.29s\n",
      "751:\tlearn: 0.2028303\ttotal: 13s\tremaining: 4.27s\n",
      "752:\tlearn: 0.2027982\ttotal: 13s\tremaining: 4.26s\n",
      "753:\tlearn: 0.2027687\ttotal: 13s\tremaining: 4.24s\n",
      "754:\tlearn: 0.2027445\ttotal: 13s\tremaining: 4.22s\n",
      "755:\tlearn: 0.2027099\ttotal: 13s\tremaining: 4.21s\n",
      "756:\tlearn: 0.2026805\ttotal: 13s\tremaining: 4.19s\n",
      "757:\tlearn: 0.2026543\ttotal: 13.1s\tremaining: 4.17s\n",
      "758:\tlearn: 0.2026352\ttotal: 13.1s\tremaining: 4.15s\n",
      "759:\tlearn: 0.2025947\ttotal: 13.1s\tremaining: 4.14s\n",
      "760:\tlearn: 0.2025693\ttotal: 13.1s\tremaining: 4.12s\n",
      "761:\tlearn: 0.2025591\ttotal: 13.1s\tremaining: 4.1s\n",
      "762:\tlearn: 0.2025408\ttotal: 13.2s\tremaining: 4.08s\n",
      "763:\tlearn: 0.2025277\ttotal: 13.2s\tremaining: 4.07s\n",
      "764:\tlearn: 0.2025041\ttotal: 13.2s\tremaining: 4.05s\n",
      "765:\tlearn: 0.2024801\ttotal: 13.2s\tremaining: 4.03s\n",
      "766:\tlearn: 0.2024695\ttotal: 13.2s\tremaining: 4.02s\n",
      "767:\tlearn: 0.2024491\ttotal: 13.2s\tremaining: 4s\n",
      "768:\tlearn: 0.2024373\ttotal: 13.3s\tremaining: 3.98s\n",
      "769:\tlearn: 0.2024256\ttotal: 13.3s\tremaining: 3.96s\n",
      "770:\tlearn: 0.2023992\ttotal: 13.3s\tremaining: 3.95s\n",
      "771:\tlearn: 0.2023765\ttotal: 13.3s\tremaining: 3.93s\n",
      "772:\tlearn: 0.2023678\ttotal: 13.3s\tremaining: 3.91s\n",
      "773:\tlearn: 0.2023569\ttotal: 13.3s\tremaining: 3.89s\n",
      "774:\tlearn: 0.2023304\ttotal: 13.4s\tremaining: 3.88s\n",
      "775:\tlearn: 0.2023189\ttotal: 13.4s\tremaining: 3.86s\n",
      "776:\tlearn: 0.2022978\ttotal: 13.4s\tremaining: 3.84s\n",
      "777:\tlearn: 0.2022651\ttotal: 13.4s\tremaining: 3.83s\n",
      "778:\tlearn: 0.2022398\ttotal: 13.4s\tremaining: 3.81s\n",
      "779:\tlearn: 0.2022131\ttotal: 13.4s\tremaining: 3.79s\n",
      "780:\tlearn: 0.2021900\ttotal: 13.5s\tremaining: 3.77s\n",
      "781:\tlearn: 0.2021793\ttotal: 13.5s\tremaining: 3.76s\n",
      "782:\tlearn: 0.2021499\ttotal: 13.5s\tremaining: 3.74s\n",
      "783:\tlearn: 0.2021323\ttotal: 13.5s\tremaining: 3.72s\n",
      "784:\tlearn: 0.2021135\ttotal: 13.5s\tremaining: 3.71s\n",
      "785:\tlearn: 0.2021047\ttotal: 13.5s\tremaining: 3.69s\n",
      "786:\tlearn: 0.2020703\ttotal: 13.6s\tremaining: 3.67s\n",
      "787:\tlearn: 0.2020445\ttotal: 13.6s\tremaining: 3.65s\n",
      "788:\tlearn: 0.2020362\ttotal: 13.6s\tremaining: 3.64s\n",
      "789:\tlearn: 0.2020104\ttotal: 13.6s\tremaining: 3.62s\n",
      "790:\tlearn: 0.2019971\ttotal: 13.6s\tremaining: 3.6s\n",
      "791:\tlearn: 0.2019873\ttotal: 13.6s\tremaining: 3.58s\n",
      "792:\tlearn: 0.2019640\ttotal: 13.7s\tremaining: 3.57s\n",
      "793:\tlearn: 0.2019521\ttotal: 13.7s\tremaining: 3.55s\n",
      "794:\tlearn: 0.2019346\ttotal: 13.7s\tremaining: 3.53s\n",
      "795:\tlearn: 0.2019091\ttotal: 13.7s\tremaining: 3.52s\n",
      "796:\tlearn: 0.2018872\ttotal: 13.7s\tremaining: 3.5s\n",
      "797:\tlearn: 0.2018759\ttotal: 13.8s\tremaining: 3.48s\n",
      "798:\tlearn: 0.2018538\ttotal: 13.8s\tremaining: 3.46s\n",
      "799:\tlearn: 0.2018437\ttotal: 13.8s\tremaining: 3.45s\n",
      "800:\tlearn: 0.2018228\ttotal: 13.8s\tremaining: 3.43s\n",
      "801:\tlearn: 0.2018134\ttotal: 13.8s\tremaining: 3.41s\n",
      "802:\tlearn: 0.2017954\ttotal: 13.8s\tremaining: 3.39s\n",
      "803:\tlearn: 0.2017580\ttotal: 13.9s\tremaining: 3.38s\n",
      "804:\tlearn: 0.2017279\ttotal: 13.9s\tremaining: 3.36s\n",
      "805:\tlearn: 0.2017037\ttotal: 13.9s\tremaining: 3.34s\n",
      "806:\tlearn: 0.2016912\ttotal: 13.9s\tremaining: 3.33s\n",
      "807:\tlearn: 0.2016677\ttotal: 13.9s\tremaining: 3.31s\n",
      "808:\tlearn: 0.2016546\ttotal: 13.9s\tremaining: 3.29s\n",
      "809:\tlearn: 0.2016312\ttotal: 14s\tremaining: 3.27s\n",
      "810:\tlearn: 0.2016216\ttotal: 14s\tremaining: 3.26s\n",
      "811:\tlearn: 0.2016123\ttotal: 14s\tremaining: 3.24s\n",
      "812:\tlearn: 0.2015813\ttotal: 14s\tremaining: 3.22s\n",
      "813:\tlearn: 0.2015616\ttotal: 14s\tremaining: 3.21s\n",
      "814:\tlearn: 0.2015291\ttotal: 14s\tremaining: 3.19s\n",
      "815:\tlearn: 0.2015011\ttotal: 14.1s\tremaining: 3.17s\n",
      "816:\tlearn: 0.2014802\ttotal: 14.1s\tremaining: 3.15s\n",
      "817:\tlearn: 0.2014568\ttotal: 14.1s\tremaining: 3.14s\n",
      "818:\tlearn: 0.2014353\ttotal: 14.1s\tremaining: 3.12s\n",
      "819:\tlearn: 0.2014119\ttotal: 14.1s\tremaining: 3.1s\n",
      "820:\tlearn: 0.2013874\ttotal: 14.2s\tremaining: 3.08s\n",
      "821:\tlearn: 0.2013671\ttotal: 14.2s\tremaining: 3.07s\n",
      "822:\tlearn: 0.2013532\ttotal: 14.2s\tremaining: 3.05s\n",
      "823:\tlearn: 0.2013394\ttotal: 14.2s\tremaining: 3.03s\n",
      "824:\tlearn: 0.2013207\ttotal: 14.2s\tremaining: 3.02s\n",
      "825:\tlearn: 0.2012939\ttotal: 14.2s\tremaining: 3s\n",
      "826:\tlearn: 0.2012757\ttotal: 14.3s\tremaining: 2.98s\n",
      "827:\tlearn: 0.2012465\ttotal: 14.3s\tremaining: 2.96s\n",
      "828:\tlearn: 0.2012115\ttotal: 14.3s\tremaining: 2.95s\n",
      "829:\tlearn: 0.2011961\ttotal: 14.3s\tremaining: 2.93s\n",
      "830:\tlearn: 0.2011514\ttotal: 14.3s\tremaining: 2.91s\n",
      "831:\tlearn: 0.2011246\ttotal: 14.3s\tremaining: 2.9s\n",
      "832:\tlearn: 0.2010951\ttotal: 14.4s\tremaining: 2.88s\n",
      "833:\tlearn: 0.2010864\ttotal: 14.4s\tremaining: 2.86s\n",
      "834:\tlearn: 0.2010554\ttotal: 14.4s\tremaining: 2.84s\n",
      "835:\tlearn: 0.2010447\ttotal: 14.4s\tremaining: 2.83s\n",
      "836:\tlearn: 0.2010358\ttotal: 14.4s\tremaining: 2.81s\n",
      "837:\tlearn: 0.2010007\ttotal: 14.4s\tremaining: 2.79s\n",
      "838:\tlearn: 0.2009792\ttotal: 14.5s\tremaining: 2.77s\n",
      "839:\tlearn: 0.2009467\ttotal: 14.5s\tremaining: 2.76s\n",
      "840:\tlearn: 0.2009177\ttotal: 14.5s\tremaining: 2.74s\n",
      "841:\tlearn: 0.2008969\ttotal: 14.5s\tremaining: 2.72s\n",
      "842:\tlearn: 0.2008845\ttotal: 14.5s\tremaining: 2.71s\n",
      "843:\tlearn: 0.2008676\ttotal: 14.5s\tremaining: 2.69s\n",
      "844:\tlearn: 0.2008468\ttotal: 14.6s\tremaining: 2.67s\n",
      "845:\tlearn: 0.2008057\ttotal: 14.6s\tremaining: 2.65s\n",
      "846:\tlearn: 0.2007867\ttotal: 14.6s\tremaining: 2.64s\n",
      "847:\tlearn: 0.2007504\ttotal: 14.6s\tremaining: 2.62s\n",
      "848:\tlearn: 0.2007334\ttotal: 14.6s\tremaining: 2.6s\n",
      "849:\tlearn: 0.2007138\ttotal: 14.6s\tremaining: 2.58s\n",
      "850:\tlearn: 0.2006958\ttotal: 14.7s\tremaining: 2.57s\n",
      "851:\tlearn: 0.2006541\ttotal: 14.7s\tremaining: 2.55s\n",
      "852:\tlearn: 0.2006360\ttotal: 14.7s\tremaining: 2.53s\n",
      "853:\tlearn: 0.2006006\ttotal: 14.7s\tremaining: 2.52s\n",
      "854:\tlearn: 0.2005775\ttotal: 14.7s\tremaining: 2.5s\n",
      "855:\tlearn: 0.2005579\ttotal: 14.8s\tremaining: 2.48s\n",
      "856:\tlearn: 0.2005304\ttotal: 14.8s\tremaining: 2.46s\n",
      "857:\tlearn: 0.2005135\ttotal: 14.8s\tremaining: 2.45s\n",
      "858:\tlearn: 0.2004935\ttotal: 14.8s\tremaining: 2.43s\n",
      "859:\tlearn: 0.2004536\ttotal: 14.8s\tremaining: 2.41s\n",
      "860:\tlearn: 0.2004112\ttotal: 14.8s\tremaining: 2.4s\n",
      "861:\tlearn: 0.2003828\ttotal: 14.9s\tremaining: 2.38s\n",
      "862:\tlearn: 0.2003449\ttotal: 14.9s\tremaining: 2.36s\n",
      "863:\tlearn: 0.2003048\ttotal: 14.9s\tremaining: 2.34s\n",
      "864:\tlearn: 0.2002693\ttotal: 14.9s\tremaining: 2.33s\n",
      "865:\tlearn: 0.2002375\ttotal: 14.9s\tremaining: 2.31s\n",
      "866:\tlearn: 0.2002006\ttotal: 14.9s\tremaining: 2.29s\n",
      "867:\tlearn: 0.2001751\ttotal: 15s\tremaining: 2.27s\n",
      "868:\tlearn: 0.2001540\ttotal: 15s\tremaining: 2.26s\n",
      "869:\tlearn: 0.2001254\ttotal: 15s\tremaining: 2.24s\n",
      "870:\tlearn: 0.2001079\ttotal: 15s\tremaining: 2.22s\n",
      "871:\tlearn: 0.2000846\ttotal: 15s\tremaining: 2.21s\n",
      "872:\tlearn: 0.2000707\ttotal: 15s\tremaining: 2.19s\n",
      "873:\tlearn: 0.2000410\ttotal: 15.1s\tremaining: 2.17s\n",
      "874:\tlearn: 0.2000249\ttotal: 15.1s\tremaining: 2.15s\n",
      "875:\tlearn: 0.1999860\ttotal: 15.1s\tremaining: 2.14s\n",
      "876:\tlearn: 0.1999659\ttotal: 15.1s\tremaining: 2.12s\n",
      "877:\tlearn: 0.1999416\ttotal: 15.1s\tremaining: 2.1s\n",
      "878:\tlearn: 0.1999206\ttotal: 15.1s\tremaining: 2.08s\n",
      "879:\tlearn: 0.1999035\ttotal: 15.2s\tremaining: 2.07s\n",
      "880:\tlearn: 0.1998672\ttotal: 15.2s\tremaining: 2.05s\n",
      "881:\tlearn: 0.1998473\ttotal: 15.2s\tremaining: 2.03s\n",
      "882:\tlearn: 0.1998375\ttotal: 15.2s\tremaining: 2.02s\n",
      "883:\tlearn: 0.1998179\ttotal: 15.2s\tremaining: 2s\n",
      "884:\tlearn: 0.1997839\ttotal: 15.3s\tremaining: 1.98s\n",
      "885:\tlearn: 0.1997438\ttotal: 15.3s\tremaining: 1.96s\n",
      "886:\tlearn: 0.1997240\ttotal: 15.3s\tremaining: 1.95s\n",
      "887:\tlearn: 0.1996947\ttotal: 15.3s\tremaining: 1.93s\n",
      "888:\tlearn: 0.1996765\ttotal: 15.3s\tremaining: 1.91s\n",
      "889:\tlearn: 0.1996484\ttotal: 15.3s\tremaining: 1.9s\n",
      "890:\tlearn: 0.1996409\ttotal: 15.4s\tremaining: 1.88s\n",
      "891:\tlearn: 0.1996321\ttotal: 15.4s\tremaining: 1.86s\n",
      "892:\tlearn: 0.1995939\ttotal: 15.4s\tremaining: 1.84s\n",
      "893:\tlearn: 0.1995847\ttotal: 15.4s\tremaining: 1.83s\n",
      "894:\tlearn: 0.1995542\ttotal: 15.4s\tremaining: 1.81s\n",
      "895:\tlearn: 0.1995265\ttotal: 15.4s\tremaining: 1.79s\n",
      "896:\tlearn: 0.1994952\ttotal: 15.5s\tremaining: 1.77s\n",
      "897:\tlearn: 0.1994617\ttotal: 15.5s\tremaining: 1.76s\n",
      "898:\tlearn: 0.1994427\ttotal: 15.5s\tremaining: 1.74s\n",
      "899:\tlearn: 0.1994285\ttotal: 15.5s\tremaining: 1.72s\n",
      "900:\tlearn: 0.1994076\ttotal: 15.5s\tremaining: 1.71s\n",
      "901:\tlearn: 0.1993896\ttotal: 15.5s\tremaining: 1.69s\n",
      "902:\tlearn: 0.1993798\ttotal: 15.6s\tremaining: 1.67s\n",
      "903:\tlearn: 0.1993628\ttotal: 15.6s\tremaining: 1.65s\n",
      "904:\tlearn: 0.1993477\ttotal: 15.6s\tremaining: 1.64s\n",
      "905:\tlearn: 0.1993238\ttotal: 15.6s\tremaining: 1.62s\n",
      "906:\tlearn: 0.1992901\ttotal: 15.6s\tremaining: 1.6s\n",
      "907:\tlearn: 0.1992718\ttotal: 15.6s\tremaining: 1.58s\n",
      "908:\tlearn: 0.1992473\ttotal: 15.7s\tremaining: 1.57s\n",
      "909:\tlearn: 0.1992269\ttotal: 15.7s\tremaining: 1.55s\n",
      "910:\tlearn: 0.1992056\ttotal: 15.7s\tremaining: 1.53s\n",
      "911:\tlearn: 0.1991846\ttotal: 15.7s\tremaining: 1.52s\n",
      "912:\tlearn: 0.1991772\ttotal: 15.7s\tremaining: 1.5s\n",
      "913:\tlearn: 0.1991595\ttotal: 15.8s\tremaining: 1.48s\n",
      "914:\tlearn: 0.1991427\ttotal: 15.8s\tremaining: 1.46s\n",
      "915:\tlearn: 0.1991268\ttotal: 15.8s\tremaining: 1.45s\n",
      "916:\tlearn: 0.1991152\ttotal: 15.8s\tremaining: 1.43s\n",
      "917:\tlearn: 0.1990858\ttotal: 15.8s\tremaining: 1.41s\n",
      "918:\tlearn: 0.1990662\ttotal: 15.8s\tremaining: 1.4s\n",
      "919:\tlearn: 0.1990356\ttotal: 15.9s\tremaining: 1.38s\n",
      "920:\tlearn: 0.1990100\ttotal: 15.9s\tremaining: 1.36s\n",
      "921:\tlearn: 0.1989961\ttotal: 15.9s\tremaining: 1.34s\n",
      "922:\tlearn: 0.1989641\ttotal: 15.9s\tremaining: 1.33s\n",
      "923:\tlearn: 0.1989429\ttotal: 15.9s\tremaining: 1.31s\n",
      "924:\tlearn: 0.1989275\ttotal: 15.9s\tremaining: 1.29s\n",
      "925:\tlearn: 0.1989148\ttotal: 16s\tremaining: 1.27s\n",
      "926:\tlearn: 0.1989065\ttotal: 16s\tremaining: 1.26s\n",
      "927:\tlearn: 0.1988928\ttotal: 16s\tremaining: 1.24s\n",
      "928:\tlearn: 0.1988683\ttotal: 16s\tremaining: 1.22s\n",
      "929:\tlearn: 0.1988506\ttotal: 16s\tremaining: 1.21s\n",
      "930:\tlearn: 0.1988335\ttotal: 16s\tremaining: 1.19s\n",
      "931:\tlearn: 0.1988068\ttotal: 16.1s\tremaining: 1.17s\n",
      "932:\tlearn: 0.1987857\ttotal: 16.1s\tremaining: 1.16s\n",
      "933:\tlearn: 0.1987652\ttotal: 16.1s\tremaining: 1.14s\n",
      "934:\tlearn: 0.1987475\ttotal: 16.1s\tremaining: 1.12s\n",
      "935:\tlearn: 0.1987354\ttotal: 16.1s\tremaining: 1.1s\n",
      "936:\tlearn: 0.1987142\ttotal: 16.1s\tremaining: 1.08s\n",
      "937:\tlearn: 0.1986869\ttotal: 16.2s\tremaining: 1.07s\n",
      "938:\tlearn: 0.1986737\ttotal: 16.2s\tremaining: 1.05s\n",
      "939:\tlearn: 0.1986455\ttotal: 16.2s\tremaining: 1.03s\n",
      "940:\tlearn: 0.1986228\ttotal: 16.2s\tremaining: 1.02s\n",
      "941:\tlearn: 0.1986061\ttotal: 16.2s\tremaining: 1000ms\n",
      "942:\tlearn: 0.1985968\ttotal: 16.3s\tremaining: 982ms\n",
      "943:\tlearn: 0.1985835\ttotal: 16.3s\tremaining: 965ms\n",
      "944:\tlearn: 0.1985580\ttotal: 16.3s\tremaining: 948ms\n",
      "945:\tlearn: 0.1985342\ttotal: 16.3s\tremaining: 931ms\n",
      "946:\tlearn: 0.1985087\ttotal: 16.3s\tremaining: 914ms\n",
      "947:\tlearn: 0.1984933\ttotal: 16.3s\tremaining: 896ms\n",
      "948:\tlearn: 0.1984719\ttotal: 16.4s\tremaining: 879ms\n",
      "949:\tlearn: 0.1984621\ttotal: 16.4s\tremaining: 862ms\n",
      "950:\tlearn: 0.1984552\ttotal: 16.4s\tremaining: 845ms\n",
      "951:\tlearn: 0.1984242\ttotal: 16.4s\tremaining: 827ms\n",
      "952:\tlearn: 0.1983940\ttotal: 16.4s\tremaining: 810ms\n",
      "953:\tlearn: 0.1983664\ttotal: 16.4s\tremaining: 793ms\n",
      "954:\tlearn: 0.1983461\ttotal: 16.5s\tremaining: 776ms\n",
      "955:\tlearn: 0.1983314\ttotal: 16.5s\tremaining: 759ms\n",
      "956:\tlearn: 0.1983109\ttotal: 16.5s\tremaining: 741ms\n",
      "957:\tlearn: 0.1983008\ttotal: 16.5s\tremaining: 724ms\n",
      "958:\tlearn: 0.1982841\ttotal: 16.5s\tremaining: 707ms\n",
      "959:\tlearn: 0.1982756\ttotal: 16.5s\tremaining: 689ms\n",
      "960:\tlearn: 0.1982591\ttotal: 16.6s\tremaining: 672ms\n",
      "961:\tlearn: 0.1982328\ttotal: 16.6s\tremaining: 655ms\n",
      "962:\tlearn: 0.1982205\ttotal: 16.6s\tremaining: 638ms\n",
      "963:\tlearn: 0.1982088\ttotal: 16.6s\tremaining: 620ms\n",
      "964:\tlearn: 0.1981897\ttotal: 16.6s\tremaining: 603ms\n",
      "965:\tlearn: 0.1981685\ttotal: 16.6s\tremaining: 586ms\n",
      "966:\tlearn: 0.1981442\ttotal: 16.7s\tremaining: 569ms\n",
      "967:\tlearn: 0.1981376\ttotal: 16.7s\tremaining: 552ms\n",
      "968:\tlearn: 0.1981134\ttotal: 16.7s\tremaining: 534ms\n",
      "969:\tlearn: 0.1980956\ttotal: 16.7s\tremaining: 517ms\n",
      "970:\tlearn: 0.1980761\ttotal: 16.7s\tremaining: 500ms\n",
      "971:\tlearn: 0.1980577\ttotal: 16.8s\tremaining: 483ms\n",
      "972:\tlearn: 0.1980361\ttotal: 16.8s\tremaining: 465ms\n",
      "973:\tlearn: 0.1980065\ttotal: 16.8s\tremaining: 448ms\n",
      "974:\tlearn: 0.1979798\ttotal: 16.8s\tremaining: 431ms\n",
      "975:\tlearn: 0.1979671\ttotal: 16.8s\tremaining: 414ms\n",
      "976:\tlearn: 0.1979389\ttotal: 16.8s\tremaining: 396ms\n",
      "977:\tlearn: 0.1978950\ttotal: 16.9s\tremaining: 379ms\n",
      "978:\tlearn: 0.1978888\ttotal: 16.9s\tremaining: 362ms\n",
      "979:\tlearn: 0.1978501\ttotal: 16.9s\tremaining: 345ms\n",
      "980:\tlearn: 0.1978218\ttotal: 16.9s\tremaining: 327ms\n",
      "981:\tlearn: 0.1977971\ttotal: 16.9s\tremaining: 310ms\n",
      "982:\tlearn: 0.1977847\ttotal: 16.9s\tremaining: 293ms\n",
      "983:\tlearn: 0.1977650\ttotal: 17s\tremaining: 276ms\n",
      "984:\tlearn: 0.1977330\ttotal: 17s\tremaining: 259ms\n",
      "985:\tlearn: 0.1977184\ttotal: 17s\tremaining: 241ms\n",
      "986:\tlearn: 0.1977003\ttotal: 17s\tremaining: 224ms\n",
      "987:\tlearn: 0.1976624\ttotal: 17s\tremaining: 207ms\n",
      "988:\tlearn: 0.1976435\ttotal: 17s\tremaining: 190ms\n",
      "989:\tlearn: 0.1976282\ttotal: 17.1s\tremaining: 172ms\n",
      "990:\tlearn: 0.1976019\ttotal: 17.1s\tremaining: 155ms\n",
      "991:\tlearn: 0.1975804\ttotal: 17.1s\tremaining: 138ms\n",
      "992:\tlearn: 0.1975582\ttotal: 17.1s\tremaining: 121ms\n",
      "993:\tlearn: 0.1975388\ttotal: 17.1s\tremaining: 103ms\n",
      "994:\tlearn: 0.1975159\ttotal: 17.2s\tremaining: 86.2ms\n",
      "995:\tlearn: 0.1974954\ttotal: 17.2s\tremaining: 69ms\n",
      "996:\tlearn: 0.1974805\ttotal: 17.2s\tremaining: 51.7ms\n",
      "997:\tlearn: 0.1974589\ttotal: 17.2s\tremaining: 34.5ms\n",
      "998:\tlearn: 0.1974397\ttotal: 17.2s\tremaining: 17.2ms\n",
      "999:\tlearn: 0.1974128\ttotal: 17.2s\tremaining: 0us\n",
      "0:\tlearn: 0.9237244\ttotal: 19.5ms\tremaining: 19.5s\n",
      "1:\tlearn: 0.8564696\ttotal: 36.4ms\tremaining: 18.2s\n",
      "2:\tlearn: 0.7974458\ttotal: 52.7ms\tremaining: 17.5s\n",
      "3:\tlearn: 0.7445755\ttotal: 68.9ms\tremaining: 17.2s\n",
      "4:\tlearn: 0.6972601\ttotal: 84.5ms\tremaining: 16.8s\n",
      "5:\tlearn: 0.6537815\ttotal: 101ms\tremaining: 16.7s\n",
      "6:\tlearn: 0.6160082\ttotal: 117ms\tremaining: 16.6s\n",
      "7:\tlearn: 0.5820470\ttotal: 132ms\tremaining: 16.4s\n",
      "8:\tlearn: 0.5516116\ttotal: 148ms\tremaining: 16.3s\n",
      "9:\tlearn: 0.5238659\ttotal: 165ms\tremaining: 16.3s\n",
      "10:\tlearn: 0.4987109\ttotal: 182ms\tremaining: 16.3s\n",
      "11:\tlearn: 0.4759909\ttotal: 199ms\tremaining: 16.4s\n",
      "12:\tlearn: 0.4551870\ttotal: 217ms\tremaining: 16.5s\n",
      "13:\tlearn: 0.4367700\ttotal: 233ms\tremaining: 16.4s\n",
      "14:\tlearn: 0.4198137\ttotal: 251ms\tremaining: 16.5s\n",
      "15:\tlearn: 0.4047301\ttotal: 266ms\tremaining: 16.4s\n",
      "16:\tlearn: 0.3909599\ttotal: 285ms\tremaining: 16.5s\n",
      "17:\tlearn: 0.3784354\ttotal: 303ms\tremaining: 16.6s\n",
      "18:\tlearn: 0.3668480\ttotal: 321ms\tremaining: 16.6s\n",
      "19:\tlearn: 0.3558611\ttotal: 338ms\tremaining: 16.6s\n",
      "20:\tlearn: 0.3463237\ttotal: 355ms\tremaining: 16.5s\n",
      "21:\tlearn: 0.3370559\ttotal: 372ms\tremaining: 16.5s\n",
      "22:\tlearn: 0.3290749\ttotal: 388ms\tremaining: 16.5s\n",
      "23:\tlearn: 0.3217542\ttotal: 405ms\tremaining: 16.5s\n",
      "24:\tlearn: 0.3150144\ttotal: 421ms\tremaining: 16.4s\n",
      "25:\tlearn: 0.3088071\ttotal: 437ms\tremaining: 16.4s\n",
      "26:\tlearn: 0.3029476\ttotal: 454ms\tremaining: 16.3s\n",
      "27:\tlearn: 0.2977348\ttotal: 471ms\tremaining: 16.3s\n",
      "28:\tlearn: 0.2929623\ttotal: 489ms\tremaining: 16.4s\n",
      "29:\tlearn: 0.2884805\ttotal: 508ms\tremaining: 16.4s\n",
      "30:\tlearn: 0.2840754\ttotal: 524ms\tremaining: 16.4s\n",
      "31:\tlearn: 0.2802431\ttotal: 541ms\tremaining: 16.4s\n",
      "32:\tlearn: 0.2767525\ttotal: 558ms\tremaining: 16.3s\n",
      "33:\tlearn: 0.2734321\ttotal: 574ms\tremaining: 16.3s\n",
      "34:\tlearn: 0.2703954\ttotal: 591ms\tremaining: 16.3s\n",
      "35:\tlearn: 0.2675157\ttotal: 608ms\tremaining: 16.3s\n",
      "36:\tlearn: 0.2646672\ttotal: 625ms\tremaining: 16.3s\n",
      "37:\tlearn: 0.2620968\ttotal: 639ms\tremaining: 16.2s\n",
      "38:\tlearn: 0.2598671\ttotal: 656ms\tremaining: 16.2s\n",
      "39:\tlearn: 0.2576948\ttotal: 673ms\tremaining: 16.2s\n",
      "40:\tlearn: 0.2557811\ttotal: 690ms\tremaining: 16.1s\n",
      "41:\tlearn: 0.2539964\ttotal: 707ms\tremaining: 16.1s\n",
      "42:\tlearn: 0.2519920\ttotal: 725ms\tremaining: 16.1s\n",
      "43:\tlearn: 0.2504672\ttotal: 742ms\tremaining: 16.1s\n",
      "44:\tlearn: 0.2489048\ttotal: 760ms\tremaining: 16.1s\n",
      "45:\tlearn: 0.2475296\ttotal: 777ms\tremaining: 16.1s\n",
      "46:\tlearn: 0.2461675\ttotal: 794ms\tremaining: 16.1s\n",
      "47:\tlearn: 0.2449547\ttotal: 812ms\tremaining: 16.1s\n",
      "48:\tlearn: 0.2437406\ttotal: 829ms\tremaining: 16.1s\n",
      "49:\tlearn: 0.2425404\ttotal: 846ms\tremaining: 16.1s\n",
      "50:\tlearn: 0.2414170\ttotal: 863ms\tremaining: 16.1s\n",
      "51:\tlearn: 0.2403348\ttotal: 879ms\tremaining: 16s\n",
      "52:\tlearn: 0.2394713\ttotal: 897ms\tremaining: 16s\n",
      "53:\tlearn: 0.2385692\ttotal: 914ms\tremaining: 16s\n",
      "54:\tlearn: 0.2378137\ttotal: 932ms\tremaining: 16s\n",
      "55:\tlearn: 0.2370594\ttotal: 950ms\tremaining: 16s\n",
      "56:\tlearn: 0.2364021\ttotal: 967ms\tremaining: 16s\n",
      "57:\tlearn: 0.2357424\ttotal: 983ms\tremaining: 16s\n",
      "58:\tlearn: 0.2351032\ttotal: 1s\tremaining: 15.9s\n",
      "59:\tlearn: 0.2345673\ttotal: 1.02s\tremaining: 15.9s\n",
      "60:\tlearn: 0.2340079\ttotal: 1.03s\tremaining: 15.9s\n",
      "61:\tlearn: 0.2333695\ttotal: 1.05s\tremaining: 15.9s\n",
      "62:\tlearn: 0.2328883\ttotal: 1.07s\tremaining: 15.9s\n",
      "63:\tlearn: 0.2325170\ttotal: 1.08s\tremaining: 15.8s\n",
      "64:\tlearn: 0.2321076\ttotal: 1.1s\tremaining: 15.8s\n",
      "65:\tlearn: 0.2315856\ttotal: 1.12s\tremaining: 15.8s\n",
      "66:\tlearn: 0.2312375\ttotal: 1.14s\tremaining: 15.8s\n",
      "67:\tlearn: 0.2308433\ttotal: 1.15s\tremaining: 15.8s\n",
      "68:\tlearn: 0.2304774\ttotal: 1.17s\tremaining: 15.8s\n",
      "69:\tlearn: 0.2300979\ttotal: 1.19s\tremaining: 15.8s\n",
      "70:\tlearn: 0.2297964\ttotal: 1.2s\tremaining: 15.8s\n",
      "71:\tlearn: 0.2294977\ttotal: 1.22s\tremaining: 15.7s\n",
      "72:\tlearn: 0.2292255\ttotal: 1.24s\tremaining: 15.7s\n",
      "73:\tlearn: 0.2288848\ttotal: 1.25s\tremaining: 15.7s\n",
      "74:\tlearn: 0.2285534\ttotal: 1.27s\tremaining: 15.7s\n",
      "75:\tlearn: 0.2283317\ttotal: 1.29s\tremaining: 15.7s\n",
      "76:\tlearn: 0.2280429\ttotal: 1.31s\tremaining: 15.7s\n",
      "77:\tlearn: 0.2277654\ttotal: 1.33s\tremaining: 15.7s\n",
      "78:\tlearn: 0.2275727\ttotal: 1.34s\tremaining: 15.7s\n",
      "79:\tlearn: 0.2273720\ttotal: 1.36s\tremaining: 15.7s\n",
      "80:\tlearn: 0.2271235\ttotal: 1.38s\tremaining: 15.7s\n",
      "81:\tlearn: 0.2269164\ttotal: 1.4s\tremaining: 15.7s\n",
      "82:\tlearn: 0.2267179\ttotal: 1.42s\tremaining: 15.7s\n",
      "83:\tlearn: 0.2265604\ttotal: 1.44s\tremaining: 15.7s\n",
      "84:\tlearn: 0.2263651\ttotal: 1.46s\tremaining: 15.7s\n",
      "85:\tlearn: 0.2261683\ttotal: 1.47s\tremaining: 15.6s\n",
      "86:\tlearn: 0.2260686\ttotal: 1.49s\tremaining: 15.6s\n",
      "87:\tlearn: 0.2259731\ttotal: 1.5s\tremaining: 15.6s\n",
      "88:\tlearn: 0.2257740\ttotal: 1.52s\tremaining: 15.6s\n",
      "89:\tlearn: 0.2255946\ttotal: 1.54s\tremaining: 15.6s\n",
      "90:\tlearn: 0.2254119\ttotal: 1.56s\tremaining: 15.5s\n",
      "91:\tlearn: 0.2252653\ttotal: 1.57s\tremaining: 15.5s\n",
      "92:\tlearn: 0.2251219\ttotal: 1.59s\tremaining: 15.5s\n",
      "93:\tlearn: 0.2249864\ttotal: 1.61s\tremaining: 15.5s\n",
      "94:\tlearn: 0.2248471\ttotal: 1.62s\tremaining: 15.5s\n",
      "95:\tlearn: 0.2247466\ttotal: 1.64s\tremaining: 15.5s\n",
      "96:\tlearn: 0.2246435\ttotal: 1.66s\tremaining: 15.4s\n",
      "97:\tlearn: 0.2245530\ttotal: 1.67s\tremaining: 15.4s\n",
      "98:\tlearn: 0.2244748\ttotal: 1.69s\tremaining: 15.4s\n",
      "99:\tlearn: 0.2243725\ttotal: 1.71s\tremaining: 15.4s\n",
      "100:\tlearn: 0.2242586\ttotal: 1.72s\tremaining: 15.3s\n",
      "101:\tlearn: 0.2241385\ttotal: 1.74s\tremaining: 15.3s\n",
      "102:\tlearn: 0.2240281\ttotal: 1.76s\tremaining: 15.3s\n",
      "103:\tlearn: 0.2239283\ttotal: 1.78s\tremaining: 15.3s\n",
      "104:\tlearn: 0.2238374\ttotal: 1.79s\tremaining: 15.3s\n",
      "105:\tlearn: 0.2237377\ttotal: 1.81s\tremaining: 15.3s\n",
      "106:\tlearn: 0.2236272\ttotal: 1.83s\tremaining: 15.3s\n",
      "107:\tlearn: 0.2235524\ttotal: 1.84s\tremaining: 15.2s\n",
      "108:\tlearn: 0.2234692\ttotal: 1.86s\tremaining: 15.2s\n",
      "109:\tlearn: 0.2233744\ttotal: 1.88s\tremaining: 15.2s\n",
      "110:\tlearn: 0.2232906\ttotal: 1.89s\tremaining: 15.2s\n",
      "111:\tlearn: 0.2232359\ttotal: 1.91s\tremaining: 15.1s\n",
      "112:\tlearn: 0.2231678\ttotal: 1.93s\tremaining: 15.1s\n",
      "113:\tlearn: 0.2230823\ttotal: 1.94s\tremaining: 15.1s\n",
      "114:\tlearn: 0.2230315\ttotal: 1.96s\tremaining: 15.1s\n",
      "115:\tlearn: 0.2229421\ttotal: 1.98s\tremaining: 15.1s\n",
      "116:\tlearn: 0.2228842\ttotal: 1.99s\tremaining: 15s\n",
      "117:\tlearn: 0.2228245\ttotal: 2.01s\tremaining: 15s\n",
      "118:\tlearn: 0.2227740\ttotal: 2.02s\tremaining: 15s\n",
      "119:\tlearn: 0.2227076\ttotal: 2.04s\tremaining: 15s\n",
      "120:\tlearn: 0.2226558\ttotal: 2.06s\tremaining: 15s\n",
      "121:\tlearn: 0.2225759\ttotal: 2.08s\tremaining: 14.9s\n",
      "122:\tlearn: 0.2225158\ttotal: 2.09s\tremaining: 14.9s\n",
      "123:\tlearn: 0.2224760\ttotal: 2.11s\tremaining: 14.9s\n",
      "124:\tlearn: 0.2224253\ttotal: 2.13s\tremaining: 14.9s\n",
      "125:\tlearn: 0.2223388\ttotal: 2.14s\tremaining: 14.9s\n",
      "126:\tlearn: 0.2222839\ttotal: 2.16s\tremaining: 14.8s\n",
      "127:\tlearn: 0.2222236\ttotal: 2.18s\tremaining: 14.8s\n",
      "128:\tlearn: 0.2221494\ttotal: 2.19s\tremaining: 14.8s\n",
      "129:\tlearn: 0.2221200\ttotal: 2.21s\tremaining: 14.8s\n",
      "130:\tlearn: 0.2220585\ttotal: 2.23s\tremaining: 14.8s\n",
      "131:\tlearn: 0.2219971\ttotal: 2.25s\tremaining: 14.8s\n",
      "132:\tlearn: 0.2219666\ttotal: 2.27s\tremaining: 14.8s\n",
      "133:\tlearn: 0.2219206\ttotal: 2.28s\tremaining: 14.7s\n",
      "134:\tlearn: 0.2218300\ttotal: 2.3s\tremaining: 14.7s\n",
      "135:\tlearn: 0.2217902\ttotal: 2.32s\tremaining: 14.7s\n",
      "136:\tlearn: 0.2217420\ttotal: 2.33s\tremaining: 14.7s\n",
      "137:\tlearn: 0.2217023\ttotal: 2.35s\tremaining: 14.7s\n",
      "138:\tlearn: 0.2216534\ttotal: 2.37s\tremaining: 14.7s\n",
      "139:\tlearn: 0.2216019\ttotal: 2.39s\tremaining: 14.7s\n",
      "140:\tlearn: 0.2215573\ttotal: 2.4s\tremaining: 14.6s\n",
      "141:\tlearn: 0.2214778\ttotal: 2.42s\tremaining: 14.6s\n",
      "142:\tlearn: 0.2214312\ttotal: 2.44s\tremaining: 14.6s\n",
      "143:\tlearn: 0.2213777\ttotal: 2.46s\tremaining: 14.6s\n",
      "144:\tlearn: 0.2213321\ttotal: 2.47s\tremaining: 14.6s\n",
      "145:\tlearn: 0.2212507\ttotal: 2.49s\tremaining: 14.6s\n",
      "146:\tlearn: 0.2211966\ttotal: 2.51s\tremaining: 14.6s\n",
      "147:\tlearn: 0.2211616\ttotal: 2.52s\tremaining: 14.5s\n",
      "148:\tlearn: 0.2210865\ttotal: 2.54s\tremaining: 14.5s\n",
      "149:\tlearn: 0.2210296\ttotal: 2.56s\tremaining: 14.5s\n",
      "150:\tlearn: 0.2209917\ttotal: 2.57s\tremaining: 14.5s\n",
      "151:\tlearn: 0.2209292\ttotal: 2.59s\tremaining: 14.4s\n",
      "152:\tlearn: 0.2208816\ttotal: 2.61s\tremaining: 14.4s\n",
      "153:\tlearn: 0.2208212\ttotal: 2.62s\tremaining: 14.4s\n",
      "154:\tlearn: 0.2207857\ttotal: 2.64s\tremaining: 14.4s\n",
      "155:\tlearn: 0.2207505\ttotal: 2.66s\tremaining: 14.4s\n",
      "156:\tlearn: 0.2207184\ttotal: 2.68s\tremaining: 14.4s\n",
      "157:\tlearn: 0.2206621\ttotal: 2.69s\tremaining: 14.4s\n",
      "158:\tlearn: 0.2206289\ttotal: 2.71s\tremaining: 14.3s\n",
      "159:\tlearn: 0.2205934\ttotal: 2.73s\tremaining: 14.3s\n",
      "160:\tlearn: 0.2205576\ttotal: 2.74s\tremaining: 14.3s\n",
      "161:\tlearn: 0.2205042\ttotal: 2.76s\tremaining: 14.3s\n",
      "162:\tlearn: 0.2204625\ttotal: 2.78s\tremaining: 14.3s\n",
      "163:\tlearn: 0.2204289\ttotal: 2.79s\tremaining: 14.2s\n",
      "164:\tlearn: 0.2203829\ttotal: 2.81s\tremaining: 14.2s\n",
      "165:\tlearn: 0.2203495\ttotal: 2.83s\tremaining: 14.2s\n",
      "166:\tlearn: 0.2203090\ttotal: 2.85s\tremaining: 14.2s\n",
      "167:\tlearn: 0.2202722\ttotal: 2.87s\tremaining: 14.2s\n",
      "168:\tlearn: 0.2202102\ttotal: 2.88s\tremaining: 14.2s\n",
      "169:\tlearn: 0.2201775\ttotal: 2.9s\tremaining: 14.2s\n",
      "170:\tlearn: 0.2201383\ttotal: 2.92s\tremaining: 14.1s\n",
      "171:\tlearn: 0.2200895\ttotal: 2.93s\tremaining: 14.1s\n",
      "172:\tlearn: 0.2200578\ttotal: 2.95s\tremaining: 14.1s\n",
      "173:\tlearn: 0.2200252\ttotal: 2.96s\tremaining: 14.1s\n",
      "174:\tlearn: 0.2199773\ttotal: 2.98s\tremaining: 14.1s\n",
      "175:\tlearn: 0.2199334\ttotal: 3s\tremaining: 14s\n",
      "176:\tlearn: 0.2198677\ttotal: 3.02s\tremaining: 14s\n",
      "177:\tlearn: 0.2198216\ttotal: 3.03s\tremaining: 14s\n",
      "178:\tlearn: 0.2197693\ttotal: 3.05s\tremaining: 14s\n",
      "179:\tlearn: 0.2197380\ttotal: 3.07s\tremaining: 14s\n",
      "180:\tlearn: 0.2196892\ttotal: 3.09s\tremaining: 14s\n",
      "181:\tlearn: 0.2196213\ttotal: 3.1s\tremaining: 13.9s\n",
      "182:\tlearn: 0.2195913\ttotal: 3.12s\tremaining: 13.9s\n",
      "183:\tlearn: 0.2195301\ttotal: 3.14s\tremaining: 13.9s\n",
      "184:\tlearn: 0.2195086\ttotal: 3.15s\tremaining: 13.9s\n",
      "185:\tlearn: 0.2194627\ttotal: 3.17s\tremaining: 13.9s\n",
      "186:\tlearn: 0.2194207\ttotal: 3.19s\tremaining: 13.9s\n",
      "187:\tlearn: 0.2193747\ttotal: 3.2s\tremaining: 13.8s\n",
      "188:\tlearn: 0.2193367\ttotal: 3.22s\tremaining: 13.8s\n",
      "189:\tlearn: 0.2192922\ttotal: 3.24s\tremaining: 13.8s\n",
      "190:\tlearn: 0.2192435\ttotal: 3.26s\tremaining: 13.8s\n",
      "191:\tlearn: 0.2192064\ttotal: 3.27s\tremaining: 13.8s\n",
      "192:\tlearn: 0.2191708\ttotal: 3.29s\tremaining: 13.8s\n",
      "193:\tlearn: 0.2191207\ttotal: 3.31s\tremaining: 13.7s\n",
      "194:\tlearn: 0.2190669\ttotal: 3.33s\tremaining: 13.7s\n",
      "195:\tlearn: 0.2190487\ttotal: 3.34s\tremaining: 13.7s\n",
      "196:\tlearn: 0.2190176\ttotal: 3.36s\tremaining: 13.7s\n",
      "197:\tlearn: 0.2189828\ttotal: 3.38s\tremaining: 13.7s\n",
      "198:\tlearn: 0.2189459\ttotal: 3.39s\tremaining: 13.7s\n",
      "199:\tlearn: 0.2189030\ttotal: 3.41s\tremaining: 13.6s\n",
      "200:\tlearn: 0.2188625\ttotal: 3.43s\tremaining: 13.6s\n",
      "201:\tlearn: 0.2188344\ttotal: 3.45s\tremaining: 13.6s\n",
      "202:\tlearn: 0.2188138\ttotal: 3.47s\tremaining: 13.6s\n",
      "203:\tlearn: 0.2187550\ttotal: 3.48s\tremaining: 13.6s\n",
      "204:\tlearn: 0.2187326\ttotal: 3.5s\tremaining: 13.6s\n",
      "205:\tlearn: 0.2187096\ttotal: 3.52s\tremaining: 13.6s\n",
      "206:\tlearn: 0.2186835\ttotal: 3.53s\tremaining: 13.5s\n",
      "207:\tlearn: 0.2186259\ttotal: 3.55s\tremaining: 13.5s\n",
      "208:\tlearn: 0.2185791\ttotal: 3.57s\tremaining: 13.5s\n",
      "209:\tlearn: 0.2185504\ttotal: 3.58s\tremaining: 13.5s\n",
      "210:\tlearn: 0.2185262\ttotal: 3.6s\tremaining: 13.5s\n",
      "211:\tlearn: 0.2184954\ttotal: 3.62s\tremaining: 13.4s\n",
      "212:\tlearn: 0.2184766\ttotal: 3.63s\tremaining: 13.4s\n",
      "213:\tlearn: 0.2184214\ttotal: 3.65s\tremaining: 13.4s\n",
      "214:\tlearn: 0.2183866\ttotal: 3.67s\tremaining: 13.4s\n",
      "215:\tlearn: 0.2183482\ttotal: 3.69s\tremaining: 13.4s\n",
      "216:\tlearn: 0.2183106\ttotal: 3.71s\tremaining: 13.4s\n",
      "217:\tlearn: 0.2182988\ttotal: 3.72s\tremaining: 13.4s\n",
      "218:\tlearn: 0.2182635\ttotal: 3.74s\tremaining: 13.3s\n",
      "219:\tlearn: 0.2182189\ttotal: 3.76s\tremaining: 13.3s\n",
      "220:\tlearn: 0.2181833\ttotal: 3.78s\tremaining: 13.3s\n",
      "221:\tlearn: 0.2181625\ttotal: 3.79s\tremaining: 13.3s\n",
      "222:\tlearn: 0.2181375\ttotal: 3.81s\tremaining: 13.3s\n",
      "223:\tlearn: 0.2181066\ttotal: 3.83s\tremaining: 13.3s\n",
      "224:\tlearn: 0.2180750\ttotal: 3.84s\tremaining: 13.2s\n",
      "225:\tlearn: 0.2180297\ttotal: 3.86s\tremaining: 13.2s\n",
      "226:\tlearn: 0.2179818\ttotal: 3.88s\tremaining: 13.2s\n",
      "227:\tlearn: 0.2179570\ttotal: 3.9s\tremaining: 13.2s\n",
      "228:\tlearn: 0.2179159\ttotal: 3.91s\tremaining: 13.2s\n",
      "229:\tlearn: 0.2178882\ttotal: 3.93s\tremaining: 13.2s\n",
      "230:\tlearn: 0.2178270\ttotal: 3.95s\tremaining: 13.1s\n",
      "231:\tlearn: 0.2177794\ttotal: 3.96s\tremaining: 13.1s\n",
      "232:\tlearn: 0.2177416\ttotal: 3.98s\tremaining: 13.1s\n",
      "233:\tlearn: 0.2177074\ttotal: 4s\tremaining: 13.1s\n",
      "234:\tlearn: 0.2176767\ttotal: 4.01s\tremaining: 13.1s\n",
      "235:\tlearn: 0.2176449\ttotal: 4.03s\tremaining: 13.1s\n",
      "236:\tlearn: 0.2176276\ttotal: 4.05s\tremaining: 13s\n",
      "237:\tlearn: 0.2175811\ttotal: 4.07s\tremaining: 13s\n",
      "238:\tlearn: 0.2175565\ttotal: 4.08s\tremaining: 13s\n",
      "239:\tlearn: 0.2175203\ttotal: 4.1s\tremaining: 13s\n",
      "240:\tlearn: 0.2174978\ttotal: 4.12s\tremaining: 13s\n",
      "241:\tlearn: 0.2174678\ttotal: 4.14s\tremaining: 13s\n",
      "242:\tlearn: 0.2174398\ttotal: 4.16s\tremaining: 12.9s\n",
      "243:\tlearn: 0.2174248\ttotal: 4.17s\tremaining: 12.9s\n",
      "244:\tlearn: 0.2173955\ttotal: 4.19s\tremaining: 12.9s\n",
      "245:\tlearn: 0.2173651\ttotal: 4.21s\tremaining: 12.9s\n",
      "246:\tlearn: 0.2173126\ttotal: 4.22s\tremaining: 12.9s\n",
      "247:\tlearn: 0.2172856\ttotal: 4.24s\tremaining: 12.9s\n",
      "248:\tlearn: 0.2172201\ttotal: 4.26s\tremaining: 12.8s\n",
      "249:\tlearn: 0.2171995\ttotal: 4.27s\tremaining: 12.8s\n",
      "250:\tlearn: 0.2171594\ttotal: 4.29s\tremaining: 12.8s\n",
      "251:\tlearn: 0.2171331\ttotal: 4.31s\tremaining: 12.8s\n",
      "252:\tlearn: 0.2170869\ttotal: 4.33s\tremaining: 12.8s\n",
      "253:\tlearn: 0.2170509\ttotal: 4.35s\tremaining: 12.8s\n",
      "254:\tlearn: 0.2170173\ttotal: 4.36s\tremaining: 12.8s\n",
      "255:\tlearn: 0.2169557\ttotal: 4.38s\tremaining: 12.7s\n",
      "256:\tlearn: 0.2169080\ttotal: 4.4s\tremaining: 12.7s\n",
      "257:\tlearn: 0.2168889\ttotal: 4.42s\tremaining: 12.7s\n",
      "258:\tlearn: 0.2168642\ttotal: 4.43s\tremaining: 12.7s\n",
      "259:\tlearn: 0.2168204\ttotal: 4.45s\tremaining: 12.7s\n",
      "260:\tlearn: 0.2167955\ttotal: 4.47s\tremaining: 12.6s\n",
      "261:\tlearn: 0.2167500\ttotal: 4.48s\tremaining: 12.6s\n",
      "262:\tlearn: 0.2167202\ttotal: 4.5s\tremaining: 12.6s\n",
      "263:\tlearn: 0.2166876\ttotal: 4.52s\tremaining: 12.6s\n",
      "264:\tlearn: 0.2166377\ttotal: 4.54s\tremaining: 12.6s\n",
      "265:\tlearn: 0.2165888\ttotal: 4.55s\tremaining: 12.6s\n",
      "266:\tlearn: 0.2165331\ttotal: 4.57s\tremaining: 12.5s\n",
      "267:\tlearn: 0.2164859\ttotal: 4.59s\tremaining: 12.5s\n",
      "268:\tlearn: 0.2164275\ttotal: 4.6s\tremaining: 12.5s\n",
      "269:\tlearn: 0.2164047\ttotal: 4.62s\tremaining: 12.5s\n",
      "270:\tlearn: 0.2163333\ttotal: 4.64s\tremaining: 12.5s\n",
      "271:\tlearn: 0.2162895\ttotal: 4.66s\tremaining: 12.5s\n",
      "272:\tlearn: 0.2162588\ttotal: 4.67s\tremaining: 12.4s\n",
      "273:\tlearn: 0.2162236\ttotal: 4.69s\tremaining: 12.4s\n",
      "274:\tlearn: 0.2161895\ttotal: 4.71s\tremaining: 12.4s\n",
      "275:\tlearn: 0.2161237\ttotal: 4.72s\tremaining: 12.4s\n",
      "276:\tlearn: 0.2160842\ttotal: 4.74s\tremaining: 12.4s\n",
      "277:\tlearn: 0.2160537\ttotal: 4.76s\tremaining: 12.4s\n",
      "278:\tlearn: 0.2160247\ttotal: 4.78s\tremaining: 12.3s\n",
      "279:\tlearn: 0.2159996\ttotal: 4.79s\tremaining: 12.3s\n",
      "280:\tlearn: 0.2159697\ttotal: 4.81s\tremaining: 12.3s\n",
      "281:\tlearn: 0.2159439\ttotal: 4.83s\tremaining: 12.3s\n",
      "282:\tlearn: 0.2159170\ttotal: 4.84s\tremaining: 12.3s\n",
      "283:\tlearn: 0.2158902\ttotal: 4.86s\tremaining: 12.3s\n",
      "284:\tlearn: 0.2158719\ttotal: 4.88s\tremaining: 12.2s\n",
      "285:\tlearn: 0.2158656\ttotal: 4.89s\tremaining: 12.2s\n",
      "286:\tlearn: 0.2158326\ttotal: 4.91s\tremaining: 12.2s\n",
      "287:\tlearn: 0.2158118\ttotal: 4.92s\tremaining: 12.2s\n",
      "288:\tlearn: 0.2157774\ttotal: 4.94s\tremaining: 12.2s\n",
      "289:\tlearn: 0.2157430\ttotal: 4.96s\tremaining: 12.1s\n",
      "290:\tlearn: 0.2157115\ttotal: 4.98s\tremaining: 12.1s\n",
      "291:\tlearn: 0.2156882\ttotal: 5s\tremaining: 12.1s\n",
      "292:\tlearn: 0.2156402\ttotal: 5.01s\tremaining: 12.1s\n",
      "293:\tlearn: 0.2156023\ttotal: 5.03s\tremaining: 12.1s\n",
      "294:\tlearn: 0.2155745\ttotal: 5.05s\tremaining: 12.1s\n",
      "295:\tlearn: 0.2155301\ttotal: 5.07s\tremaining: 12s\n",
      "296:\tlearn: 0.2155013\ttotal: 5.08s\tremaining: 12s\n",
      "297:\tlearn: 0.2154589\ttotal: 5.1s\tremaining: 12s\n",
      "298:\tlearn: 0.2154390\ttotal: 5.11s\tremaining: 12s\n",
      "299:\tlearn: 0.2154042\ttotal: 5.13s\tremaining: 12s\n",
      "300:\tlearn: 0.2153822\ttotal: 5.15s\tremaining: 12s\n",
      "301:\tlearn: 0.2153495\ttotal: 5.17s\tremaining: 11.9s\n",
      "302:\tlearn: 0.2153155\ttotal: 5.18s\tremaining: 11.9s\n",
      "303:\tlearn: 0.2152887\ttotal: 5.2s\tremaining: 11.9s\n",
      "304:\tlearn: 0.2152546\ttotal: 5.22s\tremaining: 11.9s\n",
      "305:\tlearn: 0.2152061\ttotal: 5.23s\tremaining: 11.9s\n",
      "306:\tlearn: 0.2151714\ttotal: 5.25s\tremaining: 11.9s\n",
      "307:\tlearn: 0.2151437\ttotal: 5.27s\tremaining: 11.8s\n",
      "308:\tlearn: 0.2151062\ttotal: 5.28s\tremaining: 11.8s\n",
      "309:\tlearn: 0.2150831\ttotal: 5.3s\tremaining: 11.8s\n",
      "310:\tlearn: 0.2150466\ttotal: 5.32s\tremaining: 11.8s\n",
      "311:\tlearn: 0.2150242\ttotal: 5.33s\tremaining: 11.8s\n",
      "312:\tlearn: 0.2149875\ttotal: 5.35s\tremaining: 11.7s\n",
      "313:\tlearn: 0.2149738\ttotal: 5.37s\tremaining: 11.7s\n",
      "314:\tlearn: 0.2149387\ttotal: 5.39s\tremaining: 11.7s\n",
      "315:\tlearn: 0.2149026\ttotal: 5.41s\tremaining: 11.7s\n",
      "316:\tlearn: 0.2148739\ttotal: 5.42s\tremaining: 11.7s\n",
      "317:\tlearn: 0.2148427\ttotal: 5.44s\tremaining: 11.7s\n",
      "318:\tlearn: 0.2148257\ttotal: 5.46s\tremaining: 11.6s\n",
      "319:\tlearn: 0.2147852\ttotal: 5.47s\tremaining: 11.6s\n",
      "320:\tlearn: 0.2147584\ttotal: 5.49s\tremaining: 11.6s\n",
      "321:\tlearn: 0.2147119\ttotal: 5.51s\tremaining: 11.6s\n",
      "322:\tlearn: 0.2146582\ttotal: 5.52s\tremaining: 11.6s\n",
      "323:\tlearn: 0.2146368\ttotal: 5.54s\tremaining: 11.6s\n",
      "324:\tlearn: 0.2146048\ttotal: 5.56s\tremaining: 11.5s\n",
      "325:\tlearn: 0.2145809\ttotal: 5.58s\tremaining: 11.5s\n",
      "326:\tlearn: 0.2145520\ttotal: 5.59s\tremaining: 11.5s\n",
      "327:\tlearn: 0.2145200\ttotal: 5.61s\tremaining: 11.5s\n",
      "328:\tlearn: 0.2144944\ttotal: 5.63s\tremaining: 11.5s\n",
      "329:\tlearn: 0.2144578\ttotal: 5.65s\tremaining: 11.5s\n",
      "330:\tlearn: 0.2144407\ttotal: 5.67s\tremaining: 11.5s\n",
      "331:\tlearn: 0.2144210\ttotal: 5.68s\tremaining: 11.4s\n",
      "332:\tlearn: 0.2144012\ttotal: 5.7s\tremaining: 11.4s\n",
      "333:\tlearn: 0.2143840\ttotal: 5.72s\tremaining: 11.4s\n",
      "334:\tlearn: 0.2143715\ttotal: 5.73s\tremaining: 11.4s\n",
      "335:\tlearn: 0.2143354\ttotal: 5.75s\tremaining: 11.4s\n",
      "336:\tlearn: 0.2143136\ttotal: 5.77s\tremaining: 11.3s\n",
      "337:\tlearn: 0.2142677\ttotal: 5.79s\tremaining: 11.3s\n",
      "338:\tlearn: 0.2142521\ttotal: 5.8s\tremaining: 11.3s\n",
      "339:\tlearn: 0.2142048\ttotal: 5.82s\tremaining: 11.3s\n",
      "340:\tlearn: 0.2141938\ttotal: 5.84s\tremaining: 11.3s\n",
      "341:\tlearn: 0.2141736\ttotal: 5.86s\tremaining: 11.3s\n",
      "342:\tlearn: 0.2141442\ttotal: 5.88s\tremaining: 11.3s\n",
      "343:\tlearn: 0.2141292\ttotal: 5.89s\tremaining: 11.2s\n",
      "344:\tlearn: 0.2140926\ttotal: 5.91s\tremaining: 11.2s\n",
      "345:\tlearn: 0.2140764\ttotal: 5.92s\tremaining: 11.2s\n",
      "346:\tlearn: 0.2140514\ttotal: 5.94s\tremaining: 11.2s\n",
      "347:\tlearn: 0.2140312\ttotal: 5.96s\tremaining: 11.2s\n",
      "348:\tlearn: 0.2140056\ttotal: 5.97s\tremaining: 11.1s\n",
      "349:\tlearn: 0.2139677\ttotal: 5.99s\tremaining: 11.1s\n",
      "350:\tlearn: 0.2139483\ttotal: 6.01s\tremaining: 11.1s\n",
      "351:\tlearn: 0.2139115\ttotal: 6.03s\tremaining: 11.1s\n",
      "352:\tlearn: 0.2138875\ttotal: 6.04s\tremaining: 11.1s\n",
      "353:\tlearn: 0.2138609\ttotal: 6.06s\tremaining: 11.1s\n",
      "354:\tlearn: 0.2138383\ttotal: 6.08s\tremaining: 11s\n",
      "355:\tlearn: 0.2138149\ttotal: 6.1s\tremaining: 11s\n",
      "356:\tlearn: 0.2137934\ttotal: 6.11s\tremaining: 11s\n",
      "357:\tlearn: 0.2137582\ttotal: 6.13s\tremaining: 11s\n",
      "358:\tlearn: 0.2137296\ttotal: 6.15s\tremaining: 11s\n",
      "359:\tlearn: 0.2136992\ttotal: 6.16s\tremaining: 11s\n",
      "360:\tlearn: 0.2136720\ttotal: 6.18s\tremaining: 10.9s\n",
      "361:\tlearn: 0.2136494\ttotal: 6.2s\tremaining: 10.9s\n",
      "362:\tlearn: 0.2136278\ttotal: 6.21s\tremaining: 10.9s\n",
      "363:\tlearn: 0.2135790\ttotal: 6.23s\tremaining: 10.9s\n",
      "364:\tlearn: 0.2135460\ttotal: 6.25s\tremaining: 10.9s\n",
      "365:\tlearn: 0.2135322\ttotal: 6.27s\tremaining: 10.9s\n",
      "366:\tlearn: 0.2135078\ttotal: 6.29s\tremaining: 10.8s\n",
      "367:\tlearn: 0.2134840\ttotal: 6.3s\tremaining: 10.8s\n",
      "368:\tlearn: 0.2134466\ttotal: 6.32s\tremaining: 10.8s\n",
      "369:\tlearn: 0.2134176\ttotal: 6.34s\tremaining: 10.8s\n",
      "370:\tlearn: 0.2133869\ttotal: 6.35s\tremaining: 10.8s\n",
      "371:\tlearn: 0.2133516\ttotal: 6.37s\tremaining: 10.8s\n",
      "372:\tlearn: 0.2133199\ttotal: 6.39s\tremaining: 10.7s\n",
      "373:\tlearn: 0.2132950\ttotal: 6.4s\tremaining: 10.7s\n",
      "374:\tlearn: 0.2132771\ttotal: 6.42s\tremaining: 10.7s\n",
      "375:\tlearn: 0.2132449\ttotal: 6.44s\tremaining: 10.7s\n",
      "376:\tlearn: 0.2132126\ttotal: 6.46s\tremaining: 10.7s\n",
      "377:\tlearn: 0.2132028\ttotal: 6.47s\tremaining: 10.7s\n",
      "378:\tlearn: 0.2131873\ttotal: 6.49s\tremaining: 10.6s\n",
      "379:\tlearn: 0.2131657\ttotal: 6.51s\tremaining: 10.6s\n",
      "380:\tlearn: 0.2131464\ttotal: 6.52s\tremaining: 10.6s\n",
      "381:\tlearn: 0.2131171\ttotal: 6.54s\tremaining: 10.6s\n",
      "382:\tlearn: 0.2130889\ttotal: 6.56s\tremaining: 10.6s\n",
      "383:\tlearn: 0.2130544\ttotal: 6.57s\tremaining: 10.5s\n",
      "384:\tlearn: 0.2130013\ttotal: 6.59s\tremaining: 10.5s\n",
      "385:\tlearn: 0.2129572\ttotal: 6.61s\tremaining: 10.5s\n",
      "386:\tlearn: 0.2129153\ttotal: 6.62s\tremaining: 10.5s\n",
      "387:\tlearn: 0.2128776\ttotal: 6.64s\tremaining: 10.5s\n",
      "388:\tlearn: 0.2128402\ttotal: 6.66s\tremaining: 10.5s\n",
      "389:\tlearn: 0.2127895\ttotal: 6.68s\tremaining: 10.4s\n",
      "390:\tlearn: 0.2127736\ttotal: 6.7s\tremaining: 10.4s\n",
      "391:\tlearn: 0.2127607\ttotal: 6.71s\tremaining: 10.4s\n",
      "392:\tlearn: 0.2127195\ttotal: 6.73s\tremaining: 10.4s\n",
      "393:\tlearn: 0.2126774\ttotal: 6.75s\tremaining: 10.4s\n",
      "394:\tlearn: 0.2126512\ttotal: 6.76s\tremaining: 10.4s\n",
      "395:\tlearn: 0.2126240\ttotal: 6.78s\tremaining: 10.3s\n",
      "396:\tlearn: 0.2125578\ttotal: 6.8s\tremaining: 10.3s\n",
      "397:\tlearn: 0.2125298\ttotal: 6.82s\tremaining: 10.3s\n",
      "398:\tlearn: 0.2124996\ttotal: 6.83s\tremaining: 10.3s\n",
      "399:\tlearn: 0.2124709\ttotal: 6.85s\tremaining: 10.3s\n",
      "400:\tlearn: 0.2124524\ttotal: 6.87s\tremaining: 10.3s\n",
      "401:\tlearn: 0.2124093\ttotal: 6.88s\tremaining: 10.2s\n",
      "402:\tlearn: 0.2123881\ttotal: 6.9s\tremaining: 10.2s\n",
      "403:\tlearn: 0.2123748\ttotal: 6.92s\tremaining: 10.2s\n",
      "404:\tlearn: 0.2123295\ttotal: 6.94s\tremaining: 10.2s\n",
      "405:\tlearn: 0.2123009\ttotal: 6.96s\tremaining: 10.2s\n",
      "406:\tlearn: 0.2122801\ttotal: 6.97s\tremaining: 10.2s\n",
      "407:\tlearn: 0.2122257\ttotal: 6.99s\tremaining: 10.1s\n",
      "408:\tlearn: 0.2121909\ttotal: 7s\tremaining: 10.1s\n",
      "409:\tlearn: 0.2121386\ttotal: 7.02s\tremaining: 10.1s\n",
      "410:\tlearn: 0.2121209\ttotal: 7.04s\tremaining: 10.1s\n",
      "411:\tlearn: 0.2120900\ttotal: 7.05s\tremaining: 10.1s\n",
      "412:\tlearn: 0.2120439\ttotal: 7.07s\tremaining: 10.1s\n",
      "413:\tlearn: 0.2120226\ttotal: 7.09s\tremaining: 10s\n",
      "414:\tlearn: 0.2120091\ttotal: 7.11s\tremaining: 10s\n",
      "415:\tlearn: 0.2119879\ttotal: 7.13s\tremaining: 10s\n",
      "416:\tlearn: 0.2119702\ttotal: 7.15s\tremaining: 9.99s\n",
      "417:\tlearn: 0.2119303\ttotal: 7.16s\tremaining: 9.97s\n",
      "418:\tlearn: 0.2118972\ttotal: 7.18s\tremaining: 9.96s\n",
      "419:\tlearn: 0.2118872\ttotal: 7.2s\tremaining: 9.94s\n",
      "420:\tlearn: 0.2118465\ttotal: 7.21s\tremaining: 9.92s\n",
      "421:\tlearn: 0.2118333\ttotal: 7.23s\tremaining: 9.9s\n",
      "422:\tlearn: 0.2117944\ttotal: 7.25s\tremaining: 9.88s\n",
      "423:\tlearn: 0.2117694\ttotal: 7.26s\tremaining: 9.87s\n",
      "424:\tlearn: 0.2117102\ttotal: 7.28s\tremaining: 9.85s\n",
      "425:\tlearn: 0.2116852\ttotal: 7.3s\tremaining: 9.83s\n",
      "426:\tlearn: 0.2116705\ttotal: 7.31s\tremaining: 9.81s\n",
      "427:\tlearn: 0.2116568\ttotal: 7.33s\tremaining: 9.8s\n",
      "428:\tlearn: 0.2116247\ttotal: 7.35s\tremaining: 9.78s\n",
      "429:\tlearn: 0.2116071\ttotal: 7.37s\tremaining: 9.77s\n",
      "430:\tlearn: 0.2115921\ttotal: 7.38s\tremaining: 9.75s\n",
      "431:\tlearn: 0.2115756\ttotal: 7.4s\tremaining: 9.73s\n",
      "432:\tlearn: 0.2115582\ttotal: 7.42s\tremaining: 9.71s\n",
      "433:\tlearn: 0.2115359\ttotal: 7.43s\tremaining: 9.7s\n",
      "434:\tlearn: 0.2114905\ttotal: 7.45s\tremaining: 9.68s\n",
      "435:\tlearn: 0.2114475\ttotal: 7.47s\tremaining: 9.66s\n",
      "436:\tlearn: 0.2114173\ttotal: 7.49s\tremaining: 9.64s\n",
      "437:\tlearn: 0.2114046\ttotal: 7.5s\tremaining: 9.63s\n",
      "438:\tlearn: 0.2113936\ttotal: 7.52s\tremaining: 9.61s\n",
      "439:\tlearn: 0.2113831\ttotal: 7.54s\tremaining: 9.59s\n",
      "440:\tlearn: 0.2113546\ttotal: 7.56s\tremaining: 9.58s\n",
      "441:\tlearn: 0.2113388\ttotal: 7.57s\tremaining: 9.56s\n",
      "442:\tlearn: 0.2113206\ttotal: 7.59s\tremaining: 9.54s\n",
      "443:\tlearn: 0.2112946\ttotal: 7.61s\tremaining: 9.53s\n",
      "444:\tlearn: 0.2112445\ttotal: 7.62s\tremaining: 9.51s\n",
      "445:\tlearn: 0.2112147\ttotal: 7.64s\tremaining: 9.49s\n",
      "446:\tlearn: 0.2112006\ttotal: 7.66s\tremaining: 9.47s\n",
      "447:\tlearn: 0.2111926\ttotal: 7.67s\tremaining: 9.46s\n",
      "448:\tlearn: 0.2111770\ttotal: 7.69s\tremaining: 9.44s\n",
      "449:\tlearn: 0.2111533\ttotal: 7.71s\tremaining: 9.42s\n",
      "450:\tlearn: 0.2111243\ttotal: 7.73s\tremaining: 9.4s\n",
      "451:\tlearn: 0.2111054\ttotal: 7.74s\tremaining: 9.39s\n",
      "452:\tlearn: 0.2110811\ttotal: 7.76s\tremaining: 9.37s\n",
      "453:\tlearn: 0.2110631\ttotal: 7.78s\tremaining: 9.36s\n",
      "454:\tlearn: 0.2110409\ttotal: 7.8s\tremaining: 9.34s\n",
      "455:\tlearn: 0.2110267\ttotal: 7.81s\tremaining: 9.32s\n",
      "456:\tlearn: 0.2109884\ttotal: 7.83s\tremaining: 9.3s\n",
      "457:\tlearn: 0.2109644\ttotal: 7.85s\tremaining: 9.29s\n",
      "458:\tlearn: 0.2109501\ttotal: 7.87s\tremaining: 9.27s\n",
      "459:\tlearn: 0.2109235\ttotal: 7.88s\tremaining: 9.25s\n",
      "460:\tlearn: 0.2109021\ttotal: 7.9s\tremaining: 9.24s\n",
      "461:\tlearn: 0.2108296\ttotal: 7.92s\tremaining: 9.22s\n",
      "462:\tlearn: 0.2107761\ttotal: 7.93s\tremaining: 9.2s\n",
      "463:\tlearn: 0.2107441\ttotal: 7.95s\tremaining: 9.19s\n",
      "464:\tlearn: 0.2107081\ttotal: 7.97s\tremaining: 9.17s\n",
      "465:\tlearn: 0.2106991\ttotal: 7.99s\tremaining: 9.15s\n",
      "466:\tlearn: 0.2106603\ttotal: 8.01s\tremaining: 9.14s\n",
      "467:\tlearn: 0.2106400\ttotal: 8.02s\tremaining: 9.12s\n",
      "468:\tlearn: 0.2106313\ttotal: 8.04s\tremaining: 9.1s\n",
      "469:\tlearn: 0.2106181\ttotal: 8.05s\tremaining: 9.08s\n",
      "470:\tlearn: 0.2105823\ttotal: 8.07s\tremaining: 9.07s\n",
      "471:\tlearn: 0.2105675\ttotal: 8.09s\tremaining: 9.05s\n",
      "472:\tlearn: 0.2105401\ttotal: 8.11s\tremaining: 9.03s\n",
      "473:\tlearn: 0.2105260\ttotal: 8.12s\tremaining: 9.01s\n",
      "474:\tlearn: 0.2104889\ttotal: 8.14s\tremaining: 9s\n",
      "475:\tlearn: 0.2104749\ttotal: 8.16s\tremaining: 8.98s\n",
      "476:\tlearn: 0.2104562\ttotal: 8.18s\tremaining: 8.96s\n",
      "477:\tlearn: 0.2104261\ttotal: 8.2s\tremaining: 8.95s\n",
      "478:\tlearn: 0.2103809\ttotal: 8.21s\tremaining: 8.94s\n",
      "479:\tlearn: 0.2103472\ttotal: 8.23s\tremaining: 8.92s\n",
      "480:\tlearn: 0.2103348\ttotal: 8.25s\tremaining: 8.9s\n",
      "481:\tlearn: 0.2103182\ttotal: 8.26s\tremaining: 8.88s\n",
      "482:\tlearn: 0.2103017\ttotal: 8.28s\tremaining: 8.86s\n",
      "483:\tlearn: 0.2102777\ttotal: 8.3s\tremaining: 8.85s\n",
      "484:\tlearn: 0.2102695\ttotal: 8.31s\tremaining: 8.83s\n",
      "485:\tlearn: 0.2102087\ttotal: 8.33s\tremaining: 8.81s\n",
      "486:\tlearn: 0.2101754\ttotal: 8.35s\tremaining: 8.79s\n",
      "487:\tlearn: 0.2101657\ttotal: 8.37s\tremaining: 8.78s\n",
      "488:\tlearn: 0.2101242\ttotal: 8.38s\tremaining: 8.76s\n",
      "489:\tlearn: 0.2101095\ttotal: 8.4s\tremaining: 8.74s\n",
      "490:\tlearn: 0.2100897\ttotal: 8.42s\tremaining: 8.73s\n",
      "491:\tlearn: 0.2100818\ttotal: 8.44s\tremaining: 8.71s\n",
      "492:\tlearn: 0.2100401\ttotal: 8.45s\tremaining: 8.69s\n",
      "493:\tlearn: 0.2100243\ttotal: 8.47s\tremaining: 8.67s\n",
      "494:\tlearn: 0.2100096\ttotal: 8.48s\tremaining: 8.66s\n",
      "495:\tlearn: 0.2099763\ttotal: 8.5s\tremaining: 8.64s\n",
      "496:\tlearn: 0.2099450\ttotal: 8.52s\tremaining: 8.62s\n",
      "497:\tlearn: 0.2099322\ttotal: 8.53s\tremaining: 8.6s\n",
      "498:\tlearn: 0.2099054\ttotal: 8.55s\tremaining: 8.59s\n",
      "499:\tlearn: 0.2098824\ttotal: 8.57s\tremaining: 8.57s\n",
      "500:\tlearn: 0.2098734\ttotal: 8.58s\tremaining: 8.55s\n",
      "501:\tlearn: 0.2098559\ttotal: 8.6s\tremaining: 8.53s\n",
      "502:\tlearn: 0.2098391\ttotal: 8.62s\tremaining: 8.52s\n",
      "503:\tlearn: 0.2098254\ttotal: 8.64s\tremaining: 8.51s\n",
      "504:\tlearn: 0.2097870\ttotal: 8.66s\tremaining: 8.49s\n",
      "505:\tlearn: 0.2097577\ttotal: 8.68s\tremaining: 8.47s\n",
      "506:\tlearn: 0.2097368\ttotal: 8.69s\tremaining: 8.45s\n",
      "507:\tlearn: 0.2096805\ttotal: 8.71s\tremaining: 8.44s\n",
      "508:\tlearn: 0.2096625\ttotal: 8.73s\tremaining: 8.42s\n",
      "509:\tlearn: 0.2096508\ttotal: 8.74s\tremaining: 8.4s\n",
      "510:\tlearn: 0.2096296\ttotal: 8.76s\tremaining: 8.38s\n",
      "511:\tlearn: 0.2096202\ttotal: 8.78s\tremaining: 8.37s\n",
      "512:\tlearn: 0.2096030\ttotal: 8.79s\tremaining: 8.35s\n",
      "513:\tlearn: 0.2095502\ttotal: 8.81s\tremaining: 8.33s\n",
      "514:\tlearn: 0.2095205\ttotal: 8.83s\tremaining: 8.32s\n",
      "515:\tlearn: 0.2095062\ttotal: 8.85s\tremaining: 8.3s\n",
      "516:\tlearn: 0.2094904\ttotal: 8.87s\tremaining: 8.28s\n",
      "517:\tlearn: 0.2094790\ttotal: 8.88s\tremaining: 8.27s\n",
      "518:\tlearn: 0.2094414\ttotal: 8.9s\tremaining: 8.25s\n",
      "519:\tlearn: 0.2094328\ttotal: 8.92s\tremaining: 8.23s\n",
      "520:\tlearn: 0.2094104\ttotal: 8.93s\tremaining: 8.21s\n",
      "521:\tlearn: 0.2093865\ttotal: 8.95s\tremaining: 8.2s\n",
      "522:\tlearn: 0.2093376\ttotal: 8.97s\tremaining: 8.18s\n",
      "523:\tlearn: 0.2093294\ttotal: 8.98s\tremaining: 8.16s\n",
      "524:\tlearn: 0.2092717\ttotal: 9s\tremaining: 8.14s\n",
      "525:\tlearn: 0.2092487\ttotal: 9.02s\tremaining: 8.13s\n",
      "526:\tlearn: 0.2092152\ttotal: 9.04s\tremaining: 8.11s\n",
      "527:\tlearn: 0.2092003\ttotal: 9.05s\tremaining: 8.09s\n",
      "528:\tlearn: 0.2091709\ttotal: 9.07s\tremaining: 8.08s\n",
      "529:\tlearn: 0.2091593\ttotal: 9.09s\tremaining: 8.06s\n",
      "530:\tlearn: 0.2091110\ttotal: 9.1s\tremaining: 8.04s\n",
      "531:\tlearn: 0.2090608\ttotal: 9.12s\tremaining: 8.02s\n",
      "532:\tlearn: 0.2090237\ttotal: 9.14s\tremaining: 8.01s\n",
      "533:\tlearn: 0.2090013\ttotal: 9.15s\tremaining: 7.99s\n",
      "534:\tlearn: 0.2089937\ttotal: 9.17s\tremaining: 7.97s\n",
      "535:\tlearn: 0.2089725\ttotal: 9.19s\tremaining: 7.95s\n",
      "536:\tlearn: 0.2089383\ttotal: 9.2s\tremaining: 7.93s\n",
      "537:\tlearn: 0.2089043\ttotal: 9.22s\tremaining: 7.92s\n",
      "538:\tlearn: 0.2088733\ttotal: 9.24s\tremaining: 7.9s\n",
      "539:\tlearn: 0.2088590\ttotal: 9.26s\tremaining: 7.88s\n",
      "540:\tlearn: 0.2088362\ttotal: 9.27s\tremaining: 7.87s\n",
      "541:\tlearn: 0.2088089\ttotal: 9.29s\tremaining: 7.85s\n",
      "542:\tlearn: 0.2087882\ttotal: 9.31s\tremaining: 7.83s\n",
      "543:\tlearn: 0.2087471\ttotal: 9.32s\tremaining: 7.82s\n",
      "544:\tlearn: 0.2087166\ttotal: 9.34s\tremaining: 7.8s\n",
      "545:\tlearn: 0.2087056\ttotal: 9.36s\tremaining: 7.78s\n",
      "546:\tlearn: 0.2086798\ttotal: 9.38s\tremaining: 7.77s\n",
      "547:\tlearn: 0.2086270\ttotal: 9.39s\tremaining: 7.75s\n",
      "548:\tlearn: 0.2085829\ttotal: 9.41s\tremaining: 7.73s\n",
      "549:\tlearn: 0.2085477\ttotal: 9.43s\tremaining: 7.71s\n",
      "550:\tlearn: 0.2085409\ttotal: 9.45s\tremaining: 7.7s\n",
      "551:\tlearn: 0.2085152\ttotal: 9.47s\tremaining: 7.68s\n",
      "552:\tlearn: 0.2084691\ttotal: 9.48s\tremaining: 7.67s\n",
      "553:\tlearn: 0.2084449\ttotal: 9.5s\tremaining: 7.65s\n",
      "554:\tlearn: 0.2084156\ttotal: 9.52s\tremaining: 7.63s\n",
      "555:\tlearn: 0.2083969\ttotal: 9.53s\tremaining: 7.61s\n",
      "556:\tlearn: 0.2083862\ttotal: 9.55s\tremaining: 7.6s\n",
      "557:\tlearn: 0.2083640\ttotal: 9.57s\tremaining: 7.58s\n",
      "558:\tlearn: 0.2083488\ttotal: 9.58s\tremaining: 7.56s\n",
      "559:\tlearn: 0.2082938\ttotal: 9.6s\tremaining: 7.54s\n",
      "560:\tlearn: 0.2082657\ttotal: 9.62s\tremaining: 7.53s\n",
      "561:\tlearn: 0.2082194\ttotal: 9.63s\tremaining: 7.51s\n",
      "562:\tlearn: 0.2081944\ttotal: 9.65s\tremaining: 7.49s\n",
      "563:\tlearn: 0.2081729\ttotal: 9.67s\tremaining: 7.47s\n",
      "564:\tlearn: 0.2081525\ttotal: 9.69s\tremaining: 7.46s\n",
      "565:\tlearn: 0.2081254\ttotal: 9.7s\tremaining: 7.44s\n",
      "566:\tlearn: 0.2081059\ttotal: 9.72s\tremaining: 7.42s\n",
      "567:\tlearn: 0.2080766\ttotal: 9.74s\tremaining: 7.41s\n",
      "568:\tlearn: 0.2080254\ttotal: 9.75s\tremaining: 7.39s\n",
      "569:\tlearn: 0.2080021\ttotal: 9.77s\tremaining: 7.37s\n",
      "570:\tlearn: 0.2079855\ttotal: 9.79s\tremaining: 7.35s\n",
      "571:\tlearn: 0.2079586\ttotal: 9.8s\tremaining: 7.33s\n",
      "572:\tlearn: 0.2079176\ttotal: 9.82s\tremaining: 7.32s\n",
      "573:\tlearn: 0.2079019\ttotal: 9.84s\tremaining: 7.3s\n",
      "574:\tlearn: 0.2078598\ttotal: 9.85s\tremaining: 7.28s\n",
      "575:\tlearn: 0.2078359\ttotal: 9.87s\tremaining: 7.27s\n",
      "576:\tlearn: 0.2078241\ttotal: 9.89s\tremaining: 7.25s\n",
      "577:\tlearn: 0.2078128\ttotal: 9.91s\tremaining: 7.23s\n",
      "578:\tlearn: 0.2078064\ttotal: 9.93s\tremaining: 7.22s\n",
      "579:\tlearn: 0.2077716\ttotal: 9.94s\tremaining: 7.2s\n",
      "580:\tlearn: 0.2077542\ttotal: 9.96s\tremaining: 7.18s\n",
      "581:\tlearn: 0.2077244\ttotal: 9.98s\tremaining: 7.16s\n",
      "582:\tlearn: 0.2076889\ttotal: 9.99s\tremaining: 7.15s\n",
      "583:\tlearn: 0.2076713\ttotal: 10s\tremaining: 7.13s\n",
      "584:\tlearn: 0.2076585\ttotal: 10s\tremaining: 7.11s\n",
      "585:\tlearn: 0.2075990\ttotal: 10s\tremaining: 7.1s\n",
      "586:\tlearn: 0.2075647\ttotal: 10.1s\tremaining: 7.08s\n",
      "587:\tlearn: 0.2075277\ttotal: 10.1s\tremaining: 7.06s\n",
      "588:\tlearn: 0.2075167\ttotal: 10.1s\tremaining: 7.04s\n",
      "589:\tlearn: 0.2075017\ttotal: 10.1s\tremaining: 7.03s\n",
      "590:\tlearn: 0.2074898\ttotal: 10.1s\tremaining: 7.01s\n",
      "591:\tlearn: 0.2074504\ttotal: 10.1s\tremaining: 7s\n",
      "592:\tlearn: 0.2074321\ttotal: 10.2s\tremaining: 6.98s\n",
      "593:\tlearn: 0.2073940\ttotal: 10.2s\tremaining: 6.96s\n",
      "594:\tlearn: 0.2073531\ttotal: 10.2s\tremaining: 6.94s\n",
      "595:\tlearn: 0.2073275\ttotal: 10.2s\tremaining: 6.92s\n",
      "596:\tlearn: 0.2073134\ttotal: 10.2s\tremaining: 6.91s\n",
      "597:\tlearn: 0.2072761\ttotal: 10.2s\tremaining: 6.89s\n",
      "598:\tlearn: 0.2072656\ttotal: 10.3s\tremaining: 6.87s\n",
      "599:\tlearn: 0.2072255\ttotal: 10.3s\tremaining: 6.85s\n",
      "600:\tlearn: 0.2072104\ttotal: 10.3s\tremaining: 6.84s\n",
      "601:\tlearn: 0.2071815\ttotal: 10.3s\tremaining: 6.82s\n",
      "602:\tlearn: 0.2071696\ttotal: 10.3s\tremaining: 6.8s\n",
      "603:\tlearn: 0.2071367\ttotal: 10.4s\tremaining: 6.79s\n",
      "604:\tlearn: 0.2071030\ttotal: 10.4s\tremaining: 6.77s\n",
      "605:\tlearn: 0.2070743\ttotal: 10.4s\tremaining: 6.75s\n",
      "606:\tlearn: 0.2070537\ttotal: 10.4s\tremaining: 6.73s\n",
      "607:\tlearn: 0.2070221\ttotal: 10.4s\tremaining: 6.72s\n",
      "608:\tlearn: 0.2069868\ttotal: 10.4s\tremaining: 6.7s\n",
      "609:\tlearn: 0.2069726\ttotal: 10.5s\tremaining: 6.68s\n",
      "610:\tlearn: 0.2069467\ttotal: 10.5s\tremaining: 6.66s\n",
      "611:\tlearn: 0.2069225\ttotal: 10.5s\tremaining: 6.65s\n",
      "612:\tlearn: 0.2069085\ttotal: 10.5s\tremaining: 6.63s\n",
      "613:\tlearn: 0.2068717\ttotal: 10.5s\tremaining: 6.61s\n",
      "614:\tlearn: 0.2068514\ttotal: 10.5s\tremaining: 6.6s\n",
      "615:\tlearn: 0.2068217\ttotal: 10.6s\tremaining: 6.58s\n",
      "616:\tlearn: 0.2067933\ttotal: 10.6s\tremaining: 6.56s\n",
      "617:\tlearn: 0.2067710\ttotal: 10.6s\tremaining: 6.55s\n",
      "618:\tlearn: 0.2067360\ttotal: 10.6s\tremaining: 6.53s\n",
      "619:\tlearn: 0.2067176\ttotal: 10.6s\tremaining: 6.51s\n",
      "620:\tlearn: 0.2066727\ttotal: 10.6s\tremaining: 6.5s\n",
      "621:\tlearn: 0.2066543\ttotal: 10.7s\tremaining: 6.48s\n",
      "622:\tlearn: 0.2066386\ttotal: 10.7s\tremaining: 6.46s\n",
      "623:\tlearn: 0.2066287\ttotal: 10.7s\tremaining: 6.44s\n",
      "624:\tlearn: 0.2066181\ttotal: 10.7s\tremaining: 6.43s\n",
      "625:\tlearn: 0.2066071\ttotal: 10.7s\tremaining: 6.41s\n",
      "626:\tlearn: 0.2065734\ttotal: 10.7s\tremaining: 6.39s\n",
      "627:\tlearn: 0.2065633\ttotal: 10.8s\tremaining: 6.38s\n",
      "628:\tlearn: 0.2065256\ttotal: 10.8s\tremaining: 6.36s\n",
      "629:\tlearn: 0.2064993\ttotal: 10.8s\tremaining: 6.34s\n",
      "630:\tlearn: 0.2064906\ttotal: 10.8s\tremaining: 6.32s\n",
      "631:\tlearn: 0.2064666\ttotal: 10.8s\tremaining: 6.3s\n",
      "632:\tlearn: 0.2064573\ttotal: 10.8s\tremaining: 6.29s\n",
      "633:\tlearn: 0.2064468\ttotal: 10.9s\tremaining: 6.27s\n",
      "634:\tlearn: 0.2064382\ttotal: 10.9s\tremaining: 6.25s\n",
      "635:\tlearn: 0.2064212\ttotal: 10.9s\tremaining: 6.24s\n",
      "636:\tlearn: 0.2064106\ttotal: 10.9s\tremaining: 6.22s\n",
      "637:\tlearn: 0.2063534\ttotal: 10.9s\tremaining: 6.2s\n",
      "638:\tlearn: 0.2063446\ttotal: 11s\tremaining: 6.19s\n",
      "639:\tlearn: 0.2063367\ttotal: 11s\tremaining: 6.17s\n",
      "640:\tlearn: 0.2063192\ttotal: 11s\tremaining: 6.15s\n",
      "641:\tlearn: 0.2063123\ttotal: 11s\tremaining: 6.13s\n",
      "642:\tlearn: 0.2063019\ttotal: 11s\tremaining: 6.12s\n",
      "643:\tlearn: 0.2062842\ttotal: 11s\tremaining: 6.1s\n",
      "644:\tlearn: 0.2062636\ttotal: 11.1s\tremaining: 6.08s\n",
      "645:\tlearn: 0.2062463\ttotal: 11.1s\tremaining: 6.07s\n",
      "646:\tlearn: 0.2062312\ttotal: 11.1s\tremaining: 6.05s\n",
      "647:\tlearn: 0.2062124\ttotal: 11.1s\tremaining: 6.03s\n",
      "648:\tlearn: 0.2061732\ttotal: 11.1s\tremaining: 6.01s\n",
      "649:\tlearn: 0.2061418\ttotal: 11.1s\tremaining: 6s\n",
      "650:\tlearn: 0.2061231\ttotal: 11.2s\tremaining: 5.98s\n",
      "651:\tlearn: 0.2061068\ttotal: 11.2s\tremaining: 5.96s\n",
      "652:\tlearn: 0.2060571\ttotal: 11.2s\tremaining: 5.95s\n",
      "653:\tlearn: 0.2060129\ttotal: 11.2s\tremaining: 5.93s\n",
      "654:\tlearn: 0.2059714\ttotal: 11.2s\tremaining: 5.91s\n",
      "655:\tlearn: 0.2059509\ttotal: 11.2s\tremaining: 5.89s\n",
      "656:\tlearn: 0.2059410\ttotal: 11.3s\tremaining: 5.88s\n",
      "657:\tlearn: 0.2059292\ttotal: 11.3s\tremaining: 5.86s\n",
      "658:\tlearn: 0.2058974\ttotal: 11.3s\tremaining: 5.84s\n",
      "659:\tlearn: 0.2058677\ttotal: 11.3s\tremaining: 5.82s\n",
      "660:\tlearn: 0.2058344\ttotal: 11.3s\tremaining: 5.81s\n",
      "661:\tlearn: 0.2058124\ttotal: 11.3s\tremaining: 5.79s\n",
      "662:\tlearn: 0.2057664\ttotal: 11.4s\tremaining: 5.77s\n",
      "663:\tlearn: 0.2057541\ttotal: 11.4s\tremaining: 5.76s\n",
      "664:\tlearn: 0.2057264\ttotal: 11.4s\tremaining: 5.74s\n",
      "665:\tlearn: 0.2057097\ttotal: 11.4s\tremaining: 5.72s\n",
      "666:\tlearn: 0.2057024\ttotal: 11.4s\tremaining: 5.71s\n",
      "667:\tlearn: 0.2056959\ttotal: 11.4s\tremaining: 5.69s\n",
      "668:\tlearn: 0.2056775\ttotal: 11.5s\tremaining: 5.67s\n",
      "669:\tlearn: 0.2056621\ttotal: 11.5s\tremaining: 5.65s\n",
      "670:\tlearn: 0.2056330\ttotal: 11.5s\tremaining: 5.64s\n",
      "671:\tlearn: 0.2056152\ttotal: 11.5s\tremaining: 5.62s\n",
      "672:\tlearn: 0.2055740\ttotal: 11.5s\tremaining: 5.6s\n",
      "673:\tlearn: 0.2055413\ttotal: 11.6s\tremaining: 5.59s\n",
      "674:\tlearn: 0.2055104\ttotal: 11.6s\tremaining: 5.57s\n",
      "675:\tlearn: 0.2054802\ttotal: 11.6s\tremaining: 5.55s\n",
      "676:\tlearn: 0.2054503\ttotal: 11.6s\tremaining: 5.54s\n",
      "677:\tlearn: 0.2054066\ttotal: 11.6s\tremaining: 5.52s\n",
      "678:\tlearn: 0.2053834\ttotal: 11.6s\tremaining: 5.5s\n",
      "679:\tlearn: 0.2053623\ttotal: 11.7s\tremaining: 5.49s\n",
      "680:\tlearn: 0.2053283\ttotal: 11.7s\tremaining: 5.47s\n",
      "681:\tlearn: 0.2052837\ttotal: 11.7s\tremaining: 5.45s\n",
      "682:\tlearn: 0.2052705\ttotal: 11.7s\tremaining: 5.44s\n",
      "683:\tlearn: 0.2052635\ttotal: 11.7s\tremaining: 5.42s\n",
      "684:\tlearn: 0.2052390\ttotal: 11.8s\tremaining: 5.41s\n",
      "685:\tlearn: 0.2052236\ttotal: 11.8s\tremaining: 5.39s\n",
      "686:\tlearn: 0.2051946\ttotal: 11.8s\tremaining: 5.37s\n",
      "687:\tlearn: 0.2051620\ttotal: 11.8s\tremaining: 5.36s\n",
      "688:\tlearn: 0.2051223\ttotal: 11.8s\tremaining: 5.34s\n",
      "689:\tlearn: 0.2051029\ttotal: 11.8s\tremaining: 5.32s\n",
      "690:\tlearn: 0.2050863\ttotal: 11.9s\tremaining: 5.3s\n",
      "691:\tlearn: 0.2050427\ttotal: 11.9s\tremaining: 5.29s\n",
      "692:\tlearn: 0.2050204\ttotal: 11.9s\tremaining: 5.27s\n",
      "693:\tlearn: 0.2049984\ttotal: 11.9s\tremaining: 5.25s\n",
      "694:\tlearn: 0.2049560\ttotal: 11.9s\tremaining: 5.24s\n",
      "695:\tlearn: 0.2049431\ttotal: 11.9s\tremaining: 5.22s\n",
      "696:\tlearn: 0.2049081\ttotal: 12s\tremaining: 5.2s\n",
      "697:\tlearn: 0.2049016\ttotal: 12s\tremaining: 5.18s\n",
      "698:\tlearn: 0.2048602\ttotal: 12s\tremaining: 5.17s\n",
      "699:\tlearn: 0.2048278\ttotal: 12s\tremaining: 5.15s\n",
      "700:\tlearn: 0.2047914\ttotal: 12s\tremaining: 5.13s\n",
      "701:\tlearn: 0.2047618\ttotal: 12.1s\tremaining: 5.12s\n",
      "702:\tlearn: 0.2047405\ttotal: 12.1s\tremaining: 5.1s\n",
      "703:\tlearn: 0.2047172\ttotal: 12.1s\tremaining: 5.08s\n",
      "704:\tlearn: 0.2046844\ttotal: 12.1s\tremaining: 5.07s\n",
      "705:\tlearn: 0.2046637\ttotal: 12.1s\tremaining: 5.05s\n",
      "706:\tlearn: 0.2046296\ttotal: 12.1s\tremaining: 5.03s\n",
      "707:\tlearn: 0.2045985\ttotal: 12.2s\tremaining: 5.01s\n",
      "708:\tlearn: 0.2045338\ttotal: 12.2s\tremaining: 5s\n",
      "709:\tlearn: 0.2045135\ttotal: 12.2s\tremaining: 4.98s\n",
      "710:\tlearn: 0.2044811\ttotal: 12.2s\tremaining: 4.96s\n",
      "711:\tlearn: 0.2044633\ttotal: 12.2s\tremaining: 4.94s\n",
      "712:\tlearn: 0.2044364\ttotal: 12.2s\tremaining: 4.93s\n",
      "713:\tlearn: 0.2044163\ttotal: 12.3s\tremaining: 4.91s\n",
      "714:\tlearn: 0.2043749\ttotal: 12.3s\tremaining: 4.89s\n",
      "715:\tlearn: 0.2043496\ttotal: 12.3s\tremaining: 4.87s\n",
      "716:\tlearn: 0.2043197\ttotal: 12.3s\tremaining: 4.86s\n",
      "717:\tlearn: 0.2042859\ttotal: 12.3s\tremaining: 4.84s\n",
      "718:\tlearn: 0.2042619\ttotal: 12.3s\tremaining: 4.82s\n",
      "719:\tlearn: 0.2042420\ttotal: 12.4s\tremaining: 4.8s\n",
      "720:\tlearn: 0.2042242\ttotal: 12.4s\tremaining: 4.79s\n",
      "721:\tlearn: 0.2042080\ttotal: 12.4s\tremaining: 4.77s\n",
      "722:\tlearn: 0.2041883\ttotal: 12.4s\tremaining: 4.75s\n",
      "723:\tlearn: 0.2041697\ttotal: 12.4s\tremaining: 4.74s\n",
      "724:\tlearn: 0.2041377\ttotal: 12.4s\tremaining: 4.72s\n",
      "725:\tlearn: 0.2041103\ttotal: 12.5s\tremaining: 4.7s\n",
      "726:\tlearn: 0.2040888\ttotal: 12.5s\tremaining: 4.69s\n",
      "727:\tlearn: 0.2040604\ttotal: 12.5s\tremaining: 4.67s\n",
      "728:\tlearn: 0.2040400\ttotal: 12.5s\tremaining: 4.65s\n",
      "729:\tlearn: 0.2040045\ttotal: 12.5s\tremaining: 4.63s\n",
      "730:\tlearn: 0.2039665\ttotal: 12.5s\tremaining: 4.62s\n",
      "731:\tlearn: 0.2039508\ttotal: 12.6s\tremaining: 4.6s\n",
      "732:\tlearn: 0.2039280\ttotal: 12.6s\tremaining: 4.58s\n",
      "733:\tlearn: 0.2039045\ttotal: 12.6s\tremaining: 4.57s\n",
      "734:\tlearn: 0.2038681\ttotal: 12.6s\tremaining: 4.55s\n",
      "735:\tlearn: 0.2038435\ttotal: 12.6s\tremaining: 4.53s\n",
      "736:\tlearn: 0.2038266\ttotal: 12.7s\tremaining: 4.51s\n",
      "737:\tlearn: 0.2037914\ttotal: 12.7s\tremaining: 4.5s\n",
      "738:\tlearn: 0.2037756\ttotal: 12.7s\tremaining: 4.48s\n",
      "739:\tlearn: 0.2037589\ttotal: 12.7s\tremaining: 4.46s\n",
      "740:\tlearn: 0.2037439\ttotal: 12.7s\tremaining: 4.45s\n",
      "741:\tlearn: 0.2037248\ttotal: 12.7s\tremaining: 4.43s\n",
      "742:\tlearn: 0.2036663\ttotal: 12.8s\tremaining: 4.41s\n",
      "743:\tlearn: 0.2036590\ttotal: 12.8s\tremaining: 4.39s\n",
      "744:\tlearn: 0.2036195\ttotal: 12.8s\tremaining: 4.38s\n",
      "745:\tlearn: 0.2035927\ttotal: 12.8s\tremaining: 4.36s\n",
      "746:\tlearn: 0.2035774\ttotal: 12.8s\tremaining: 4.34s\n",
      "747:\tlearn: 0.2035564\ttotal: 12.8s\tremaining: 4.33s\n",
      "748:\tlearn: 0.2035403\ttotal: 12.9s\tremaining: 4.31s\n",
      "749:\tlearn: 0.2035082\ttotal: 12.9s\tremaining: 4.29s\n",
      "750:\tlearn: 0.2034916\ttotal: 12.9s\tremaining: 4.27s\n",
      "751:\tlearn: 0.2034688\ttotal: 12.9s\tremaining: 4.26s\n",
      "752:\tlearn: 0.2034316\ttotal: 12.9s\tremaining: 4.24s\n",
      "753:\tlearn: 0.2034122\ttotal: 12.9s\tremaining: 4.22s\n",
      "754:\tlearn: 0.2033615\ttotal: 13s\tremaining: 4.21s\n",
      "755:\tlearn: 0.2033365\ttotal: 13s\tremaining: 4.19s\n",
      "756:\tlearn: 0.2033062\ttotal: 13s\tremaining: 4.17s\n",
      "757:\tlearn: 0.2032940\ttotal: 13s\tremaining: 4.15s\n",
      "758:\tlearn: 0.2032574\ttotal: 13s\tremaining: 4.14s\n",
      "759:\tlearn: 0.2032318\ttotal: 13s\tremaining: 4.12s\n",
      "760:\tlearn: 0.2032118\ttotal: 13.1s\tremaining: 4.11s\n",
      "761:\tlearn: 0.2031548\ttotal: 13.1s\tremaining: 4.09s\n",
      "762:\tlearn: 0.2031014\ttotal: 13.1s\tremaining: 4.07s\n",
      "763:\tlearn: 0.2030668\ttotal: 13.1s\tremaining: 4.05s\n",
      "764:\tlearn: 0.2030342\ttotal: 13.1s\tremaining: 4.04s\n",
      "765:\tlearn: 0.2030033\ttotal: 13.2s\tremaining: 4.02s\n",
      "766:\tlearn: 0.2029731\ttotal: 13.2s\tremaining: 4s\n",
      "767:\tlearn: 0.2029398\ttotal: 13.2s\tremaining: 3.98s\n",
      "768:\tlearn: 0.2029209\ttotal: 13.2s\tremaining: 3.97s\n",
      "769:\tlearn: 0.2028820\ttotal: 13.2s\tremaining: 3.95s\n",
      "770:\tlearn: 0.2028531\ttotal: 13.2s\tremaining: 3.93s\n",
      "771:\tlearn: 0.2028283\ttotal: 13.3s\tremaining: 3.92s\n",
      "772:\tlearn: 0.2028134\ttotal: 13.3s\tremaining: 3.9s\n",
      "773:\tlearn: 0.2027867\ttotal: 13.3s\tremaining: 3.88s\n",
      "774:\tlearn: 0.2027501\ttotal: 13.3s\tremaining: 3.87s\n",
      "775:\tlearn: 0.2027243\ttotal: 13.3s\tremaining: 3.85s\n",
      "776:\tlearn: 0.2026997\ttotal: 13.4s\tremaining: 3.83s\n",
      "777:\tlearn: 0.2026728\ttotal: 13.4s\tremaining: 3.81s\n",
      "778:\tlearn: 0.2026499\ttotal: 13.4s\tremaining: 3.8s\n",
      "779:\tlearn: 0.2026236\ttotal: 13.4s\tremaining: 3.78s\n",
      "780:\tlearn: 0.2025737\ttotal: 13.4s\tremaining: 3.76s\n",
      "781:\tlearn: 0.2025324\ttotal: 13.4s\tremaining: 3.75s\n",
      "782:\tlearn: 0.2024852\ttotal: 13.5s\tremaining: 3.73s\n",
      "783:\tlearn: 0.2024669\ttotal: 13.5s\tremaining: 3.71s\n",
      "784:\tlearn: 0.2024430\ttotal: 13.5s\tremaining: 3.69s\n",
      "785:\tlearn: 0.2024042\ttotal: 13.5s\tremaining: 3.68s\n",
      "786:\tlearn: 0.2023529\ttotal: 13.5s\tremaining: 3.66s\n",
      "787:\tlearn: 0.2023282\ttotal: 13.5s\tremaining: 3.64s\n",
      "788:\tlearn: 0.2022838\ttotal: 13.6s\tremaining: 3.63s\n",
      "789:\tlearn: 0.2022646\ttotal: 13.6s\tremaining: 3.61s\n",
      "790:\tlearn: 0.2022455\ttotal: 13.6s\tremaining: 3.59s\n",
      "791:\tlearn: 0.2022212\ttotal: 13.6s\tremaining: 3.57s\n",
      "792:\tlearn: 0.2021983\ttotal: 13.6s\tremaining: 3.56s\n",
      "793:\tlearn: 0.2021625\ttotal: 13.6s\tremaining: 3.54s\n",
      "794:\tlearn: 0.2021289\ttotal: 13.7s\tremaining: 3.52s\n",
      "795:\tlearn: 0.2020963\ttotal: 13.7s\tremaining: 3.51s\n",
      "796:\tlearn: 0.2020813\ttotal: 13.7s\tremaining: 3.49s\n",
      "797:\tlearn: 0.2020299\ttotal: 13.7s\tremaining: 3.47s\n",
      "798:\tlearn: 0.2020083\ttotal: 13.7s\tremaining: 3.46s\n",
      "799:\tlearn: 0.2019879\ttotal: 13.8s\tremaining: 3.44s\n",
      "800:\tlearn: 0.2019702\ttotal: 13.8s\tremaining: 3.42s\n",
      "801:\tlearn: 0.2019522\ttotal: 13.8s\tremaining: 3.41s\n",
      "802:\tlearn: 0.2019147\ttotal: 13.8s\tremaining: 3.39s\n",
      "803:\tlearn: 0.2018935\ttotal: 13.8s\tremaining: 3.37s\n",
      "804:\tlearn: 0.2018795\ttotal: 13.9s\tremaining: 3.35s\n",
      "805:\tlearn: 0.2018621\ttotal: 13.9s\tremaining: 3.34s\n",
      "806:\tlearn: 0.2018455\ttotal: 13.9s\tremaining: 3.32s\n",
      "807:\tlearn: 0.2017939\ttotal: 13.9s\tremaining: 3.3s\n",
      "808:\tlearn: 0.2017594\ttotal: 13.9s\tremaining: 3.29s\n",
      "809:\tlearn: 0.2017439\ttotal: 13.9s\tremaining: 3.27s\n",
      "810:\tlearn: 0.2017344\ttotal: 14s\tremaining: 3.25s\n",
      "811:\tlearn: 0.2017175\ttotal: 14s\tremaining: 3.23s\n",
      "812:\tlearn: 0.2016732\ttotal: 14s\tremaining: 3.22s\n",
      "813:\tlearn: 0.2016549\ttotal: 14s\tremaining: 3.2s\n",
      "814:\tlearn: 0.2016339\ttotal: 14s\tremaining: 3.18s\n",
      "815:\tlearn: 0.2015985\ttotal: 14s\tremaining: 3.17s\n",
      "816:\tlearn: 0.2015443\ttotal: 14.1s\tremaining: 3.15s\n",
      "817:\tlearn: 0.2015138\ttotal: 14.1s\tremaining: 3.13s\n",
      "818:\tlearn: 0.2014914\ttotal: 14.1s\tremaining: 3.11s\n",
      "819:\tlearn: 0.2014637\ttotal: 14.1s\tremaining: 3.1s\n",
      "820:\tlearn: 0.2014418\ttotal: 14.1s\tremaining: 3.08s\n",
      "821:\tlearn: 0.2014012\ttotal: 14.2s\tremaining: 3.06s\n",
      "822:\tlearn: 0.2013887\ttotal: 14.2s\tremaining: 3.05s\n",
      "823:\tlearn: 0.2013418\ttotal: 14.2s\tremaining: 3.03s\n",
      "824:\tlearn: 0.2013145\ttotal: 14.2s\tremaining: 3.01s\n",
      "825:\tlearn: 0.2012984\ttotal: 14.2s\tremaining: 3s\n",
      "826:\tlearn: 0.2012528\ttotal: 14.2s\tremaining: 2.98s\n",
      "827:\tlearn: 0.2012343\ttotal: 14.3s\tremaining: 2.96s\n",
      "828:\tlearn: 0.2012112\ttotal: 14.3s\tremaining: 2.94s\n",
      "829:\tlearn: 0.2011857\ttotal: 14.3s\tremaining: 2.92s\n",
      "830:\tlearn: 0.2011782\ttotal: 14.3s\tremaining: 2.91s\n",
      "831:\tlearn: 0.2011275\ttotal: 14.3s\tremaining: 2.89s\n",
      "832:\tlearn: 0.2011137\ttotal: 14.3s\tremaining: 2.87s\n",
      "833:\tlearn: 0.2010906\ttotal: 14.4s\tremaining: 2.86s\n",
      "834:\tlearn: 0.2010703\ttotal: 14.4s\tremaining: 2.84s\n",
      "835:\tlearn: 0.2010568\ttotal: 14.4s\tremaining: 2.82s\n",
      "836:\tlearn: 0.2010291\ttotal: 14.4s\tremaining: 2.81s\n",
      "837:\tlearn: 0.2010150\ttotal: 14.4s\tremaining: 2.79s\n",
      "838:\tlearn: 0.2009901\ttotal: 14.4s\tremaining: 2.77s\n",
      "839:\tlearn: 0.2009726\ttotal: 14.5s\tremaining: 2.75s\n",
      "840:\tlearn: 0.2009539\ttotal: 14.5s\tremaining: 2.74s\n",
      "841:\tlearn: 0.2009360\ttotal: 14.5s\tremaining: 2.72s\n",
      "842:\tlearn: 0.2009180\ttotal: 14.5s\tremaining: 2.7s\n",
      "843:\tlearn: 0.2009068\ttotal: 14.5s\tremaining: 2.68s\n",
      "844:\tlearn: 0.2008783\ttotal: 14.5s\tremaining: 2.67s\n",
      "845:\tlearn: 0.2008661\ttotal: 14.6s\tremaining: 2.65s\n",
      "846:\tlearn: 0.2008498\ttotal: 14.6s\tremaining: 2.63s\n",
      "847:\tlearn: 0.2008317\ttotal: 14.6s\tremaining: 2.62s\n",
      "848:\tlearn: 0.2008100\ttotal: 14.6s\tremaining: 2.6s\n",
      "849:\tlearn: 0.2008030\ttotal: 14.6s\tremaining: 2.58s\n",
      "850:\tlearn: 0.2007709\ttotal: 14.6s\tremaining: 2.56s\n",
      "851:\tlearn: 0.2007606\ttotal: 14.7s\tremaining: 2.55s\n",
      "852:\tlearn: 0.2007397\ttotal: 14.7s\tremaining: 2.53s\n",
      "853:\tlearn: 0.2007162\ttotal: 14.7s\tremaining: 2.51s\n",
      "854:\tlearn: 0.2006982\ttotal: 14.7s\tremaining: 2.5s\n",
      "855:\tlearn: 0.2006784\ttotal: 14.7s\tremaining: 2.48s\n",
      "856:\tlearn: 0.2006555\ttotal: 14.7s\tremaining: 2.46s\n",
      "857:\tlearn: 0.2006333\ttotal: 14.8s\tremaining: 2.44s\n",
      "858:\tlearn: 0.2006154\ttotal: 14.8s\tremaining: 2.43s\n",
      "859:\tlearn: 0.2006016\ttotal: 14.8s\tremaining: 2.41s\n",
      "860:\tlearn: 0.2005750\ttotal: 14.8s\tremaining: 2.39s\n",
      "861:\tlearn: 0.2005578\ttotal: 14.8s\tremaining: 2.37s\n",
      "862:\tlearn: 0.2005457\ttotal: 14.8s\tremaining: 2.36s\n",
      "863:\tlearn: 0.2004958\ttotal: 14.9s\tremaining: 2.34s\n",
      "864:\tlearn: 0.2004654\ttotal: 14.9s\tremaining: 2.32s\n",
      "865:\tlearn: 0.2004497\ttotal: 14.9s\tremaining: 2.31s\n",
      "866:\tlearn: 0.2004336\ttotal: 14.9s\tremaining: 2.29s\n",
      "867:\tlearn: 0.2004086\ttotal: 14.9s\tremaining: 2.27s\n",
      "868:\tlearn: 0.2003883\ttotal: 15s\tremaining: 2.25s\n",
      "869:\tlearn: 0.2003666\ttotal: 15s\tremaining: 2.24s\n",
      "870:\tlearn: 0.2003505\ttotal: 15s\tremaining: 2.22s\n",
      "871:\tlearn: 0.2003407\ttotal: 15s\tremaining: 2.2s\n",
      "872:\tlearn: 0.2003209\ttotal: 15s\tremaining: 2.19s\n",
      "873:\tlearn: 0.2003004\ttotal: 15s\tremaining: 2.17s\n",
      "874:\tlearn: 0.2002849\ttotal: 15.1s\tremaining: 2.15s\n",
      "875:\tlearn: 0.2002648\ttotal: 15.1s\tremaining: 2.13s\n",
      "876:\tlearn: 0.2002498\ttotal: 15.1s\tremaining: 2.12s\n",
      "877:\tlearn: 0.2002307\ttotal: 15.1s\tremaining: 2.1s\n",
      "878:\tlearn: 0.2001760\ttotal: 15.1s\tremaining: 2.08s\n",
      "879:\tlearn: 0.2001656\ttotal: 15.1s\tremaining: 2.06s\n",
      "880:\tlearn: 0.2001337\ttotal: 15.2s\tremaining: 2.05s\n",
      "881:\tlearn: 0.2001189\ttotal: 15.2s\tremaining: 2.03s\n",
      "882:\tlearn: 0.2001022\ttotal: 15.2s\tremaining: 2.01s\n",
      "883:\tlearn: 0.2000824\ttotal: 15.2s\tremaining: 2s\n",
      "884:\tlearn: 0.2000570\ttotal: 15.2s\tremaining: 1.98s\n",
      "885:\tlearn: 0.2000214\ttotal: 15.2s\tremaining: 1.96s\n",
      "886:\tlearn: 0.2000026\ttotal: 15.3s\tremaining: 1.94s\n",
      "887:\tlearn: 0.1999837\ttotal: 15.3s\tremaining: 1.93s\n",
      "888:\tlearn: 0.1999649\ttotal: 15.3s\tremaining: 1.91s\n",
      "889:\tlearn: 0.1999472\ttotal: 15.3s\tremaining: 1.89s\n",
      "890:\tlearn: 0.1999260\ttotal: 15.3s\tremaining: 1.88s\n",
      "891:\tlearn: 0.1999137\ttotal: 15.3s\tremaining: 1.86s\n",
      "892:\tlearn: 0.1998942\ttotal: 15.4s\tremaining: 1.84s\n",
      "893:\tlearn: 0.1998745\ttotal: 15.4s\tremaining: 1.82s\n",
      "894:\tlearn: 0.1998612\ttotal: 15.4s\tremaining: 1.81s\n",
      "895:\tlearn: 0.1998404\ttotal: 15.4s\tremaining: 1.79s\n",
      "896:\tlearn: 0.1998282\ttotal: 15.4s\tremaining: 1.77s\n",
      "897:\tlearn: 0.1998126\ttotal: 15.5s\tremaining: 1.75s\n",
      "898:\tlearn: 0.1997860\ttotal: 15.5s\tremaining: 1.74s\n",
      "899:\tlearn: 0.1997671\ttotal: 15.5s\tremaining: 1.72s\n",
      "900:\tlearn: 0.1997464\ttotal: 15.5s\tremaining: 1.7s\n",
      "901:\tlearn: 0.1997288\ttotal: 15.5s\tremaining: 1.69s\n",
      "902:\tlearn: 0.1997167\ttotal: 15.5s\tremaining: 1.67s\n",
      "903:\tlearn: 0.1997009\ttotal: 15.6s\tremaining: 1.65s\n",
      "904:\tlearn: 0.1996847\ttotal: 15.6s\tremaining: 1.63s\n",
      "905:\tlearn: 0.1996675\ttotal: 15.6s\tremaining: 1.62s\n",
      "906:\tlearn: 0.1996477\ttotal: 15.6s\tremaining: 1.6s\n",
      "907:\tlearn: 0.1996012\ttotal: 15.6s\tremaining: 1.58s\n",
      "908:\tlearn: 0.1995674\ttotal: 15.6s\tremaining: 1.56s\n",
      "909:\tlearn: 0.1995525\ttotal: 15.7s\tremaining: 1.55s\n",
      "910:\tlearn: 0.1995393\ttotal: 15.7s\tremaining: 1.53s\n",
      "911:\tlearn: 0.1995277\ttotal: 15.7s\tremaining: 1.51s\n",
      "912:\tlearn: 0.1995006\ttotal: 15.7s\tremaining: 1.5s\n",
      "913:\tlearn: 0.1994916\ttotal: 15.7s\tremaining: 1.48s\n",
      "914:\tlearn: 0.1994738\ttotal: 15.7s\tremaining: 1.46s\n",
      "915:\tlearn: 0.1994560\ttotal: 15.8s\tremaining: 1.45s\n",
      "916:\tlearn: 0.1994359\ttotal: 15.8s\tremaining: 1.43s\n",
      "917:\tlearn: 0.1994134\ttotal: 15.8s\tremaining: 1.41s\n",
      "918:\tlearn: 0.1993856\ttotal: 15.8s\tremaining: 1.39s\n",
      "919:\tlearn: 0.1993701\ttotal: 15.8s\tremaining: 1.38s\n",
      "920:\tlearn: 0.1993602\ttotal: 15.8s\tremaining: 1.36s\n",
      "921:\tlearn: 0.1993471\ttotal: 15.9s\tremaining: 1.34s\n",
      "922:\tlearn: 0.1993152\ttotal: 15.9s\tremaining: 1.32s\n",
      "923:\tlearn: 0.1992975\ttotal: 15.9s\tremaining: 1.31s\n",
      "924:\tlearn: 0.1992799\ttotal: 15.9s\tremaining: 1.29s\n",
      "925:\tlearn: 0.1992602\ttotal: 15.9s\tremaining: 1.27s\n",
      "926:\tlearn: 0.1992389\ttotal: 15.9s\tremaining: 1.25s\n",
      "927:\tlearn: 0.1992243\ttotal: 16s\tremaining: 1.24s\n",
      "928:\tlearn: 0.1992184\ttotal: 16s\tremaining: 1.22s\n",
      "929:\tlearn: 0.1991995\ttotal: 16s\tremaining: 1.2s\n",
      "930:\tlearn: 0.1991852\ttotal: 16s\tremaining: 1.19s\n",
      "931:\tlearn: 0.1991660\ttotal: 16s\tremaining: 1.17s\n",
      "932:\tlearn: 0.1991347\ttotal: 16.1s\tremaining: 1.15s\n",
      "933:\tlearn: 0.1991168\ttotal: 16.1s\tremaining: 1.14s\n",
      "934:\tlearn: 0.1991011\ttotal: 16.1s\tremaining: 1.12s\n",
      "935:\tlearn: 0.1990813\ttotal: 16.1s\tremaining: 1.1s\n",
      "936:\tlearn: 0.1990643\ttotal: 16.1s\tremaining: 1.08s\n",
      "937:\tlearn: 0.1990396\ttotal: 16.1s\tremaining: 1.07s\n",
      "938:\tlearn: 0.1990196\ttotal: 16.2s\tremaining: 1.05s\n",
      "939:\tlearn: 0.1990140\ttotal: 16.2s\tremaining: 1.03s\n",
      "940:\tlearn: 0.1989890\ttotal: 16.2s\tremaining: 1.01s\n",
      "941:\tlearn: 0.1989677\ttotal: 16.2s\tremaining: 998ms\n",
      "942:\tlearn: 0.1989489\ttotal: 16.2s\tremaining: 981ms\n",
      "943:\tlearn: 0.1989340\ttotal: 16.2s\tremaining: 963ms\n",
      "944:\tlearn: 0.1989013\ttotal: 16.3s\tremaining: 946ms\n",
      "945:\tlearn: 0.1988907\ttotal: 16.3s\tremaining: 929ms\n",
      "946:\tlearn: 0.1988593\ttotal: 16.3s\tremaining: 912ms\n",
      "947:\tlearn: 0.1988157\ttotal: 16.3s\tremaining: 895ms\n",
      "948:\tlearn: 0.1988012\ttotal: 16.3s\tremaining: 877ms\n",
      "949:\tlearn: 0.1987737\ttotal: 16.3s\tremaining: 860ms\n",
      "950:\tlearn: 0.1987542\ttotal: 16.4s\tremaining: 843ms\n",
      "951:\tlearn: 0.1987401\ttotal: 16.4s\tremaining: 826ms\n",
      "952:\tlearn: 0.1987304\ttotal: 16.4s\tremaining: 809ms\n",
      "953:\tlearn: 0.1986999\ttotal: 16.4s\tremaining: 792ms\n",
      "954:\tlearn: 0.1986839\ttotal: 16.4s\tremaining: 774ms\n",
      "955:\tlearn: 0.1986554\ttotal: 16.5s\tremaining: 757ms\n",
      "956:\tlearn: 0.1986144\ttotal: 16.5s\tremaining: 740ms\n",
      "957:\tlearn: 0.1986037\ttotal: 16.5s\tremaining: 723ms\n",
      "958:\tlearn: 0.1985863\ttotal: 16.5s\tremaining: 705ms\n",
      "959:\tlearn: 0.1985675\ttotal: 16.5s\tremaining: 688ms\n",
      "960:\tlearn: 0.1985513\ttotal: 16.5s\tremaining: 671ms\n",
      "961:\tlearn: 0.1985371\ttotal: 16.6s\tremaining: 654ms\n",
      "962:\tlearn: 0.1985102\ttotal: 16.6s\tremaining: 637ms\n",
      "963:\tlearn: 0.1984916\ttotal: 16.6s\tremaining: 619ms\n",
      "964:\tlearn: 0.1984699\ttotal: 16.6s\tremaining: 602ms\n",
      "965:\tlearn: 0.1984563\ttotal: 16.6s\tremaining: 585ms\n",
      "966:\tlearn: 0.1984395\ttotal: 16.6s\tremaining: 568ms\n",
      "967:\tlearn: 0.1984085\ttotal: 16.7s\tremaining: 551ms\n",
      "968:\tlearn: 0.1983842\ttotal: 16.7s\tremaining: 533ms\n",
      "969:\tlearn: 0.1983645\ttotal: 16.7s\tremaining: 516ms\n",
      "970:\tlearn: 0.1983389\ttotal: 16.7s\tremaining: 499ms\n",
      "971:\tlearn: 0.1983078\ttotal: 16.7s\tremaining: 482ms\n",
      "972:\tlearn: 0.1982906\ttotal: 16.7s\tremaining: 465ms\n",
      "973:\tlearn: 0.1982590\ttotal: 16.8s\tremaining: 447ms\n",
      "974:\tlearn: 0.1982303\ttotal: 16.8s\tremaining: 430ms\n",
      "975:\tlearn: 0.1982126\ttotal: 16.8s\tremaining: 413ms\n",
      "976:\tlearn: 0.1981964\ttotal: 16.8s\tremaining: 396ms\n",
      "977:\tlearn: 0.1981753\ttotal: 16.8s\tremaining: 379ms\n",
      "978:\tlearn: 0.1981466\ttotal: 16.8s\tremaining: 361ms\n",
      "979:\tlearn: 0.1981178\ttotal: 16.9s\tremaining: 344ms\n",
      "980:\tlearn: 0.1980711\ttotal: 16.9s\tremaining: 327ms\n",
      "981:\tlearn: 0.1980250\ttotal: 16.9s\tremaining: 310ms\n",
      "982:\tlearn: 0.1979978\ttotal: 16.9s\tremaining: 292ms\n",
      "983:\tlearn: 0.1979892\ttotal: 16.9s\tremaining: 275ms\n",
      "984:\tlearn: 0.1979817\ttotal: 16.9s\tremaining: 258ms\n",
      "985:\tlearn: 0.1979746\ttotal: 17s\tremaining: 241ms\n",
      "986:\tlearn: 0.1979649\ttotal: 17s\tremaining: 224ms\n",
      "987:\tlearn: 0.1979598\ttotal: 17s\tremaining: 206ms\n",
      "988:\tlearn: 0.1979422\ttotal: 17s\tremaining: 189ms\n",
      "989:\tlearn: 0.1979215\ttotal: 17s\tremaining: 172ms\n",
      "990:\tlearn: 0.1979148\ttotal: 17.1s\tremaining: 155ms\n",
      "991:\tlearn: 0.1979000\ttotal: 17.1s\tremaining: 138ms\n",
      "992:\tlearn: 0.1978810\ttotal: 17.1s\tremaining: 120ms\n",
      "993:\tlearn: 0.1978704\ttotal: 17.1s\tremaining: 103ms\n",
      "994:\tlearn: 0.1978193\ttotal: 17.1s\tremaining: 86ms\n",
      "995:\tlearn: 0.1978029\ttotal: 17.1s\tremaining: 68.8ms\n",
      "996:\tlearn: 0.1977866\ttotal: 17.2s\tremaining: 51.6ms\n",
      "997:\tlearn: 0.1977703\ttotal: 17.2s\tremaining: 34.4ms\n",
      "998:\tlearn: 0.1977357\ttotal: 17.2s\tremaining: 17.2ms\n",
      "999:\tlearn: 0.1977180\ttotal: 17.2s\tremaining: 0us\n",
      "0:\tlearn: 0.9238330\ttotal: 21ms\tremaining: 21s\n",
      "1:\tlearn: 0.8565684\ttotal: 37.4ms\tremaining: 18.7s\n",
      "2:\tlearn: 0.7974165\ttotal: 54.4ms\tremaining: 18.1s\n",
      "3:\tlearn: 0.7445258\ttotal: 70.8ms\tremaining: 17.6s\n",
      "4:\tlearn: 0.6972904\ttotal: 87.8ms\tremaining: 17.5s\n",
      "5:\tlearn: 0.6549170\ttotal: 104ms\tremaining: 17.2s\n",
      "6:\tlearn: 0.6170863\ttotal: 120ms\tremaining: 17s\n",
      "7:\tlearn: 0.5829992\ttotal: 136ms\tremaining: 16.9s\n",
      "8:\tlearn: 0.5522375\ttotal: 152ms\tremaining: 16.8s\n",
      "9:\tlearn: 0.5242789\ttotal: 168ms\tremaining: 16.7s\n",
      "10:\tlearn: 0.4993044\ttotal: 185ms\tremaining: 16.6s\n",
      "11:\tlearn: 0.4767738\ttotal: 202ms\tremaining: 16.6s\n",
      "12:\tlearn: 0.4561399\ttotal: 219ms\tremaining: 16.6s\n",
      "13:\tlearn: 0.4375382\ttotal: 237ms\tremaining: 16.7s\n",
      "14:\tlearn: 0.4207041\ttotal: 254ms\tremaining: 16.7s\n",
      "15:\tlearn: 0.4053873\ttotal: 272ms\tremaining: 16.7s\n",
      "16:\tlearn: 0.3916003\ttotal: 288ms\tremaining: 16.6s\n",
      "17:\tlearn: 0.3784890\ttotal: 305ms\tremaining: 16.6s\n",
      "18:\tlearn: 0.3670671\ttotal: 320ms\tremaining: 16.5s\n",
      "19:\tlearn: 0.3565636\ttotal: 336ms\tremaining: 16.4s\n",
      "20:\tlearn: 0.3469547\ttotal: 352ms\tremaining: 16.4s\n",
      "21:\tlearn: 0.3381562\ttotal: 370ms\tremaining: 16.4s\n",
      "22:\tlearn: 0.3296398\ttotal: 387ms\tremaining: 16.4s\n",
      "23:\tlearn: 0.3223071\ttotal: 403ms\tremaining: 16.4s\n",
      "24:\tlearn: 0.3155544\ttotal: 421ms\tremaining: 16.4s\n",
      "25:\tlearn: 0.3088804\ttotal: 439ms\tremaining: 16.5s\n",
      "26:\tlearn: 0.3031567\ttotal: 457ms\tremaining: 16.5s\n",
      "27:\tlearn: 0.2978159\ttotal: 476ms\tremaining: 16.5s\n",
      "28:\tlearn: 0.2930027\ttotal: 494ms\tremaining: 16.6s\n",
      "29:\tlearn: 0.2881999\ttotal: 512ms\tremaining: 16.5s\n",
      "30:\tlearn: 0.2840339\ttotal: 528ms\tremaining: 16.5s\n",
      "31:\tlearn: 0.2801929\ttotal: 545ms\tremaining: 16.5s\n",
      "32:\tlearn: 0.2764445\ttotal: 562ms\tremaining: 16.5s\n",
      "33:\tlearn: 0.2732729\ttotal: 577ms\tremaining: 16.4s\n",
      "34:\tlearn: 0.2699752\ttotal: 595ms\tremaining: 16.4s\n",
      "35:\tlearn: 0.2672036\ttotal: 612ms\tremaining: 16.4s\n",
      "36:\tlearn: 0.2645291\ttotal: 628ms\tremaining: 16.3s\n",
      "37:\tlearn: 0.2620817\ttotal: 645ms\tremaining: 16.3s\n",
      "38:\tlearn: 0.2599159\ttotal: 662ms\tremaining: 16.3s\n",
      "39:\tlearn: 0.2575000\ttotal: 680ms\tremaining: 16.3s\n",
      "40:\tlearn: 0.2556635\ttotal: 697ms\tremaining: 16.3s\n",
      "41:\tlearn: 0.2537588\ttotal: 714ms\tremaining: 16.3s\n",
      "42:\tlearn: 0.2520577\ttotal: 731ms\tremaining: 16.3s\n",
      "43:\tlearn: 0.2503619\ttotal: 748ms\tremaining: 16.2s\n",
      "44:\tlearn: 0.2487872\ttotal: 764ms\tremaining: 16.2s\n",
      "45:\tlearn: 0.2473491\ttotal: 781ms\tremaining: 16.2s\n",
      "46:\tlearn: 0.2460864\ttotal: 797ms\tremaining: 16.2s\n",
      "47:\tlearn: 0.2449011\ttotal: 815ms\tremaining: 16.2s\n",
      "48:\tlearn: 0.2438175\ttotal: 831ms\tremaining: 16.1s\n",
      "49:\tlearn: 0.2427725\ttotal: 849ms\tremaining: 16.1s\n",
      "50:\tlearn: 0.2416417\ttotal: 866ms\tremaining: 16.1s\n",
      "51:\tlearn: 0.2405732\ttotal: 884ms\tremaining: 16.1s\n",
      "52:\tlearn: 0.2397077\ttotal: 902ms\tremaining: 16.1s\n",
      "53:\tlearn: 0.2387551\ttotal: 919ms\tremaining: 16.1s\n",
      "54:\tlearn: 0.2379591\ttotal: 935ms\tremaining: 16.1s\n",
      "55:\tlearn: 0.2370917\ttotal: 952ms\tremaining: 16.1s\n",
      "56:\tlearn: 0.2363165\ttotal: 969ms\tremaining: 16s\n",
      "57:\tlearn: 0.2356176\ttotal: 985ms\tremaining: 16s\n",
      "58:\tlearn: 0.2350257\ttotal: 1s\tremaining: 16s\n",
      "59:\tlearn: 0.2343640\ttotal: 1.02s\tremaining: 16s\n",
      "60:\tlearn: 0.2337327\ttotal: 1.04s\tremaining: 16s\n",
      "61:\tlearn: 0.2332892\ttotal: 1.05s\tremaining: 16s\n",
      "62:\tlearn: 0.2327514\ttotal: 1.07s\tremaining: 16s\n",
      "63:\tlearn: 0.2322102\ttotal: 1.09s\tremaining: 16s\n",
      "64:\tlearn: 0.2317101\ttotal: 1.11s\tremaining: 16s\n",
      "65:\tlearn: 0.2312178\ttotal: 1.13s\tremaining: 15.9s\n",
      "66:\tlearn: 0.2309138\ttotal: 1.14s\tremaining: 15.9s\n",
      "67:\tlearn: 0.2305307\ttotal: 1.16s\tremaining: 15.9s\n",
      "68:\tlearn: 0.2301124\ttotal: 1.18s\tremaining: 15.9s\n",
      "69:\tlearn: 0.2297900\ttotal: 1.19s\tremaining: 15.9s\n",
      "70:\tlearn: 0.2295264\ttotal: 1.21s\tremaining: 15.8s\n",
      "71:\tlearn: 0.2292051\ttotal: 1.23s\tremaining: 15.8s\n",
      "72:\tlearn: 0.2289130\ttotal: 1.24s\tremaining: 15.8s\n",
      "73:\tlearn: 0.2286678\ttotal: 1.26s\tremaining: 15.8s\n",
      "74:\tlearn: 0.2283642\ttotal: 1.28s\tremaining: 15.8s\n",
      "75:\tlearn: 0.2281268\ttotal: 1.3s\tremaining: 15.8s\n",
      "76:\tlearn: 0.2278577\ttotal: 1.32s\tremaining: 15.8s\n",
      "77:\tlearn: 0.2276555\ttotal: 1.33s\tremaining: 15.8s\n",
      "78:\tlearn: 0.2274162\ttotal: 1.35s\tremaining: 15.7s\n",
      "79:\tlearn: 0.2272524\ttotal: 1.37s\tremaining: 15.7s\n",
      "80:\tlearn: 0.2270205\ttotal: 1.39s\tremaining: 15.7s\n",
      "81:\tlearn: 0.2268303\ttotal: 1.4s\tremaining: 15.7s\n",
      "82:\tlearn: 0.2266674\ttotal: 1.42s\tremaining: 15.7s\n",
      "83:\tlearn: 0.2264386\ttotal: 1.44s\tremaining: 15.7s\n",
      "84:\tlearn: 0.2262274\ttotal: 1.46s\tremaining: 15.7s\n",
      "85:\tlearn: 0.2260716\ttotal: 1.47s\tremaining: 15.7s\n",
      "86:\tlearn: 0.2259172\ttotal: 1.49s\tremaining: 15.7s\n",
      "87:\tlearn: 0.2257133\ttotal: 1.51s\tremaining: 15.6s\n",
      "88:\tlearn: 0.2255681\ttotal: 1.53s\tremaining: 15.6s\n",
      "89:\tlearn: 0.2253945\ttotal: 1.54s\tremaining: 15.6s\n",
      "90:\tlearn: 0.2252668\ttotal: 1.56s\tremaining: 15.6s\n",
      "91:\tlearn: 0.2251224\ttotal: 1.58s\tremaining: 15.6s\n",
      "92:\tlearn: 0.2250365\ttotal: 1.6s\tremaining: 15.6s\n",
      "93:\tlearn: 0.2249323\ttotal: 1.61s\tremaining: 15.6s\n",
      "94:\tlearn: 0.2248174\ttotal: 1.63s\tremaining: 15.5s\n",
      "95:\tlearn: 0.2247200\ttotal: 1.65s\tremaining: 15.5s\n",
      "96:\tlearn: 0.2245957\ttotal: 1.66s\tremaining: 15.5s\n",
      "97:\tlearn: 0.2244664\ttotal: 1.68s\tremaining: 15.5s\n",
      "98:\tlearn: 0.2243939\ttotal: 1.7s\tremaining: 15.5s\n",
      "99:\tlearn: 0.2242528\ttotal: 1.72s\tremaining: 15.4s\n",
      "100:\tlearn: 0.2241063\ttotal: 1.73s\tremaining: 15.4s\n",
      "101:\tlearn: 0.2240019\ttotal: 1.75s\tremaining: 15.4s\n",
      "102:\tlearn: 0.2238920\ttotal: 1.77s\tremaining: 15.4s\n",
      "103:\tlearn: 0.2238115\ttotal: 1.79s\tremaining: 15.4s\n",
      "104:\tlearn: 0.2237439\ttotal: 1.81s\tremaining: 15.4s\n",
      "105:\tlearn: 0.2236448\ttotal: 1.83s\tremaining: 15.4s\n",
      "106:\tlearn: 0.2235440\ttotal: 1.85s\tremaining: 15.4s\n",
      "107:\tlearn: 0.2234965\ttotal: 1.87s\tremaining: 15.4s\n",
      "108:\tlearn: 0.2234378\ttotal: 1.89s\tremaining: 15.4s\n",
      "109:\tlearn: 0.2233709\ttotal: 1.91s\tremaining: 15.4s\n",
      "110:\tlearn: 0.2233131\ttotal: 1.93s\tremaining: 15.4s\n",
      "111:\tlearn: 0.2232417\ttotal: 1.94s\tremaining: 15.4s\n",
      "112:\tlearn: 0.2231691\ttotal: 1.96s\tremaining: 15.4s\n",
      "113:\tlearn: 0.2231236\ttotal: 1.98s\tremaining: 15.4s\n",
      "114:\tlearn: 0.2230427\ttotal: 2s\tremaining: 15.4s\n",
      "115:\tlearn: 0.2229641\ttotal: 2.01s\tremaining: 15.3s\n",
      "116:\tlearn: 0.2228852\ttotal: 2.03s\tremaining: 15.3s\n",
      "117:\tlearn: 0.2228108\ttotal: 2.05s\tremaining: 15.3s\n",
      "118:\tlearn: 0.2227663\ttotal: 2.06s\tremaining: 15.3s\n",
      "119:\tlearn: 0.2226786\ttotal: 2.08s\tremaining: 15.3s\n",
      "120:\tlearn: 0.2226280\ttotal: 2.1s\tremaining: 15.2s\n",
      "121:\tlearn: 0.2225448\ttotal: 2.11s\tremaining: 15.2s\n",
      "122:\tlearn: 0.2224725\ttotal: 2.13s\tremaining: 15.2s\n",
      "123:\tlearn: 0.2224005\ttotal: 2.15s\tremaining: 15.2s\n",
      "124:\tlearn: 0.2223341\ttotal: 2.17s\tremaining: 15.2s\n",
      "125:\tlearn: 0.2222991\ttotal: 2.18s\tremaining: 15.2s\n",
      "126:\tlearn: 0.2222384\ttotal: 2.2s\tremaining: 15.1s\n",
      "127:\tlearn: 0.2221684\ttotal: 2.22s\tremaining: 15.1s\n",
      "128:\tlearn: 0.2221299\ttotal: 2.23s\tremaining: 15.1s\n",
      "129:\tlearn: 0.2220376\ttotal: 2.25s\tremaining: 15.1s\n",
      "130:\tlearn: 0.2219857\ttotal: 2.27s\tremaining: 15s\n",
      "131:\tlearn: 0.2219495\ttotal: 2.29s\tremaining: 15s\n",
      "132:\tlearn: 0.2218823\ttotal: 2.3s\tremaining: 15s\n",
      "133:\tlearn: 0.2218324\ttotal: 2.32s\tremaining: 15s\n",
      "134:\tlearn: 0.2218081\ttotal: 2.34s\tremaining: 15s\n",
      "135:\tlearn: 0.2217372\ttotal: 2.35s\tremaining: 15s\n",
      "136:\tlearn: 0.2217067\ttotal: 2.37s\tremaining: 14.9s\n",
      "137:\tlearn: 0.2216685\ttotal: 2.4s\tremaining: 15s\n",
      "138:\tlearn: 0.2216274\ttotal: 2.42s\tremaining: 15s\n",
      "139:\tlearn: 0.2215583\ttotal: 2.44s\tremaining: 15s\n",
      "140:\tlearn: 0.2215052\ttotal: 2.45s\tremaining: 15s\n",
      "141:\tlearn: 0.2214567\ttotal: 2.47s\tremaining: 14.9s\n",
      "142:\tlearn: 0.2214145\ttotal: 2.49s\tremaining: 14.9s\n",
      "143:\tlearn: 0.2213663\ttotal: 2.51s\tremaining: 14.9s\n",
      "144:\tlearn: 0.2212998\ttotal: 2.52s\tremaining: 14.9s\n",
      "145:\tlearn: 0.2212436\ttotal: 2.54s\tremaining: 14.9s\n",
      "146:\tlearn: 0.2211979\ttotal: 2.56s\tremaining: 14.8s\n",
      "147:\tlearn: 0.2211730\ttotal: 2.57s\tremaining: 14.8s\n",
      "148:\tlearn: 0.2211429\ttotal: 2.59s\tremaining: 14.8s\n",
      "149:\tlearn: 0.2210971\ttotal: 2.61s\tremaining: 14.8s\n",
      "150:\tlearn: 0.2210515\ttotal: 2.63s\tremaining: 14.8s\n",
      "151:\tlearn: 0.2210092\ttotal: 2.64s\tremaining: 14.8s\n",
      "152:\tlearn: 0.2209800\ttotal: 2.66s\tremaining: 14.7s\n",
      "153:\tlearn: 0.2209320\ttotal: 2.68s\tremaining: 14.7s\n",
      "154:\tlearn: 0.2208466\ttotal: 2.69s\tremaining: 14.7s\n",
      "155:\tlearn: 0.2208047\ttotal: 2.71s\tremaining: 14.7s\n",
      "156:\tlearn: 0.2207769\ttotal: 2.73s\tremaining: 14.6s\n",
      "157:\tlearn: 0.2207294\ttotal: 2.74s\tremaining: 14.6s\n",
      "158:\tlearn: 0.2207081\ttotal: 2.76s\tremaining: 14.6s\n",
      "159:\tlearn: 0.2206655\ttotal: 2.78s\tremaining: 14.6s\n",
      "160:\tlearn: 0.2206136\ttotal: 2.8s\tremaining: 14.6s\n",
      "161:\tlearn: 0.2205763\ttotal: 2.81s\tremaining: 14.6s\n",
      "162:\tlearn: 0.2205391\ttotal: 2.83s\tremaining: 14.5s\n",
      "163:\tlearn: 0.2204725\ttotal: 2.85s\tremaining: 14.5s\n",
      "164:\tlearn: 0.2204510\ttotal: 2.87s\tremaining: 14.5s\n",
      "165:\tlearn: 0.2203968\ttotal: 2.88s\tremaining: 14.5s\n",
      "166:\tlearn: 0.2203543\ttotal: 2.9s\tremaining: 14.5s\n",
      "167:\tlearn: 0.2203063\ttotal: 2.92s\tremaining: 14.5s\n",
      "168:\tlearn: 0.2202666\ttotal: 2.94s\tremaining: 14.4s\n",
      "169:\tlearn: 0.2202330\ttotal: 2.95s\tremaining: 14.4s\n",
      "170:\tlearn: 0.2202008\ttotal: 2.97s\tremaining: 14.4s\n",
      "171:\tlearn: 0.2201739\ttotal: 2.99s\tremaining: 14.4s\n",
      "172:\tlearn: 0.2201546\ttotal: 3s\tremaining: 14.4s\n",
      "173:\tlearn: 0.2200987\ttotal: 3.02s\tremaining: 14.3s\n",
      "174:\tlearn: 0.2200709\ttotal: 3.04s\tremaining: 14.3s\n",
      "175:\tlearn: 0.2200219\ttotal: 3.06s\tremaining: 14.3s\n",
      "176:\tlearn: 0.2199758\ttotal: 3.08s\tremaining: 14.3s\n",
      "177:\tlearn: 0.2199419\ttotal: 3.09s\tremaining: 14.3s\n",
      "178:\tlearn: 0.2198892\ttotal: 3.11s\tremaining: 14.3s\n",
      "179:\tlearn: 0.2198263\ttotal: 3.13s\tremaining: 14.3s\n",
      "180:\tlearn: 0.2197691\ttotal: 3.15s\tremaining: 14.2s\n",
      "181:\tlearn: 0.2197380\ttotal: 3.16s\tremaining: 14.2s\n",
      "182:\tlearn: 0.2197108\ttotal: 3.18s\tremaining: 14.2s\n",
      "183:\tlearn: 0.2196913\ttotal: 3.2s\tremaining: 14.2s\n",
      "184:\tlearn: 0.2196380\ttotal: 3.21s\tremaining: 14.2s\n",
      "185:\tlearn: 0.2196041\ttotal: 3.23s\tremaining: 14.1s\n",
      "186:\tlearn: 0.2195765\ttotal: 3.25s\tremaining: 14.1s\n",
      "187:\tlearn: 0.2195418\ttotal: 3.27s\tremaining: 14.1s\n",
      "188:\tlearn: 0.2195094\ttotal: 3.29s\tremaining: 14.1s\n",
      "189:\tlearn: 0.2194789\ttotal: 3.3s\tremaining: 14.1s\n",
      "190:\tlearn: 0.2194567\ttotal: 3.32s\tremaining: 14.1s\n",
      "191:\tlearn: 0.2194117\ttotal: 3.33s\tremaining: 14s\n",
      "192:\tlearn: 0.2193865\ttotal: 3.35s\tremaining: 14s\n",
      "193:\tlearn: 0.2193561\ttotal: 3.37s\tremaining: 14s\n",
      "194:\tlearn: 0.2193123\ttotal: 3.38s\tremaining: 14s\n",
      "195:\tlearn: 0.2192953\ttotal: 3.4s\tremaining: 14s\n",
      "196:\tlearn: 0.2192393\ttotal: 3.42s\tremaining: 13.9s\n",
      "197:\tlearn: 0.2192043\ttotal: 3.44s\tremaining: 13.9s\n",
      "198:\tlearn: 0.2191871\ttotal: 3.46s\tremaining: 13.9s\n",
      "199:\tlearn: 0.2191485\ttotal: 3.47s\tremaining: 13.9s\n",
      "200:\tlearn: 0.2191262\ttotal: 3.49s\tremaining: 13.9s\n",
      "201:\tlearn: 0.2190942\ttotal: 3.51s\tremaining: 13.9s\n",
      "202:\tlearn: 0.2190474\ttotal: 3.52s\tremaining: 13.8s\n",
      "203:\tlearn: 0.2190098\ttotal: 3.54s\tremaining: 13.8s\n",
      "204:\tlearn: 0.2189761\ttotal: 3.56s\tremaining: 13.8s\n",
      "205:\tlearn: 0.2189487\ttotal: 3.58s\tremaining: 13.8s\n",
      "206:\tlearn: 0.2188916\ttotal: 3.59s\tremaining: 13.8s\n",
      "207:\tlearn: 0.2188491\ttotal: 3.61s\tremaining: 13.7s\n",
      "208:\tlearn: 0.2188201\ttotal: 3.63s\tremaining: 13.7s\n",
      "209:\tlearn: 0.2187894\ttotal: 3.64s\tremaining: 13.7s\n",
      "210:\tlearn: 0.2187525\ttotal: 3.66s\tremaining: 13.7s\n",
      "211:\tlearn: 0.2187322\ttotal: 3.68s\tremaining: 13.7s\n",
      "212:\tlearn: 0.2186809\ttotal: 3.7s\tremaining: 13.7s\n",
      "213:\tlearn: 0.2186507\ttotal: 3.72s\tremaining: 13.6s\n",
      "214:\tlearn: 0.2186148\ttotal: 3.73s\tremaining: 13.6s\n",
      "215:\tlearn: 0.2185931\ttotal: 3.75s\tremaining: 13.6s\n",
      "216:\tlearn: 0.2185440\ttotal: 3.77s\tremaining: 13.6s\n",
      "217:\tlearn: 0.2185067\ttotal: 3.78s\tremaining: 13.6s\n",
      "218:\tlearn: 0.2184744\ttotal: 3.8s\tremaining: 13.6s\n",
      "219:\tlearn: 0.2184490\ttotal: 3.82s\tremaining: 13.5s\n",
      "220:\tlearn: 0.2184208\ttotal: 3.83s\tremaining: 13.5s\n",
      "221:\tlearn: 0.2183803\ttotal: 3.85s\tremaining: 13.5s\n",
      "222:\tlearn: 0.2183472\ttotal: 3.87s\tremaining: 13.5s\n",
      "223:\tlearn: 0.2183043\ttotal: 3.89s\tremaining: 13.5s\n",
      "224:\tlearn: 0.2182764\ttotal: 3.9s\tremaining: 13.5s\n",
      "225:\tlearn: 0.2182430\ttotal: 3.92s\tremaining: 13.4s\n",
      "226:\tlearn: 0.2182039\ttotal: 3.94s\tremaining: 13.4s\n",
      "227:\tlearn: 0.2181491\ttotal: 3.96s\tremaining: 13.4s\n",
      "228:\tlearn: 0.2181174\ttotal: 3.97s\tremaining: 13.4s\n",
      "229:\tlearn: 0.2180834\ttotal: 3.99s\tremaining: 13.4s\n",
      "230:\tlearn: 0.2180650\ttotal: 4.01s\tremaining: 13.3s\n",
      "231:\tlearn: 0.2180479\ttotal: 4.02s\tremaining: 13.3s\n",
      "232:\tlearn: 0.2180254\ttotal: 4.04s\tremaining: 13.3s\n",
      "233:\tlearn: 0.2179909\ttotal: 4.06s\tremaining: 13.3s\n",
      "234:\tlearn: 0.2179544\ttotal: 4.08s\tremaining: 13.3s\n",
      "235:\tlearn: 0.2179248\ttotal: 4.09s\tremaining: 13.3s\n",
      "236:\tlearn: 0.2178975\ttotal: 4.11s\tremaining: 13.2s\n",
      "237:\tlearn: 0.2178657\ttotal: 4.13s\tremaining: 13.2s\n",
      "238:\tlearn: 0.2178409\ttotal: 4.14s\tremaining: 13.2s\n",
      "239:\tlearn: 0.2177851\ttotal: 4.16s\tremaining: 13.2s\n",
      "240:\tlearn: 0.2177467\ttotal: 4.18s\tremaining: 13.2s\n",
      "241:\tlearn: 0.2177160\ttotal: 4.19s\tremaining: 13.1s\n",
      "242:\tlearn: 0.2177028\ttotal: 4.21s\tremaining: 13.1s\n",
      "243:\tlearn: 0.2176628\ttotal: 4.23s\tremaining: 13.1s\n",
      "244:\tlearn: 0.2176160\ttotal: 4.24s\tremaining: 13.1s\n",
      "245:\tlearn: 0.2175874\ttotal: 4.26s\tremaining: 13.1s\n",
      "246:\tlearn: 0.2175529\ttotal: 4.28s\tremaining: 13s\n",
      "247:\tlearn: 0.2175129\ttotal: 4.29s\tremaining: 13s\n",
      "248:\tlearn: 0.2175049\ttotal: 4.31s\tremaining: 13s\n",
      "249:\tlearn: 0.2174761\ttotal: 4.33s\tremaining: 13s\n",
      "250:\tlearn: 0.2174498\ttotal: 4.35s\tremaining: 13s\n",
      "251:\tlearn: 0.2174296\ttotal: 4.36s\tremaining: 13s\n",
      "252:\tlearn: 0.2173889\ttotal: 4.38s\tremaining: 12.9s\n",
      "253:\tlearn: 0.2173565\ttotal: 4.4s\tremaining: 12.9s\n",
      "254:\tlearn: 0.2173274\ttotal: 4.42s\tremaining: 12.9s\n",
      "255:\tlearn: 0.2172986\ttotal: 4.44s\tremaining: 12.9s\n",
      "256:\tlearn: 0.2172913\ttotal: 4.46s\tremaining: 12.9s\n",
      "257:\tlearn: 0.2172641\ttotal: 4.47s\tremaining: 12.9s\n",
      "258:\tlearn: 0.2172412\ttotal: 4.49s\tremaining: 12.9s\n",
      "259:\tlearn: 0.2172079\ttotal: 4.51s\tremaining: 12.8s\n",
      "260:\tlearn: 0.2171772\ttotal: 4.53s\tremaining: 12.8s\n",
      "261:\tlearn: 0.2171294\ttotal: 4.54s\tremaining: 12.8s\n",
      "262:\tlearn: 0.2170988\ttotal: 4.56s\tremaining: 12.8s\n",
      "263:\tlearn: 0.2170311\ttotal: 4.58s\tremaining: 12.8s\n",
      "264:\tlearn: 0.2170118\ttotal: 4.59s\tremaining: 12.7s\n",
      "265:\tlearn: 0.2169841\ttotal: 4.61s\tremaining: 12.7s\n",
      "266:\tlearn: 0.2169464\ttotal: 4.63s\tremaining: 12.7s\n",
      "267:\tlearn: 0.2168788\ttotal: 4.65s\tremaining: 12.7s\n",
      "268:\tlearn: 0.2168448\ttotal: 4.66s\tremaining: 12.7s\n",
      "269:\tlearn: 0.2168098\ttotal: 4.68s\tremaining: 12.7s\n",
      "270:\tlearn: 0.2167767\ttotal: 4.7s\tremaining: 12.6s\n",
      "271:\tlearn: 0.2167344\ttotal: 4.72s\tremaining: 12.6s\n",
      "272:\tlearn: 0.2167113\ttotal: 4.74s\tremaining: 12.6s\n",
      "273:\tlearn: 0.2166791\ttotal: 4.75s\tremaining: 12.6s\n",
      "274:\tlearn: 0.2166421\ttotal: 4.77s\tremaining: 12.6s\n",
      "275:\tlearn: 0.2166002\ttotal: 4.79s\tremaining: 12.6s\n",
      "276:\tlearn: 0.2165471\ttotal: 4.8s\tremaining: 12.5s\n",
      "277:\tlearn: 0.2165206\ttotal: 4.82s\tremaining: 12.5s\n",
      "278:\tlearn: 0.2164879\ttotal: 4.84s\tremaining: 12.5s\n",
      "279:\tlearn: 0.2164460\ttotal: 4.85s\tremaining: 12.5s\n",
      "280:\tlearn: 0.2164141\ttotal: 4.87s\tremaining: 12.5s\n",
      "281:\tlearn: 0.2163760\ttotal: 4.89s\tremaining: 12.4s\n",
      "282:\tlearn: 0.2163538\ttotal: 4.91s\tremaining: 12.4s\n",
      "283:\tlearn: 0.2163399\ttotal: 4.92s\tremaining: 12.4s\n",
      "284:\tlearn: 0.2163214\ttotal: 4.94s\tremaining: 12.4s\n",
      "285:\tlearn: 0.2162948\ttotal: 4.96s\tremaining: 12.4s\n",
      "286:\tlearn: 0.2162606\ttotal: 4.98s\tremaining: 12.4s\n",
      "287:\tlearn: 0.2162195\ttotal: 4.99s\tremaining: 12.3s\n",
      "288:\tlearn: 0.2161780\ttotal: 5.01s\tremaining: 12.3s\n",
      "289:\tlearn: 0.2161496\ttotal: 5.03s\tremaining: 12.3s\n",
      "290:\tlearn: 0.2161229\ttotal: 5.04s\tremaining: 12.3s\n",
      "291:\tlearn: 0.2160843\ttotal: 5.06s\tremaining: 12.3s\n",
      "292:\tlearn: 0.2160494\ttotal: 5.08s\tremaining: 12.3s\n",
      "293:\tlearn: 0.2160166\ttotal: 5.09s\tremaining: 12.2s\n",
      "294:\tlearn: 0.2159760\ttotal: 5.11s\tremaining: 12.2s\n",
      "295:\tlearn: 0.2159312\ttotal: 5.13s\tremaining: 12.2s\n",
      "296:\tlearn: 0.2159102\ttotal: 5.15s\tremaining: 12.2s\n",
      "297:\tlearn: 0.2158887\ttotal: 5.17s\tremaining: 12.2s\n",
      "298:\tlearn: 0.2158516\ttotal: 5.18s\tremaining: 12.2s\n",
      "299:\tlearn: 0.2158196\ttotal: 5.2s\tremaining: 12.1s\n",
      "300:\tlearn: 0.2157931\ttotal: 5.22s\tremaining: 12.1s\n",
      "301:\tlearn: 0.2157597\ttotal: 5.23s\tremaining: 12.1s\n",
      "302:\tlearn: 0.2157411\ttotal: 5.25s\tremaining: 12.1s\n",
      "303:\tlearn: 0.2156934\ttotal: 5.27s\tremaining: 12.1s\n",
      "304:\tlearn: 0.2156808\ttotal: 5.28s\tremaining: 12s\n",
      "305:\tlearn: 0.2156527\ttotal: 5.3s\tremaining: 12s\n",
      "306:\tlearn: 0.2156255\ttotal: 5.32s\tremaining: 12s\n",
      "307:\tlearn: 0.2155872\ttotal: 5.33s\tremaining: 12s\n",
      "308:\tlearn: 0.2155500\ttotal: 5.35s\tremaining: 12s\n",
      "309:\tlearn: 0.2155239\ttotal: 5.37s\tremaining: 12s\n",
      "310:\tlearn: 0.2154953\ttotal: 5.39s\tremaining: 11.9s\n",
      "311:\tlearn: 0.2154670\ttotal: 5.41s\tremaining: 11.9s\n",
      "312:\tlearn: 0.2154500\ttotal: 5.42s\tremaining: 11.9s\n",
      "313:\tlearn: 0.2154233\ttotal: 5.44s\tremaining: 11.9s\n",
      "314:\tlearn: 0.2153770\ttotal: 5.46s\tremaining: 11.9s\n",
      "315:\tlearn: 0.2153552\ttotal: 5.47s\tremaining: 11.8s\n",
      "316:\tlearn: 0.2153303\ttotal: 5.49s\tremaining: 11.8s\n",
      "317:\tlearn: 0.2153008\ttotal: 5.51s\tremaining: 11.8s\n",
      "318:\tlearn: 0.2152594\ttotal: 5.52s\tremaining: 11.8s\n",
      "319:\tlearn: 0.2152450\ttotal: 5.54s\tremaining: 11.8s\n",
      "320:\tlearn: 0.2152145\ttotal: 5.56s\tremaining: 11.8s\n",
      "321:\tlearn: 0.2151911\ttotal: 5.58s\tremaining: 11.7s\n",
      "322:\tlearn: 0.2151636\ttotal: 5.59s\tremaining: 11.7s\n",
      "323:\tlearn: 0.2151504\ttotal: 5.61s\tremaining: 11.7s\n",
      "324:\tlearn: 0.2151228\ttotal: 5.63s\tremaining: 11.7s\n",
      "325:\tlearn: 0.2150963\ttotal: 5.64s\tremaining: 11.7s\n",
      "326:\tlearn: 0.2150531\ttotal: 5.66s\tremaining: 11.7s\n",
      "327:\tlearn: 0.2150027\ttotal: 5.68s\tremaining: 11.6s\n",
      "328:\tlearn: 0.2149741\ttotal: 5.7s\tremaining: 11.6s\n",
      "329:\tlearn: 0.2149476\ttotal: 5.71s\tremaining: 11.6s\n",
      "330:\tlearn: 0.2149319\ttotal: 5.73s\tremaining: 11.6s\n",
      "331:\tlearn: 0.2149175\ttotal: 5.75s\tremaining: 11.6s\n",
      "332:\tlearn: 0.2148923\ttotal: 5.77s\tremaining: 11.6s\n",
      "333:\tlearn: 0.2148766\ttotal: 5.8s\tremaining: 11.6s\n",
      "334:\tlearn: 0.2148479\ttotal: 5.82s\tremaining: 11.6s\n",
      "335:\tlearn: 0.2148264\ttotal: 5.85s\tremaining: 11.6s\n",
      "336:\tlearn: 0.2148019\ttotal: 5.88s\tremaining: 11.6s\n",
      "337:\tlearn: 0.2147753\ttotal: 5.9s\tremaining: 11.6s\n",
      "338:\tlearn: 0.2147411\ttotal: 5.92s\tremaining: 11.6s\n",
      "339:\tlearn: 0.2147221\ttotal: 5.95s\tremaining: 11.5s\n",
      "340:\tlearn: 0.2147011\ttotal: 5.97s\tremaining: 11.5s\n",
      "341:\tlearn: 0.2146553\ttotal: 5.99s\tremaining: 11.5s\n",
      "342:\tlearn: 0.2146066\ttotal: 6.01s\tremaining: 11.5s\n",
      "343:\tlearn: 0.2145805\ttotal: 6.04s\tremaining: 11.5s\n",
      "344:\tlearn: 0.2145523\ttotal: 6.06s\tremaining: 11.5s\n",
      "345:\tlearn: 0.2145070\ttotal: 6.08s\tremaining: 11.5s\n",
      "346:\tlearn: 0.2144885\ttotal: 6.09s\tremaining: 11.5s\n",
      "347:\tlearn: 0.2144560\ttotal: 6.11s\tremaining: 11.5s\n",
      "348:\tlearn: 0.2144303\ttotal: 6.13s\tremaining: 11.4s\n",
      "349:\tlearn: 0.2144047\ttotal: 6.15s\tremaining: 11.4s\n",
      "350:\tlearn: 0.2143609\ttotal: 6.16s\tremaining: 11.4s\n",
      "351:\tlearn: 0.2143279\ttotal: 6.18s\tremaining: 11.4s\n",
      "352:\tlearn: 0.2142914\ttotal: 6.2s\tremaining: 11.4s\n",
      "353:\tlearn: 0.2142698\ttotal: 6.22s\tremaining: 11.3s\n",
      "354:\tlearn: 0.2142303\ttotal: 6.24s\tremaining: 11.3s\n",
      "355:\tlearn: 0.2141857\ttotal: 6.25s\tremaining: 11.3s\n",
      "356:\tlearn: 0.2141392\ttotal: 6.27s\tremaining: 11.3s\n",
      "357:\tlearn: 0.2141148\ttotal: 6.29s\tremaining: 11.3s\n",
      "358:\tlearn: 0.2140749\ttotal: 6.32s\tremaining: 11.3s\n",
      "359:\tlearn: 0.2140305\ttotal: 6.33s\tremaining: 11.3s\n",
      "360:\tlearn: 0.2139898\ttotal: 6.35s\tremaining: 11.2s\n",
      "361:\tlearn: 0.2139545\ttotal: 6.37s\tremaining: 11.2s\n",
      "362:\tlearn: 0.2139189\ttotal: 6.38s\tremaining: 11.2s\n",
      "363:\tlearn: 0.2139005\ttotal: 6.4s\tremaining: 11.2s\n",
      "364:\tlearn: 0.2138664\ttotal: 6.42s\tremaining: 11.2s\n",
      "365:\tlearn: 0.2138432\ttotal: 6.44s\tremaining: 11.2s\n",
      "366:\tlearn: 0.2138209\ttotal: 6.45s\tremaining: 11.1s\n",
      "367:\tlearn: 0.2137955\ttotal: 6.47s\tremaining: 11.1s\n",
      "368:\tlearn: 0.2137744\ttotal: 6.49s\tremaining: 11.1s\n",
      "369:\tlearn: 0.2137399\ttotal: 6.5s\tremaining: 11.1s\n",
      "370:\tlearn: 0.2137108\ttotal: 6.52s\tremaining: 11.1s\n",
      "371:\tlearn: 0.2136800\ttotal: 6.54s\tremaining: 11s\n",
      "372:\tlearn: 0.2136560\ttotal: 6.56s\tremaining: 11s\n",
      "373:\tlearn: 0.2136294\ttotal: 6.58s\tremaining: 11s\n",
      "374:\tlearn: 0.2135977\ttotal: 6.59s\tremaining: 11s\n",
      "375:\tlearn: 0.2135771\ttotal: 6.61s\tremaining: 11s\n",
      "376:\tlearn: 0.2135473\ttotal: 6.63s\tremaining: 10.9s\n",
      "377:\tlearn: 0.2135149\ttotal: 6.64s\tremaining: 10.9s\n",
      "378:\tlearn: 0.2134949\ttotal: 6.66s\tremaining: 10.9s\n",
      "379:\tlearn: 0.2134720\ttotal: 6.68s\tremaining: 10.9s\n",
      "380:\tlearn: 0.2134480\ttotal: 6.69s\tremaining: 10.9s\n",
      "381:\tlearn: 0.2134240\ttotal: 6.71s\tremaining: 10.9s\n",
      "382:\tlearn: 0.2134004\ttotal: 6.73s\tremaining: 10.8s\n",
      "383:\tlearn: 0.2133825\ttotal: 6.75s\tremaining: 10.8s\n",
      "384:\tlearn: 0.2133556\ttotal: 6.76s\tremaining: 10.8s\n",
      "385:\tlearn: 0.2133322\ttotal: 6.78s\tremaining: 10.8s\n",
      "386:\tlearn: 0.2133097\ttotal: 6.8s\tremaining: 10.8s\n",
      "387:\tlearn: 0.2132853\ttotal: 6.82s\tremaining: 10.8s\n",
      "388:\tlearn: 0.2132536\ttotal: 6.83s\tremaining: 10.7s\n",
      "389:\tlearn: 0.2132199\ttotal: 6.85s\tremaining: 10.7s\n",
      "390:\tlearn: 0.2132031\ttotal: 6.87s\tremaining: 10.7s\n",
      "391:\tlearn: 0.2131774\ttotal: 6.89s\tremaining: 10.7s\n",
      "392:\tlearn: 0.2131322\ttotal: 6.9s\tremaining: 10.7s\n",
      "393:\tlearn: 0.2131042\ttotal: 6.92s\tremaining: 10.6s\n",
      "394:\tlearn: 0.2130805\ttotal: 6.94s\tremaining: 10.6s\n",
      "395:\tlearn: 0.2130632\ttotal: 6.96s\tremaining: 10.6s\n",
      "396:\tlearn: 0.2130396\ttotal: 6.98s\tremaining: 10.6s\n",
      "397:\tlearn: 0.2130067\ttotal: 7s\tremaining: 10.6s\n",
      "398:\tlearn: 0.2129903\ttotal: 7.01s\tremaining: 10.6s\n",
      "399:\tlearn: 0.2129409\ttotal: 7.03s\tremaining: 10.5s\n",
      "400:\tlearn: 0.2129124\ttotal: 7.05s\tremaining: 10.5s\n",
      "401:\tlearn: 0.2128892\ttotal: 7.06s\tremaining: 10.5s\n",
      "402:\tlearn: 0.2128572\ttotal: 7.08s\tremaining: 10.5s\n",
      "403:\tlearn: 0.2127963\ttotal: 7.1s\tremaining: 10.5s\n",
      "404:\tlearn: 0.2127661\ttotal: 7.11s\tremaining: 10.5s\n",
      "405:\tlearn: 0.2127194\ttotal: 7.13s\tremaining: 10.4s\n",
      "406:\tlearn: 0.2126882\ttotal: 7.15s\tremaining: 10.4s\n",
      "407:\tlearn: 0.2126683\ttotal: 7.17s\tremaining: 10.4s\n",
      "408:\tlearn: 0.2126407\ttotal: 7.18s\tremaining: 10.4s\n",
      "409:\tlearn: 0.2126254\ttotal: 7.2s\tremaining: 10.4s\n",
      "410:\tlearn: 0.2125942\ttotal: 7.22s\tremaining: 10.3s\n",
      "411:\tlearn: 0.2125657\ttotal: 7.24s\tremaining: 10.3s\n",
      "412:\tlearn: 0.2125379\ttotal: 7.25s\tremaining: 10.3s\n",
      "413:\tlearn: 0.2125083\ttotal: 7.27s\tremaining: 10.3s\n",
      "414:\tlearn: 0.2124842\ttotal: 7.29s\tremaining: 10.3s\n",
      "415:\tlearn: 0.2124637\ttotal: 7.3s\tremaining: 10.3s\n",
      "416:\tlearn: 0.2124364\ttotal: 7.32s\tremaining: 10.2s\n",
      "417:\tlearn: 0.2124200\ttotal: 7.33s\tremaining: 10.2s\n",
      "418:\tlearn: 0.2123950\ttotal: 7.35s\tremaining: 10.2s\n",
      "419:\tlearn: 0.2123547\ttotal: 7.37s\tremaining: 10.2s\n",
      "420:\tlearn: 0.2123435\ttotal: 7.39s\tremaining: 10.2s\n",
      "421:\tlearn: 0.2123343\ttotal: 7.41s\tremaining: 10.1s\n",
      "422:\tlearn: 0.2123153\ttotal: 7.42s\tremaining: 10.1s\n",
      "423:\tlearn: 0.2122708\ttotal: 7.44s\tremaining: 10.1s\n",
      "424:\tlearn: 0.2122350\ttotal: 7.46s\tremaining: 10.1s\n",
      "425:\tlearn: 0.2121962\ttotal: 7.48s\tremaining: 10.1s\n",
      "426:\tlearn: 0.2121742\ttotal: 7.49s\tremaining: 10.1s\n",
      "427:\tlearn: 0.2121651\ttotal: 7.51s\tremaining: 10s\n",
      "428:\tlearn: 0.2121476\ttotal: 7.53s\tremaining: 10s\n",
      "429:\tlearn: 0.2121236\ttotal: 7.54s\tremaining: 10s\n",
      "430:\tlearn: 0.2121019\ttotal: 7.56s\tremaining: 9.98s\n",
      "431:\tlearn: 0.2120605\ttotal: 7.58s\tremaining: 9.96s\n",
      "432:\tlearn: 0.2120430\ttotal: 7.59s\tremaining: 9.94s\n",
      "433:\tlearn: 0.2120259\ttotal: 7.61s\tremaining: 9.93s\n",
      "434:\tlearn: 0.2120002\ttotal: 7.63s\tremaining: 9.92s\n",
      "435:\tlearn: 0.2119803\ttotal: 7.65s\tremaining: 9.9s\n",
      "436:\tlearn: 0.2119547\ttotal: 7.67s\tremaining: 9.88s\n",
      "437:\tlearn: 0.2119291\ttotal: 7.69s\tremaining: 9.87s\n",
      "438:\tlearn: 0.2118952\ttotal: 7.71s\tremaining: 9.85s\n",
      "439:\tlearn: 0.2118726\ttotal: 7.73s\tremaining: 9.83s\n",
      "440:\tlearn: 0.2118362\ttotal: 7.74s\tremaining: 9.81s\n",
      "441:\tlearn: 0.2118127\ttotal: 7.76s\tremaining: 9.8s\n",
      "442:\tlearn: 0.2117904\ttotal: 7.78s\tremaining: 9.78s\n",
      "443:\tlearn: 0.2117685\ttotal: 7.79s\tremaining: 9.76s\n",
      "444:\tlearn: 0.2117497\ttotal: 7.81s\tremaining: 9.74s\n",
      "445:\tlearn: 0.2117071\ttotal: 7.83s\tremaining: 9.73s\n",
      "446:\tlearn: 0.2116665\ttotal: 7.85s\tremaining: 9.71s\n",
      "447:\tlearn: 0.2116316\ttotal: 7.87s\tremaining: 9.69s\n",
      "448:\tlearn: 0.2115961\ttotal: 7.88s\tremaining: 9.68s\n",
      "449:\tlearn: 0.2115504\ttotal: 7.9s\tremaining: 9.66s\n",
      "450:\tlearn: 0.2115327\ttotal: 7.92s\tremaining: 9.64s\n",
      "451:\tlearn: 0.2115084\ttotal: 7.94s\tremaining: 9.62s\n",
      "452:\tlearn: 0.2114864\ttotal: 7.96s\tremaining: 9.61s\n",
      "453:\tlearn: 0.2114731\ttotal: 7.97s\tremaining: 9.59s\n",
      "454:\tlearn: 0.2114521\ttotal: 7.99s\tremaining: 9.57s\n",
      "455:\tlearn: 0.2114105\ttotal: 8.01s\tremaining: 9.55s\n",
      "456:\tlearn: 0.2113844\ttotal: 8.02s\tremaining: 9.53s\n",
      "457:\tlearn: 0.2113675\ttotal: 8.04s\tremaining: 9.52s\n",
      "458:\tlearn: 0.2113259\ttotal: 8.06s\tremaining: 9.5s\n",
      "459:\tlearn: 0.2112914\ttotal: 8.08s\tremaining: 9.48s\n",
      "460:\tlearn: 0.2112686\ttotal: 8.09s\tremaining: 9.46s\n",
      "461:\tlearn: 0.2112265\ttotal: 8.11s\tremaining: 9.44s\n",
      "462:\tlearn: 0.2112123\ttotal: 8.13s\tremaining: 9.43s\n",
      "463:\tlearn: 0.2111907\ttotal: 8.14s\tremaining: 9.4s\n",
      "464:\tlearn: 0.2111656\ttotal: 8.16s\tremaining: 9.39s\n",
      "465:\tlearn: 0.2111349\ttotal: 8.18s\tremaining: 9.37s\n",
      "466:\tlearn: 0.2111105\ttotal: 8.19s\tremaining: 9.35s\n",
      "467:\tlearn: 0.2110981\ttotal: 8.21s\tremaining: 9.33s\n",
      "468:\tlearn: 0.2110796\ttotal: 8.23s\tremaining: 9.31s\n",
      "469:\tlearn: 0.2110540\ttotal: 8.24s\tremaining: 9.3s\n",
      "470:\tlearn: 0.2110323\ttotal: 8.27s\tremaining: 9.28s\n",
      "471:\tlearn: 0.2110000\ttotal: 8.28s\tremaining: 9.27s\n",
      "472:\tlearn: 0.2109524\ttotal: 8.3s\tremaining: 9.25s\n",
      "473:\tlearn: 0.2109321\ttotal: 8.32s\tremaining: 9.23s\n",
      "474:\tlearn: 0.2109171\ttotal: 8.33s\tremaining: 9.21s\n",
      "475:\tlearn: 0.2108998\ttotal: 8.35s\tremaining: 9.19s\n",
      "476:\tlearn: 0.2108836\ttotal: 8.37s\tremaining: 9.17s\n",
      "477:\tlearn: 0.2108702\ttotal: 8.38s\tremaining: 9.15s\n",
      "478:\tlearn: 0.2108303\ttotal: 8.4s\tremaining: 9.14s\n",
      "479:\tlearn: 0.2108117\ttotal: 8.42s\tremaining: 9.12s\n",
      "480:\tlearn: 0.2107791\ttotal: 8.44s\tremaining: 9.1s\n",
      "481:\tlearn: 0.2107484\ttotal: 8.45s\tremaining: 9.09s\n",
      "482:\tlearn: 0.2107319\ttotal: 8.47s\tremaining: 9.07s\n",
      "483:\tlearn: 0.2107128\ttotal: 8.49s\tremaining: 9.05s\n",
      "484:\tlearn: 0.2106958\ttotal: 8.51s\tremaining: 9.03s\n",
      "485:\tlearn: 0.2106843\ttotal: 8.52s\tremaining: 9.01s\n",
      "486:\tlearn: 0.2106590\ttotal: 8.54s\tremaining: 9s\n",
      "487:\tlearn: 0.2106330\ttotal: 8.56s\tremaining: 8.98s\n",
      "488:\tlearn: 0.2106195\ttotal: 8.57s\tremaining: 8.96s\n",
      "489:\tlearn: 0.2105681\ttotal: 8.59s\tremaining: 8.94s\n",
      "490:\tlearn: 0.2105506\ttotal: 8.61s\tremaining: 8.92s\n",
      "491:\tlearn: 0.2105392\ttotal: 8.62s\tremaining: 8.9s\n",
      "492:\tlearn: 0.2105055\ttotal: 8.64s\tremaining: 8.89s\n",
      "493:\tlearn: 0.2104892\ttotal: 8.66s\tremaining: 8.87s\n",
      "494:\tlearn: 0.2104441\ttotal: 8.68s\tremaining: 8.85s\n",
      "495:\tlearn: 0.2104206\ttotal: 8.7s\tremaining: 8.84s\n",
      "496:\tlearn: 0.2104033\ttotal: 8.71s\tremaining: 8.82s\n",
      "497:\tlearn: 0.2103560\ttotal: 8.73s\tremaining: 8.8s\n",
      "498:\tlearn: 0.2103359\ttotal: 8.74s\tremaining: 8.78s\n",
      "499:\tlearn: 0.2102900\ttotal: 8.76s\tremaining: 8.76s\n",
      "500:\tlearn: 0.2102504\ttotal: 8.78s\tremaining: 8.74s\n",
      "501:\tlearn: 0.2102198\ttotal: 8.79s\tremaining: 8.72s\n",
      "502:\tlearn: 0.2102050\ttotal: 8.81s\tremaining: 8.71s\n",
      "503:\tlearn: 0.2101627\ttotal: 8.83s\tremaining: 8.69s\n",
      "504:\tlearn: 0.2101468\ttotal: 8.85s\tremaining: 8.67s\n",
      "505:\tlearn: 0.2101125\ttotal: 8.86s\tremaining: 8.65s\n",
      "506:\tlearn: 0.2100817\ttotal: 8.88s\tremaining: 8.63s\n",
      "507:\tlearn: 0.2100292\ttotal: 8.9s\tremaining: 8.62s\n",
      "508:\tlearn: 0.2100082\ttotal: 8.91s\tremaining: 8.6s\n",
      "509:\tlearn: 0.2099749\ttotal: 8.93s\tremaining: 8.58s\n",
      "510:\tlearn: 0.2099475\ttotal: 8.95s\tremaining: 8.57s\n",
      "511:\tlearn: 0.2099314\ttotal: 8.97s\tremaining: 8.55s\n",
      "512:\tlearn: 0.2098977\ttotal: 8.98s\tremaining: 8.53s\n",
      "513:\tlearn: 0.2098900\ttotal: 9s\tremaining: 8.51s\n",
      "514:\tlearn: 0.2098727\ttotal: 9.02s\tremaining: 8.49s\n",
      "515:\tlearn: 0.2098562\ttotal: 9.04s\tremaining: 8.48s\n",
      "516:\tlearn: 0.2098175\ttotal: 9.05s\tremaining: 8.46s\n",
      "517:\tlearn: 0.2097944\ttotal: 9.07s\tremaining: 8.44s\n",
      "518:\tlearn: 0.2097803\ttotal: 9.09s\tremaining: 8.42s\n",
      "519:\tlearn: 0.2097378\ttotal: 9.11s\tremaining: 8.4s\n",
      "520:\tlearn: 0.2097039\ttotal: 9.13s\tremaining: 8.39s\n",
      "521:\tlearn: 0.2096867\ttotal: 9.14s\tremaining: 8.37s\n",
      "522:\tlearn: 0.2096714\ttotal: 9.16s\tremaining: 8.36s\n",
      "523:\tlearn: 0.2096570\ttotal: 9.18s\tremaining: 8.34s\n",
      "524:\tlearn: 0.2096282\ttotal: 9.2s\tremaining: 8.32s\n",
      "525:\tlearn: 0.2095806\ttotal: 9.21s\tremaining: 8.3s\n",
      "526:\tlearn: 0.2095448\ttotal: 9.23s\tremaining: 8.28s\n",
      "527:\tlearn: 0.2095285\ttotal: 9.25s\tremaining: 8.27s\n",
      "528:\tlearn: 0.2094677\ttotal: 9.26s\tremaining: 8.25s\n",
      "529:\tlearn: 0.2094359\ttotal: 9.28s\tremaining: 8.23s\n",
      "530:\tlearn: 0.2094001\ttotal: 9.3s\tremaining: 8.21s\n",
      "531:\tlearn: 0.2093803\ttotal: 9.31s\tremaining: 8.19s\n",
      "532:\tlearn: 0.2093600\ttotal: 9.33s\tremaining: 8.18s\n",
      "533:\tlearn: 0.2093381\ttotal: 9.35s\tremaining: 8.16s\n",
      "534:\tlearn: 0.2093064\ttotal: 9.37s\tremaining: 8.14s\n",
      "535:\tlearn: 0.2092872\ttotal: 9.38s\tremaining: 8.12s\n",
      "536:\tlearn: 0.2092785\ttotal: 9.4s\tremaining: 8.11s\n",
      "537:\tlearn: 0.2092627\ttotal: 9.42s\tremaining: 8.09s\n",
      "538:\tlearn: 0.2092422\ttotal: 9.44s\tremaining: 8.07s\n",
      "539:\tlearn: 0.2092117\ttotal: 9.45s\tremaining: 8.05s\n",
      "540:\tlearn: 0.2091982\ttotal: 9.47s\tremaining: 8.04s\n",
      "541:\tlearn: 0.2091627\ttotal: 9.49s\tremaining: 8.02s\n",
      "542:\tlearn: 0.2091335\ttotal: 9.51s\tremaining: 8s\n",
      "543:\tlearn: 0.2090993\ttotal: 9.52s\tremaining: 7.98s\n",
      "544:\tlearn: 0.2090620\ttotal: 9.54s\tremaining: 7.96s\n",
      "545:\tlearn: 0.2090526\ttotal: 9.56s\tremaining: 7.95s\n",
      "546:\tlearn: 0.2090250\ttotal: 9.58s\tremaining: 7.93s\n",
      "547:\tlearn: 0.2089767\ttotal: 9.59s\tremaining: 7.91s\n",
      "548:\tlearn: 0.2089695\ttotal: 9.61s\tremaining: 7.89s\n",
      "549:\tlearn: 0.2089224\ttotal: 9.63s\tremaining: 7.88s\n",
      "550:\tlearn: 0.2088940\ttotal: 9.64s\tremaining: 7.86s\n",
      "551:\tlearn: 0.2088537\ttotal: 9.66s\tremaining: 7.84s\n",
      "552:\tlearn: 0.2088084\ttotal: 9.68s\tremaining: 7.82s\n",
      "553:\tlearn: 0.2087823\ttotal: 9.69s\tremaining: 7.8s\n",
      "554:\tlearn: 0.2087675\ttotal: 9.71s\tremaining: 7.78s\n",
      "555:\tlearn: 0.2087315\ttotal: 9.72s\tremaining: 7.77s\n",
      "556:\tlearn: 0.2087093\ttotal: 9.74s\tremaining: 7.75s\n",
      "557:\tlearn: 0.2086950\ttotal: 9.76s\tremaining: 7.73s\n",
      "558:\tlearn: 0.2086765\ttotal: 9.78s\tremaining: 7.71s\n",
      "559:\tlearn: 0.2086351\ttotal: 9.8s\tremaining: 7.7s\n",
      "560:\tlearn: 0.2086188\ttotal: 9.81s\tremaining: 7.68s\n",
      "561:\tlearn: 0.2085989\ttotal: 9.83s\tremaining: 7.66s\n",
      "562:\tlearn: 0.2085887\ttotal: 9.85s\tremaining: 7.64s\n",
      "563:\tlearn: 0.2085642\ttotal: 9.87s\tremaining: 7.63s\n",
      "564:\tlearn: 0.2085265\ttotal: 9.88s\tremaining: 7.61s\n",
      "565:\tlearn: 0.2085032\ttotal: 9.9s\tremaining: 7.59s\n",
      "566:\tlearn: 0.2084785\ttotal: 9.92s\tremaining: 7.57s\n",
      "567:\tlearn: 0.2084332\ttotal: 9.93s\tremaining: 7.55s\n",
      "568:\tlearn: 0.2084234\ttotal: 9.95s\tremaining: 7.54s\n",
      "569:\tlearn: 0.2084109\ttotal: 9.97s\tremaining: 7.52s\n",
      "570:\tlearn: 0.2083848\ttotal: 9.99s\tremaining: 7.5s\n",
      "571:\tlearn: 0.2083715\ttotal: 10s\tremaining: 7.49s\n",
      "572:\tlearn: 0.2083508\ttotal: 10s\tremaining: 7.47s\n",
      "573:\tlearn: 0.2083305\ttotal: 10s\tremaining: 7.45s\n",
      "574:\tlearn: 0.2083128\ttotal: 10.1s\tremaining: 7.43s\n",
      "575:\tlearn: 0.2082994\ttotal: 10.1s\tremaining: 7.42s\n",
      "576:\tlearn: 0.2082801\ttotal: 10.1s\tremaining: 7.4s\n",
      "577:\tlearn: 0.2082647\ttotal: 10.1s\tremaining: 7.38s\n",
      "578:\tlearn: 0.2082180\ttotal: 10.1s\tremaining: 7.36s\n",
      "579:\tlearn: 0.2081989\ttotal: 10.1s\tremaining: 7.34s\n",
      "580:\tlearn: 0.2081896\ttotal: 10.2s\tremaining: 7.33s\n",
      "581:\tlearn: 0.2081685\ttotal: 10.2s\tremaining: 7.31s\n",
      "582:\tlearn: 0.2081552\ttotal: 10.2s\tremaining: 7.29s\n",
      "583:\tlearn: 0.2081346\ttotal: 10.2s\tremaining: 7.28s\n",
      "584:\tlearn: 0.2081166\ttotal: 10.2s\tremaining: 7.26s\n",
      "585:\tlearn: 0.2080974\ttotal: 10.2s\tremaining: 7.24s\n",
      "586:\tlearn: 0.2080851\ttotal: 10.3s\tremaining: 7.22s\n",
      "587:\tlearn: 0.2080518\ttotal: 10.3s\tremaining: 7.2s\n",
      "588:\tlearn: 0.2080307\ttotal: 10.3s\tremaining: 7.18s\n",
      "589:\tlearn: 0.2079921\ttotal: 10.3s\tremaining: 7.17s\n",
      "590:\tlearn: 0.2079602\ttotal: 10.3s\tremaining: 7.15s\n",
      "591:\tlearn: 0.2079443\ttotal: 10.3s\tremaining: 7.13s\n",
      "592:\tlearn: 0.2079252\ttotal: 10.4s\tremaining: 7.11s\n",
      "593:\tlearn: 0.2078994\ttotal: 10.4s\tremaining: 7.1s\n",
      "594:\tlearn: 0.2078711\ttotal: 10.4s\tremaining: 7.08s\n",
      "595:\tlearn: 0.2078529\ttotal: 10.4s\tremaining: 7.06s\n",
      "596:\tlearn: 0.2078274\ttotal: 10.4s\tremaining: 7.05s\n",
      "597:\tlearn: 0.2077998\ttotal: 10.5s\tremaining: 7.03s\n",
      "598:\tlearn: 0.2077848\ttotal: 10.5s\tremaining: 7.01s\n",
      "599:\tlearn: 0.2077676\ttotal: 10.5s\tremaining: 6.99s\n",
      "600:\tlearn: 0.2077116\ttotal: 10.5s\tremaining: 6.97s\n",
      "601:\tlearn: 0.2076946\ttotal: 10.5s\tremaining: 6.96s\n",
      "602:\tlearn: 0.2076731\ttotal: 10.5s\tremaining: 6.94s\n",
      "603:\tlearn: 0.2076569\ttotal: 10.6s\tremaining: 6.92s\n",
      "604:\tlearn: 0.2076438\ttotal: 10.6s\tremaining: 6.9s\n",
      "605:\tlearn: 0.2076199\ttotal: 10.6s\tremaining: 6.89s\n",
      "606:\tlearn: 0.2076058\ttotal: 10.6s\tremaining: 6.87s\n",
      "607:\tlearn: 0.2075771\ttotal: 10.6s\tremaining: 6.85s\n",
      "608:\tlearn: 0.2075681\ttotal: 10.6s\tremaining: 6.83s\n",
      "609:\tlearn: 0.2075449\ttotal: 10.7s\tremaining: 6.82s\n",
      "610:\tlearn: 0.2075286\ttotal: 10.7s\tremaining: 6.8s\n",
      "611:\tlearn: 0.2075065\ttotal: 10.7s\tremaining: 6.78s\n",
      "612:\tlearn: 0.2074847\ttotal: 10.7s\tremaining: 6.76s\n",
      "613:\tlearn: 0.2074617\ttotal: 10.7s\tremaining: 6.75s\n",
      "614:\tlearn: 0.2074543\ttotal: 10.7s\tremaining: 6.73s\n",
      "615:\tlearn: 0.2074270\ttotal: 10.8s\tremaining: 6.71s\n",
      "616:\tlearn: 0.2074022\ttotal: 10.8s\tremaining: 6.69s\n",
      "617:\tlearn: 0.2073873\ttotal: 10.8s\tremaining: 6.67s\n",
      "618:\tlearn: 0.2073709\ttotal: 10.8s\tremaining: 6.66s\n",
      "619:\tlearn: 0.2073535\ttotal: 10.8s\tremaining: 6.64s\n",
      "620:\tlearn: 0.2073434\ttotal: 10.8s\tremaining: 6.62s\n",
      "621:\tlearn: 0.2073287\ttotal: 10.9s\tremaining: 6.61s\n",
      "622:\tlearn: 0.2073115\ttotal: 10.9s\tremaining: 6.59s\n",
      "623:\tlearn: 0.2072780\ttotal: 10.9s\tremaining: 6.57s\n",
      "624:\tlearn: 0.2072694\ttotal: 10.9s\tremaining: 6.55s\n",
      "625:\tlearn: 0.2072368\ttotal: 10.9s\tremaining: 6.54s\n",
      "626:\tlearn: 0.2072145\ttotal: 11s\tremaining: 6.52s\n",
      "627:\tlearn: 0.2071781\ttotal: 11s\tremaining: 6.5s\n",
      "628:\tlearn: 0.2071474\ttotal: 11s\tremaining: 6.48s\n",
      "629:\tlearn: 0.2071244\ttotal: 11s\tremaining: 6.46s\n",
      "630:\tlearn: 0.2071091\ttotal: 11s\tremaining: 6.45s\n",
      "631:\tlearn: 0.2070862\ttotal: 11s\tremaining: 6.43s\n",
      "632:\tlearn: 0.2070685\ttotal: 11.1s\tremaining: 6.41s\n",
      "633:\tlearn: 0.2070546\ttotal: 11.1s\tremaining: 6.39s\n",
      "634:\tlearn: 0.2070323\ttotal: 11.1s\tremaining: 6.38s\n",
      "635:\tlearn: 0.2070145\ttotal: 11.1s\tremaining: 6.36s\n",
      "636:\tlearn: 0.2070030\ttotal: 11.1s\tremaining: 6.34s\n",
      "637:\tlearn: 0.2069834\ttotal: 11.1s\tremaining: 6.32s\n",
      "638:\tlearn: 0.2069619\ttotal: 11.2s\tremaining: 6.3s\n",
      "639:\tlearn: 0.2069389\ttotal: 11.2s\tremaining: 6.29s\n",
      "640:\tlearn: 0.2069178\ttotal: 11.2s\tremaining: 6.27s\n",
      "641:\tlearn: 0.2068771\ttotal: 11.2s\tremaining: 6.25s\n",
      "642:\tlearn: 0.2068633\ttotal: 11.2s\tremaining: 6.23s\n",
      "643:\tlearn: 0.2068415\ttotal: 11.2s\tremaining: 6.22s\n",
      "644:\tlearn: 0.2068248\ttotal: 11.3s\tremaining: 6.2s\n",
      "645:\tlearn: 0.2068061\ttotal: 11.3s\tremaining: 6.18s\n",
      "646:\tlearn: 0.2067781\ttotal: 11.3s\tremaining: 6.17s\n",
      "647:\tlearn: 0.2067493\ttotal: 11.3s\tremaining: 6.15s\n",
      "648:\tlearn: 0.2067187\ttotal: 11.3s\tremaining: 6.13s\n",
      "649:\tlearn: 0.2067019\ttotal: 11.4s\tremaining: 6.11s\n",
      "650:\tlearn: 0.2066823\ttotal: 11.4s\tremaining: 6.09s\n",
      "651:\tlearn: 0.2066624\ttotal: 11.4s\tremaining: 6.08s\n",
      "652:\tlearn: 0.2066542\ttotal: 11.4s\tremaining: 6.06s\n",
      "653:\tlearn: 0.2066414\ttotal: 11.4s\tremaining: 6.04s\n",
      "654:\tlearn: 0.2066230\ttotal: 11.4s\tremaining: 6.03s\n",
      "655:\tlearn: 0.2066070\ttotal: 11.5s\tremaining: 6.01s\n",
      "656:\tlearn: 0.2065724\ttotal: 11.5s\tremaining: 5.99s\n",
      "657:\tlearn: 0.2065572\ttotal: 11.5s\tremaining: 5.97s\n",
      "658:\tlearn: 0.2065347\ttotal: 11.5s\tremaining: 5.96s\n",
      "659:\tlearn: 0.2065171\ttotal: 11.5s\tremaining: 5.94s\n",
      "660:\tlearn: 0.2064995\ttotal: 11.5s\tremaining: 5.92s\n",
      "661:\tlearn: 0.2064722\ttotal: 11.6s\tremaining: 5.9s\n",
      "662:\tlearn: 0.2064346\ttotal: 11.6s\tremaining: 5.88s\n",
      "663:\tlearn: 0.2064107\ttotal: 11.6s\tremaining: 5.87s\n",
      "664:\tlearn: 0.2063941\ttotal: 11.6s\tremaining: 5.85s\n",
      "665:\tlearn: 0.2063680\ttotal: 11.6s\tremaining: 5.83s\n",
      "666:\tlearn: 0.2063502\ttotal: 11.6s\tremaining: 5.81s\n",
      "667:\tlearn: 0.2063370\ttotal: 11.7s\tremaining: 5.79s\n",
      "668:\tlearn: 0.2063221\ttotal: 11.7s\tremaining: 5.78s\n",
      "669:\tlearn: 0.2063055\ttotal: 11.7s\tremaining: 5.76s\n",
      "670:\tlearn: 0.2062850\ttotal: 11.7s\tremaining: 5.74s\n",
      "671:\tlearn: 0.2062733\ttotal: 11.7s\tremaining: 5.73s\n",
      "672:\tlearn: 0.2062479\ttotal: 11.7s\tremaining: 5.71s\n",
      "673:\tlearn: 0.2062239\ttotal: 11.8s\tremaining: 5.69s\n",
      "674:\tlearn: 0.2062083\ttotal: 11.8s\tremaining: 5.67s\n",
      "675:\tlearn: 0.2061885\ttotal: 11.8s\tremaining: 5.65s\n",
      "676:\tlearn: 0.2061595\ttotal: 11.8s\tremaining: 5.64s\n",
      "677:\tlearn: 0.2061344\ttotal: 11.8s\tremaining: 5.62s\n",
      "678:\tlearn: 0.2061004\ttotal: 11.8s\tremaining: 5.6s\n",
      "679:\tlearn: 0.2060527\ttotal: 11.9s\tremaining: 5.58s\n",
      "680:\tlearn: 0.2060079\ttotal: 11.9s\tremaining: 5.57s\n",
      "681:\tlearn: 0.2059658\ttotal: 11.9s\tremaining: 5.55s\n",
      "682:\tlearn: 0.2059304\ttotal: 11.9s\tremaining: 5.53s\n",
      "683:\tlearn: 0.2058908\ttotal: 11.9s\tremaining: 5.52s\n",
      "684:\tlearn: 0.2058581\ttotal: 12s\tremaining: 5.5s\n",
      "685:\tlearn: 0.2058234\ttotal: 12s\tremaining: 5.48s\n",
      "686:\tlearn: 0.2058071\ttotal: 12s\tremaining: 5.46s\n",
      "687:\tlearn: 0.2057740\ttotal: 12s\tremaining: 5.45s\n",
      "688:\tlearn: 0.2057593\ttotal: 12s\tremaining: 5.43s\n",
      "689:\tlearn: 0.2057260\ttotal: 12s\tremaining: 5.41s\n",
      "690:\tlearn: 0.2056999\ttotal: 12.1s\tremaining: 5.39s\n",
      "691:\tlearn: 0.2056838\ttotal: 12.1s\tremaining: 5.38s\n",
      "692:\tlearn: 0.2056612\ttotal: 12.1s\tremaining: 5.36s\n",
      "693:\tlearn: 0.2056354\ttotal: 12.1s\tremaining: 5.34s\n",
      "694:\tlearn: 0.2056096\ttotal: 12.1s\tremaining: 5.32s\n",
      "695:\tlearn: 0.2055957\ttotal: 12.1s\tremaining: 5.3s\n",
      "696:\tlearn: 0.2055648\ttotal: 12.2s\tremaining: 5.29s\n",
      "697:\tlearn: 0.2055358\ttotal: 12.2s\tremaining: 5.27s\n",
      "698:\tlearn: 0.2055248\ttotal: 12.2s\tremaining: 5.25s\n",
      "699:\tlearn: 0.2055057\ttotal: 12.2s\tremaining: 5.24s\n",
      "700:\tlearn: 0.2054835\ttotal: 12.2s\tremaining: 5.22s\n",
      "701:\tlearn: 0.2054730\ttotal: 12.3s\tremaining: 5.2s\n",
      "702:\tlearn: 0.2054647\ttotal: 12.3s\tremaining: 5.18s\n",
      "703:\tlearn: 0.2054466\ttotal: 12.3s\tremaining: 5.16s\n",
      "704:\tlearn: 0.2054315\ttotal: 12.3s\tremaining: 5.15s\n",
      "705:\tlearn: 0.2054005\ttotal: 12.3s\tremaining: 5.13s\n",
      "706:\tlearn: 0.2053772\ttotal: 12.3s\tremaining: 5.11s\n",
      "707:\tlearn: 0.2053620\ttotal: 12.4s\tremaining: 5.09s\n",
      "708:\tlearn: 0.2053389\ttotal: 12.4s\tremaining: 5.08s\n",
      "709:\tlearn: 0.2053223\ttotal: 12.4s\tremaining: 5.06s\n",
      "710:\tlearn: 0.2053087\ttotal: 12.4s\tremaining: 5.04s\n",
      "711:\tlearn: 0.2052904\ttotal: 12.4s\tremaining: 5.02s\n",
      "712:\tlearn: 0.2052370\ttotal: 12.4s\tremaining: 5.01s\n",
      "713:\tlearn: 0.2051924\ttotal: 12.5s\tremaining: 4.99s\n",
      "714:\tlearn: 0.2051565\ttotal: 12.5s\tremaining: 4.97s\n",
      "715:\tlearn: 0.2051430\ttotal: 12.5s\tremaining: 4.95s\n",
      "716:\tlearn: 0.2051237\ttotal: 12.5s\tremaining: 4.94s\n",
      "717:\tlearn: 0.2051083\ttotal: 12.5s\tremaining: 4.92s\n",
      "718:\tlearn: 0.2050944\ttotal: 12.5s\tremaining: 4.9s\n",
      "719:\tlearn: 0.2050287\ttotal: 12.6s\tremaining: 4.88s\n",
      "720:\tlearn: 0.2049841\ttotal: 12.6s\tremaining: 4.87s\n",
      "721:\tlearn: 0.2049233\ttotal: 12.6s\tremaining: 4.85s\n",
      "722:\tlearn: 0.2048660\ttotal: 12.6s\tremaining: 4.83s\n",
      "723:\tlearn: 0.2048120\ttotal: 12.6s\tremaining: 4.82s\n",
      "724:\tlearn: 0.2047610\ttotal: 12.6s\tremaining: 4.8s\n",
      "725:\tlearn: 0.2047224\ttotal: 12.7s\tremaining: 4.78s\n",
      "726:\tlearn: 0.2046955\ttotal: 12.7s\tremaining: 4.76s\n",
      "727:\tlearn: 0.2046491\ttotal: 12.7s\tremaining: 4.74s\n",
      "728:\tlearn: 0.2046003\ttotal: 12.7s\tremaining: 4.73s\n",
      "729:\tlearn: 0.2045544\ttotal: 12.7s\tremaining: 4.71s\n",
      "730:\tlearn: 0.2045112\ttotal: 12.7s\tremaining: 4.69s\n",
      "731:\tlearn: 0.2044707\ttotal: 12.8s\tremaining: 4.67s\n",
      "732:\tlearn: 0.2044292\ttotal: 12.8s\tremaining: 4.66s\n",
      "733:\tlearn: 0.2043953\ttotal: 12.8s\tremaining: 4.64s\n",
      "734:\tlearn: 0.2043711\ttotal: 12.8s\tremaining: 4.62s\n",
      "735:\tlearn: 0.2043308\ttotal: 12.8s\tremaining: 4.61s\n",
      "736:\tlearn: 0.2043079\ttotal: 12.9s\tremaining: 4.59s\n",
      "737:\tlearn: 0.2042708\ttotal: 12.9s\tremaining: 4.57s\n",
      "738:\tlearn: 0.2042377\ttotal: 12.9s\tremaining: 4.55s\n",
      "739:\tlearn: 0.2042157\ttotal: 12.9s\tremaining: 4.53s\n",
      "740:\tlearn: 0.2041873\ttotal: 12.9s\tremaining: 4.52s\n",
      "741:\tlearn: 0.2041607\ttotal: 12.9s\tremaining: 4.5s\n",
      "742:\tlearn: 0.2041389\ttotal: 13s\tremaining: 4.48s\n",
      "743:\tlearn: 0.2041175\ttotal: 13s\tremaining: 4.46s\n",
      "744:\tlearn: 0.2040873\ttotal: 13s\tremaining: 4.45s\n",
      "745:\tlearn: 0.2040588\ttotal: 13s\tremaining: 4.43s\n",
      "746:\tlearn: 0.2040240\ttotal: 13s\tremaining: 4.41s\n",
      "747:\tlearn: 0.2040048\ttotal: 13s\tremaining: 4.4s\n",
      "748:\tlearn: 0.2039734\ttotal: 13.1s\tremaining: 4.38s\n",
      "749:\tlearn: 0.2039375\ttotal: 13.1s\tremaining: 4.36s\n",
      "750:\tlearn: 0.2039164\ttotal: 13.1s\tremaining: 4.34s\n",
      "751:\tlearn: 0.2038964\ttotal: 13.1s\tremaining: 4.33s\n",
      "752:\tlearn: 0.2038613\ttotal: 13.1s\tremaining: 4.31s\n",
      "753:\tlearn: 0.2038338\ttotal: 13.2s\tremaining: 4.29s\n",
      "754:\tlearn: 0.2038116\ttotal: 13.2s\tremaining: 4.27s\n",
      "755:\tlearn: 0.2037927\ttotal: 13.2s\tremaining: 4.25s\n",
      "756:\tlearn: 0.2037671\ttotal: 13.2s\tremaining: 4.24s\n",
      "757:\tlearn: 0.2037556\ttotal: 13.2s\tremaining: 4.22s\n",
      "758:\tlearn: 0.2037385\ttotal: 13.2s\tremaining: 4.2s\n",
      "759:\tlearn: 0.2037212\ttotal: 13.3s\tremaining: 4.18s\n",
      "760:\tlearn: 0.2037044\ttotal: 13.3s\tremaining: 4.17s\n",
      "761:\tlearn: 0.2036842\ttotal: 13.3s\tremaining: 4.15s\n",
      "762:\tlearn: 0.2036677\ttotal: 13.3s\tremaining: 4.13s\n",
      "763:\tlearn: 0.2036323\ttotal: 13.3s\tremaining: 4.12s\n",
      "764:\tlearn: 0.2036200\ttotal: 13.3s\tremaining: 4.1s\n",
      "765:\tlearn: 0.2035629\ttotal: 13.4s\tremaining: 4.08s\n",
      "766:\tlearn: 0.2035497\ttotal: 13.4s\tremaining: 4.06s\n",
      "767:\tlearn: 0.2035340\ttotal: 13.4s\tremaining: 4.04s\n",
      "768:\tlearn: 0.2035182\ttotal: 13.4s\tremaining: 4.03s\n",
      "769:\tlearn: 0.2035004\ttotal: 13.4s\tremaining: 4.01s\n",
      "770:\tlearn: 0.2034576\ttotal: 13.4s\tremaining: 3.99s\n",
      "771:\tlearn: 0.2034351\ttotal: 13.5s\tremaining: 3.98s\n",
      "772:\tlearn: 0.2034205\ttotal: 13.5s\tremaining: 3.96s\n",
      "773:\tlearn: 0.2033940\ttotal: 13.5s\tremaining: 3.94s\n",
      "774:\tlearn: 0.2033688\ttotal: 13.5s\tremaining: 3.92s\n",
      "775:\tlearn: 0.2033577\ttotal: 13.5s\tremaining: 3.9s\n",
      "776:\tlearn: 0.2033332\ttotal: 13.5s\tremaining: 3.89s\n",
      "777:\tlearn: 0.2033052\ttotal: 13.6s\tremaining: 3.87s\n",
      "778:\tlearn: 0.2032804\ttotal: 13.6s\tremaining: 3.85s\n",
      "779:\tlearn: 0.2032648\ttotal: 13.6s\tremaining: 3.83s\n",
      "780:\tlearn: 0.2032537\ttotal: 13.6s\tremaining: 3.82s\n",
      "781:\tlearn: 0.2032369\ttotal: 13.6s\tremaining: 3.8s\n",
      "782:\tlearn: 0.2032202\ttotal: 13.7s\tremaining: 3.78s\n",
      "783:\tlearn: 0.2032004\ttotal: 13.7s\tremaining: 3.77s\n",
      "784:\tlearn: 0.2031862\ttotal: 13.7s\tremaining: 3.75s\n",
      "785:\tlearn: 0.2031758\ttotal: 13.7s\tremaining: 3.73s\n",
      "786:\tlearn: 0.2031375\ttotal: 13.7s\tremaining: 3.71s\n",
      "787:\tlearn: 0.2031171\ttotal: 13.7s\tremaining: 3.69s\n",
      "788:\tlearn: 0.2030976\ttotal: 13.8s\tremaining: 3.68s\n",
      "789:\tlearn: 0.2030783\ttotal: 13.8s\tremaining: 3.66s\n",
      "790:\tlearn: 0.2030238\ttotal: 13.8s\tremaining: 3.64s\n",
      "791:\tlearn: 0.2029943\ttotal: 13.8s\tremaining: 3.62s\n",
      "792:\tlearn: 0.2029717\ttotal: 13.8s\tremaining: 3.61s\n",
      "793:\tlearn: 0.2029366\ttotal: 13.8s\tremaining: 3.59s\n",
      "794:\tlearn: 0.2028875\ttotal: 13.9s\tremaining: 3.57s\n",
      "795:\tlearn: 0.2028649\ttotal: 13.9s\tremaining: 3.56s\n",
      "796:\tlearn: 0.2028383\ttotal: 13.9s\tremaining: 3.54s\n",
      "797:\tlearn: 0.2028225\ttotal: 13.9s\tremaining: 3.52s\n",
      "798:\tlearn: 0.2027883\ttotal: 13.9s\tremaining: 3.5s\n",
      "799:\tlearn: 0.2027750\ttotal: 13.9s\tremaining: 3.48s\n",
      "800:\tlearn: 0.2027526\ttotal: 14s\tremaining: 3.47s\n",
      "801:\tlearn: 0.2027397\ttotal: 14s\tremaining: 3.45s\n",
      "802:\tlearn: 0.2027205\ttotal: 14s\tremaining: 3.43s\n",
      "803:\tlearn: 0.2027105\ttotal: 14s\tremaining: 3.42s\n",
      "804:\tlearn: 0.2026855\ttotal: 14s\tremaining: 3.4s\n",
      "805:\tlearn: 0.2026719\ttotal: 14s\tremaining: 3.38s\n",
      "806:\tlearn: 0.2026449\ttotal: 14.1s\tremaining: 3.36s\n",
      "807:\tlearn: 0.2026242\ttotal: 14.1s\tremaining: 3.35s\n",
      "808:\tlearn: 0.2026039\ttotal: 14.1s\tremaining: 3.33s\n",
      "809:\tlearn: 0.2025589\ttotal: 14.1s\tremaining: 3.31s\n",
      "810:\tlearn: 0.2025377\ttotal: 14.1s\tremaining: 3.29s\n",
      "811:\tlearn: 0.2025284\ttotal: 14.2s\tremaining: 3.28s\n",
      "812:\tlearn: 0.2025008\ttotal: 14.2s\tremaining: 3.26s\n",
      "813:\tlearn: 0.2024732\ttotal: 14.2s\tremaining: 3.24s\n",
      "814:\tlearn: 0.2024568\ttotal: 14.2s\tremaining: 3.22s\n",
      "815:\tlearn: 0.2024378\ttotal: 14.2s\tremaining: 3.21s\n",
      "816:\tlearn: 0.2024231\ttotal: 14.2s\tremaining: 3.19s\n",
      "817:\tlearn: 0.2023998\ttotal: 14.3s\tremaining: 3.17s\n",
      "818:\tlearn: 0.2023725\ttotal: 14.3s\tremaining: 3.15s\n",
      "819:\tlearn: 0.2023586\ttotal: 14.3s\tremaining: 3.14s\n",
      "820:\tlearn: 0.2023492\ttotal: 14.3s\tremaining: 3.12s\n",
      "821:\tlearn: 0.2023263\ttotal: 14.3s\tremaining: 3.1s\n",
      "822:\tlearn: 0.2022872\ttotal: 14.3s\tremaining: 3.08s\n",
      "823:\tlearn: 0.2022626\ttotal: 14.4s\tremaining: 3.07s\n",
      "824:\tlearn: 0.2022361\ttotal: 14.4s\tremaining: 3.05s\n",
      "825:\tlearn: 0.2022170\ttotal: 14.4s\tremaining: 3.03s\n",
      "826:\tlearn: 0.2022078\ttotal: 14.4s\tremaining: 3.01s\n",
      "827:\tlearn: 0.2021948\ttotal: 14.4s\tremaining: 3s\n",
      "828:\tlearn: 0.2021812\ttotal: 14.4s\tremaining: 2.98s\n",
      "829:\tlearn: 0.2021611\ttotal: 14.5s\tremaining: 2.96s\n",
      "830:\tlearn: 0.2021459\ttotal: 14.5s\tremaining: 2.94s\n",
      "831:\tlearn: 0.2021208\ttotal: 14.5s\tremaining: 2.93s\n",
      "832:\tlearn: 0.2021105\ttotal: 14.5s\tremaining: 2.91s\n",
      "833:\tlearn: 0.2020974\ttotal: 14.5s\tremaining: 2.89s\n",
      "834:\tlearn: 0.2020850\ttotal: 14.5s\tremaining: 2.87s\n",
      "835:\tlearn: 0.2020476\ttotal: 14.6s\tremaining: 2.86s\n",
      "836:\tlearn: 0.2020115\ttotal: 14.6s\tremaining: 2.84s\n",
      "837:\tlearn: 0.2019989\ttotal: 14.6s\tremaining: 2.82s\n",
      "838:\tlearn: 0.2019539\ttotal: 14.6s\tremaining: 2.8s\n",
      "839:\tlearn: 0.2019368\ttotal: 14.6s\tremaining: 2.79s\n",
      "840:\tlearn: 0.2019165\ttotal: 14.7s\tremaining: 2.77s\n",
      "841:\tlearn: 0.2018927\ttotal: 14.7s\tremaining: 2.75s\n",
      "842:\tlearn: 0.2018701\ttotal: 14.7s\tremaining: 2.73s\n",
      "843:\tlearn: 0.2018573\ttotal: 14.7s\tremaining: 2.72s\n",
      "844:\tlearn: 0.2018365\ttotal: 14.7s\tremaining: 2.7s\n",
      "845:\tlearn: 0.2018079\ttotal: 14.7s\tremaining: 2.68s\n",
      "846:\tlearn: 0.2017826\ttotal: 14.8s\tremaining: 2.67s\n",
      "847:\tlearn: 0.2017688\ttotal: 14.8s\tremaining: 2.65s\n",
      "848:\tlearn: 0.2017512\ttotal: 14.8s\tremaining: 2.63s\n",
      "849:\tlearn: 0.2017309\ttotal: 14.8s\tremaining: 2.61s\n",
      "850:\tlearn: 0.2017215\ttotal: 14.8s\tremaining: 2.6s\n",
      "851:\tlearn: 0.2016973\ttotal: 14.8s\tremaining: 2.58s\n",
      "852:\tlearn: 0.2016678\ttotal: 14.9s\tremaining: 2.56s\n",
      "853:\tlearn: 0.2016473\ttotal: 14.9s\tremaining: 2.54s\n",
      "854:\tlearn: 0.2016266\ttotal: 14.9s\tremaining: 2.53s\n",
      "855:\tlearn: 0.2015953\ttotal: 14.9s\tremaining: 2.51s\n",
      "856:\tlearn: 0.2015539\ttotal: 14.9s\tremaining: 2.49s\n",
      "857:\tlearn: 0.2015353\ttotal: 14.9s\tremaining: 2.47s\n",
      "858:\tlearn: 0.2015156\ttotal: 15s\tremaining: 2.46s\n",
      "859:\tlearn: 0.2014985\ttotal: 15s\tremaining: 2.44s\n",
      "860:\tlearn: 0.2014781\ttotal: 15s\tremaining: 2.42s\n",
      "861:\tlearn: 0.2014465\ttotal: 15s\tremaining: 2.4s\n",
      "862:\tlearn: 0.2014259\ttotal: 15s\tremaining: 2.39s\n",
      "863:\tlearn: 0.2013997\ttotal: 15s\tremaining: 2.37s\n",
      "864:\tlearn: 0.2013875\ttotal: 15.1s\tremaining: 2.35s\n",
      "865:\tlearn: 0.2013738\ttotal: 15.1s\tremaining: 2.33s\n",
      "866:\tlearn: 0.2013523\ttotal: 15.1s\tremaining: 2.32s\n",
      "867:\tlearn: 0.2013124\ttotal: 15.1s\tremaining: 2.3s\n",
      "868:\tlearn: 0.2012714\ttotal: 15.1s\tremaining: 2.28s\n",
      "869:\tlearn: 0.2012593\ttotal: 15.2s\tremaining: 2.26s\n",
      "870:\tlearn: 0.2012435\ttotal: 15.2s\tremaining: 2.25s\n",
      "871:\tlearn: 0.2012161\ttotal: 15.2s\tremaining: 2.23s\n",
      "872:\tlearn: 0.2012099\ttotal: 15.2s\tremaining: 2.21s\n",
      "873:\tlearn: 0.2011915\ttotal: 15.2s\tremaining: 2.19s\n",
      "874:\tlearn: 0.2011704\ttotal: 15.2s\tremaining: 2.18s\n",
      "875:\tlearn: 0.2011450\ttotal: 15.3s\tremaining: 2.16s\n",
      "876:\tlearn: 0.2011300\ttotal: 15.3s\tremaining: 2.14s\n",
      "877:\tlearn: 0.2011106\ttotal: 15.3s\tremaining: 2.12s\n",
      "878:\tlearn: 0.2011012\ttotal: 15.3s\tremaining: 2.11s\n",
      "879:\tlearn: 0.2010694\ttotal: 15.3s\tremaining: 2.09s\n",
      "880:\tlearn: 0.2010536\ttotal: 15.3s\tremaining: 2.07s\n",
      "881:\tlearn: 0.2010335\ttotal: 15.4s\tremaining: 2.06s\n",
      "882:\tlearn: 0.2010167\ttotal: 15.4s\tremaining: 2.04s\n",
      "883:\tlearn: 0.2009967\ttotal: 15.4s\tremaining: 2.02s\n",
      "884:\tlearn: 0.2009792\ttotal: 15.4s\tremaining: 2s\n",
      "885:\tlearn: 0.2009627\ttotal: 15.4s\tremaining: 1.99s\n",
      "886:\tlearn: 0.2009386\ttotal: 15.4s\tremaining: 1.97s\n",
      "887:\tlearn: 0.2009091\ttotal: 15.5s\tremaining: 1.95s\n",
      "888:\tlearn: 0.2008836\ttotal: 15.5s\tremaining: 1.93s\n",
      "889:\tlearn: 0.2008669\ttotal: 15.5s\tremaining: 1.92s\n",
      "890:\tlearn: 0.2008514\ttotal: 15.5s\tremaining: 1.9s\n",
      "891:\tlearn: 0.2008433\ttotal: 15.5s\tremaining: 1.88s\n",
      "892:\tlearn: 0.2008134\ttotal: 15.6s\tremaining: 1.86s\n",
      "893:\tlearn: 0.2007995\ttotal: 15.6s\tremaining: 1.85s\n",
      "894:\tlearn: 0.2007612\ttotal: 15.6s\tremaining: 1.83s\n",
      "895:\tlearn: 0.2007396\ttotal: 15.6s\tremaining: 1.81s\n",
      "896:\tlearn: 0.2007206\ttotal: 15.6s\tremaining: 1.79s\n",
      "897:\tlearn: 0.2006809\ttotal: 15.6s\tremaining: 1.78s\n",
      "898:\tlearn: 0.2006639\ttotal: 15.7s\tremaining: 1.76s\n",
      "899:\tlearn: 0.2006365\ttotal: 15.7s\tremaining: 1.74s\n",
      "900:\tlearn: 0.2006209\ttotal: 15.7s\tremaining: 1.72s\n",
      "901:\tlearn: 0.2005893\ttotal: 15.7s\tremaining: 1.71s\n",
      "902:\tlearn: 0.2005742\ttotal: 15.7s\tremaining: 1.69s\n",
      "903:\tlearn: 0.2005559\ttotal: 15.7s\tremaining: 1.67s\n",
      "904:\tlearn: 0.2005411\ttotal: 15.8s\tremaining: 1.65s\n",
      "905:\tlearn: 0.2004968\ttotal: 15.8s\tremaining: 1.64s\n",
      "906:\tlearn: 0.2004723\ttotal: 15.8s\tremaining: 1.62s\n",
      "907:\tlearn: 0.2004599\ttotal: 15.8s\tremaining: 1.6s\n",
      "908:\tlearn: 0.2004457\ttotal: 15.8s\tremaining: 1.58s\n",
      "909:\tlearn: 0.2004233\ttotal: 15.8s\tremaining: 1.57s\n",
      "910:\tlearn: 0.2004008\ttotal: 15.9s\tremaining: 1.55s\n",
      "911:\tlearn: 0.2003726\ttotal: 15.9s\tremaining: 1.53s\n",
      "912:\tlearn: 0.2003465\ttotal: 15.9s\tremaining: 1.51s\n",
      "913:\tlearn: 0.2003235\ttotal: 15.9s\tremaining: 1.5s\n",
      "914:\tlearn: 0.2003119\ttotal: 15.9s\tremaining: 1.48s\n",
      "915:\tlearn: 0.2002913\ttotal: 16s\tremaining: 1.46s\n",
      "916:\tlearn: 0.2002718\ttotal: 16s\tremaining: 1.45s\n",
      "917:\tlearn: 0.2002533\ttotal: 16s\tremaining: 1.43s\n",
      "918:\tlearn: 0.2002301\ttotal: 16s\tremaining: 1.41s\n",
      "919:\tlearn: 0.2002148\ttotal: 16s\tremaining: 1.39s\n",
      "920:\tlearn: 0.2001932\ttotal: 16s\tremaining: 1.38s\n",
      "921:\tlearn: 0.2001639\ttotal: 16.1s\tremaining: 1.36s\n",
      "922:\tlearn: 0.2001341\ttotal: 16.1s\tremaining: 1.34s\n",
      "923:\tlearn: 0.2001225\ttotal: 16.1s\tremaining: 1.32s\n",
      "924:\tlearn: 0.2001016\ttotal: 16.1s\tremaining: 1.31s\n",
      "925:\tlearn: 0.2000807\ttotal: 16.1s\tremaining: 1.29s\n",
      "926:\tlearn: 0.2000480\ttotal: 16.1s\tremaining: 1.27s\n",
      "927:\tlearn: 0.2000317\ttotal: 16.2s\tremaining: 1.25s\n",
      "928:\tlearn: 0.2000214\ttotal: 16.2s\tremaining: 1.24s\n",
      "929:\tlearn: 0.2000117\ttotal: 16.2s\tremaining: 1.22s\n",
      "930:\tlearn: 0.1999943\ttotal: 16.2s\tremaining: 1.2s\n",
      "931:\tlearn: 0.1999707\ttotal: 16.2s\tremaining: 1.18s\n",
      "932:\tlearn: 0.1999397\ttotal: 16.3s\tremaining: 1.17s\n",
      "933:\tlearn: 0.1999244\ttotal: 16.3s\tremaining: 1.15s\n",
      "934:\tlearn: 0.1998928\ttotal: 16.3s\tremaining: 1.13s\n",
      "935:\tlearn: 0.1998797\ttotal: 16.3s\tremaining: 1.11s\n",
      "936:\tlearn: 0.1998539\ttotal: 16.3s\tremaining: 1.1s\n",
      "937:\tlearn: 0.1998270\ttotal: 16.3s\tremaining: 1.08s\n",
      "938:\tlearn: 0.1998069\ttotal: 16.4s\tremaining: 1.06s\n",
      "939:\tlearn: 0.1997846\ttotal: 16.4s\tremaining: 1.04s\n",
      "940:\tlearn: 0.1997580\ttotal: 16.4s\tremaining: 1.03s\n",
      "941:\tlearn: 0.1997471\ttotal: 16.4s\tremaining: 1.01s\n",
      "942:\tlearn: 0.1997330\ttotal: 16.4s\tremaining: 993ms\n",
      "943:\tlearn: 0.1997140\ttotal: 16.4s\tremaining: 976ms\n",
      "944:\tlearn: 0.1996863\ttotal: 16.5s\tremaining: 958ms\n",
      "945:\tlearn: 0.1996644\ttotal: 16.5s\tremaining: 941ms\n",
      "946:\tlearn: 0.1996487\ttotal: 16.5s\tremaining: 923ms\n",
      "947:\tlearn: 0.1996154\ttotal: 16.5s\tremaining: 906ms\n",
      "948:\tlearn: 0.1995842\ttotal: 16.5s\tremaining: 888ms\n",
      "949:\tlearn: 0.1995581\ttotal: 16.5s\tremaining: 871ms\n",
      "950:\tlearn: 0.1995538\ttotal: 16.6s\tremaining: 854ms\n",
      "951:\tlearn: 0.1995390\ttotal: 16.6s\tremaining: 836ms\n",
      "952:\tlearn: 0.1995054\ttotal: 16.6s\tremaining: 819ms\n",
      "953:\tlearn: 0.1994875\ttotal: 16.6s\tremaining: 801ms\n",
      "954:\tlearn: 0.1994683\ttotal: 16.6s\tremaining: 784ms\n",
      "955:\tlearn: 0.1994499\ttotal: 16.7s\tremaining: 766ms\n",
      "956:\tlearn: 0.1994383\ttotal: 16.7s\tremaining: 749ms\n",
      "957:\tlearn: 0.1994174\ttotal: 16.7s\tremaining: 732ms\n",
      "958:\tlearn: 0.1993928\ttotal: 16.7s\tremaining: 714ms\n",
      "959:\tlearn: 0.1993714\ttotal: 16.7s\tremaining: 697ms\n",
      "960:\tlearn: 0.1993516\ttotal: 16.7s\tremaining: 679ms\n",
      "961:\tlearn: 0.1993279\ttotal: 16.8s\tremaining: 662ms\n",
      "962:\tlearn: 0.1993107\ttotal: 16.8s\tremaining: 644ms\n",
      "963:\tlearn: 0.1992894\ttotal: 16.8s\tremaining: 627ms\n",
      "964:\tlearn: 0.1992648\ttotal: 16.8s\tremaining: 610ms\n",
      "965:\tlearn: 0.1992468\ttotal: 16.8s\tremaining: 592ms\n",
      "966:\tlearn: 0.1992254\ttotal: 16.8s\tremaining: 575ms\n",
      "967:\tlearn: 0.1992134\ttotal: 16.9s\tremaining: 557ms\n",
      "968:\tlearn: 0.1991874\ttotal: 16.9s\tremaining: 540ms\n",
      "969:\tlearn: 0.1991671\ttotal: 16.9s\tremaining: 523ms\n",
      "970:\tlearn: 0.1991472\ttotal: 16.9s\tremaining: 505ms\n",
      "971:\tlearn: 0.1991347\ttotal: 16.9s\tremaining: 488ms\n",
      "972:\tlearn: 0.1991222\ttotal: 16.9s\tremaining: 470ms\n",
      "973:\tlearn: 0.1991116\ttotal: 17s\tremaining: 453ms\n",
      "974:\tlearn: 0.1990840\ttotal: 17s\tremaining: 435ms\n",
      "975:\tlearn: 0.1990617\ttotal: 17s\tremaining: 418ms\n",
      "976:\tlearn: 0.1990447\ttotal: 17s\tremaining: 401ms\n",
      "977:\tlearn: 0.1990173\ttotal: 17s\tremaining: 383ms\n",
      "978:\tlearn: 0.1990093\ttotal: 17.1s\tremaining: 366ms\n",
      "979:\tlearn: 0.1989932\ttotal: 17.1s\tremaining: 349ms\n",
      "980:\tlearn: 0.1989739\ttotal: 17.1s\tremaining: 331ms\n",
      "981:\tlearn: 0.1989611\ttotal: 17.1s\tremaining: 314ms\n",
      "982:\tlearn: 0.1989513\ttotal: 17.1s\tremaining: 296ms\n",
      "983:\tlearn: 0.1989230\ttotal: 17.1s\tremaining: 279ms\n",
      "984:\tlearn: 0.1988990\ttotal: 17.2s\tremaining: 261ms\n",
      "985:\tlearn: 0.1988821\ttotal: 17.2s\tremaining: 244ms\n",
      "986:\tlearn: 0.1988751\ttotal: 17.2s\tremaining: 227ms\n",
      "987:\tlearn: 0.1988630\ttotal: 17.2s\tremaining: 209ms\n",
      "988:\tlearn: 0.1988367\ttotal: 17.2s\tremaining: 192ms\n",
      "989:\tlearn: 0.1988216\ttotal: 17.3s\tremaining: 174ms\n",
      "990:\tlearn: 0.1988065\ttotal: 17.3s\tremaining: 157ms\n",
      "991:\tlearn: 0.1987897\ttotal: 17.3s\tremaining: 139ms\n",
      "992:\tlearn: 0.1987676\ttotal: 17.3s\tremaining: 122ms\n",
      "993:\tlearn: 0.1987468\ttotal: 17.3s\tremaining: 105ms\n",
      "994:\tlearn: 0.1987303\ttotal: 17.3s\tremaining: 87.1ms\n",
      "995:\tlearn: 0.1987117\ttotal: 17.4s\tremaining: 69.7ms\n",
      "996:\tlearn: 0.1986814\ttotal: 17.4s\tremaining: 52.3ms\n",
      "997:\tlearn: 0.1986582\ttotal: 17.4s\tremaining: 34.8ms\n",
      "998:\tlearn: 0.1986382\ttotal: 17.4s\tremaining: 17.4ms\n",
      "999:\tlearn: 0.1986216\ttotal: 17.4s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Model   Type               Metric    Mean    Std\n1   CGB-N   test                  MAE  0.0940 0.0005\n5   CGB-N   test             MaxError  2.9465 0.6107\n7   CGB-N   test  MeanPoissonDeviance  0.2933 0.0044\n9   CGB-N   test                  PDE  0.0644 0.0049\n11  CGB-N   test                   R2  0.0257 0.0017\n3   CGB-N   test                 RMSE  0.2311 0.0037\n15  CGB-N   test               memory  0.0038 0.0000\n13  CGB-N   test                 time  4.8856 0.0498\n0   CGB-N  train                  MAE  0.0895 0.0003\n4   CGB-N  train             MaxError  3.6866 0.3619\n6   CGB-N  train  MeanPoissonDeviance  0.2544 0.0006\n8   CGB-N  train                  PDE  0.1885 0.0020\n10  CGB-N  train                   R2  0.1324 0.0022\n2   CGB-N  train                 RMSE  0.2181 0.0007\n14  CGB-N  train               memory  0.0039 0.0000\n12  CGB-N  train                 time 22.3055 0.4646",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0940</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9465</td>\n      <td>0.6107</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2933</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0644</td>\n      <td>0.0049</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0257</td>\n      <td>0.0017</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2311</td>\n      <td>0.0037</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0038</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CGB-N</td>\n      <td>test</td>\n      <td>time</td>\n      <td>4.8856</td>\n      <td>0.0498</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0895</td>\n      <td>0.0003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.6866</td>\n      <td>0.3619</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2544</td>\n      <td>0.0006</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.1885</td>\n      <td>0.0020</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.1324</td>\n      <td>0.0022</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2181</td>\n      <td>0.0007</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0039</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>CGB-N</td>\n      <td>train</td>\n      <td>time</td>\n      <td>22.3055</td>\n      <td>0.4646</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"CGB-N\")\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "catboost_poisson_model = CatBoostRegressor(loss_function=\"Poisson\")\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model = catboost_poisson_model.fit(X_train, y_train, sample_weight=X_train[\"Exposure\"])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T21:52:25.512366100Z",
     "start_time": "2024-02-26T21:50:08.983519700Z"
    }
   },
   "id": "fde0d19dbc1dfb85",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoML"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d72d12d058dfcc2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "727bcf877f0141d392483cc90da0cd63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240226_215226\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 180 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240226_215226/ds_sub_fit/sub_fit_ho.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 45s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_215226/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       18.08 GB / 31.70 GB (57.0%)\n",
      "Disk Space Avail:   91.52 GB / 953.85 GB (9.6%)\n",
      "===================================================\n",
      "Train Data Rows:    48213\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    18495.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.77 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.76 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.39s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 29.06s of the 43.58s of remaining time.\n",
      "\t-0.2046\t = Validation score   (r2)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 22.63s of the 37.15s of remaining time.\n",
      "\t-0.3369\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 22.33s of the 36.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t-0.0007\t = Validation score   (r2)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 14.89s of the 29.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\t0.0005\t = Validation score   (r2)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 11.88s of the 26.4s of remaining time.\n",
      "\t-0.213\t = Validation score   (r2)\n",
      "\t5.68s\t = Training   runtime\n",
      "\t1.72s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4.17s of the 18.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.0077\t = Validation score   (r2)\n",
      "\t4.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 43.61s of the 12.62s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.633, 'LightGBMXT_BAG_L1': 0.256, 'LightGBM_BAG_L1': 0.067, 'KNeighborsUnif_BAG_L1': 0.022, 'RandomForestMSE_BAG_L1': 0.022}\n",
      "\t0.0107\t = Validation score   (r2)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 11.8s of the 11.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t-0.0004\t = Validation score   (r2)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 8.66s of the 8.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\t-0.0001\t = Validation score   (r2)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 5.34s of the 5.25s of remaining time.\n",
      "\t-0.0254\t = Validation score   (r2)\n",
      "\t10.68s\t = Training   runtime\n",
      "\t1.76s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 43.61s of the -8.06s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.467, 'RandomForestMSE_BAG_L2': 0.244, 'LightGBM_BAG_L2': 0.156, 'LightGBMXT_BAG_L2': 0.133}\n",
      "\t0.0146\t = Validation score   (r2)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.21s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_215226/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                     model  holdout_score  score_val eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3         0.0088     0.0146          r2          0.7989         4.0679   27.6847                   0.0060                  0.0010             1.0929            3       True         11\n",
      "1      WeightedEnsemble_L2         0.0084     0.0107          r2          0.4654         2.0168   13.1714                   0.0060                  0.0020             0.7959            2       True          7\n",
      "2          CatBoost_BAG_L1         0.0055     0.0077          r2          0.0874         0.0530    4.5215                   0.0874                  0.0530             4.5215            1       True          6\n",
      "3          LightGBM_BAG_L1         0.0016     0.0005          r2          0.0410         0.0300    1.4908                   0.0410                  0.0300             1.4908            1       True          4\n",
      "4        LightGBMXT_BAG_L2        -0.0005    -0.0004          r2          0.5329         2.2690   14.1789                   0.0405                  0.0390             1.7634            2       True          8\n",
      "5          LightGBM_BAG_L2        -0.0008    -0.0001          r2          0.5344         2.2700   14.1463                   0.0420                  0.0400             1.7308            2       True          9\n",
      "6        LightGBMXT_BAG_L1        -0.0008    -0.0007          r2          0.0405         0.0340    0.6129                   0.0405                  0.0340             0.6129            1       True          3\n",
      "7   RandomForestMSE_BAG_L2        -0.0156    -0.0254          r2          0.7104         3.9879   23.0976                   0.2180                  1.7579            10.6820            2       True         10\n",
      "8   RandomForestMSE_BAG_L1        -0.1832    -0.2130          r2          0.2400         1.7177    5.6751                   0.2400                  1.7177             5.6751            1       True          5\n",
      "9    KNeighborsUnif_BAG_L1        -0.2033    -0.2046          r2          0.0505         0.1801    0.0752                   0.0505                  0.1801             0.0752            1       True          1\n",
      "10   KNeighborsDist_BAG_L1        -0.3110    -0.3369          r2          0.0330         0.2151    0.0400                   0.0330                  0.2151             0.0400            1       True          2\n",
      "Stacked overfitting occurred: False.\n",
      "Spend 57 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 123 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 123s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_215226\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.02 GB / 31.70 GB (53.7%)\n",
      "Disk Space Avail:   91.52 GB / 953.85 GB (9.6%)\n",
      "===================================================\n",
      "Train Data Rows:    54240\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    17425.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.4s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 81.01s of the 121.52s of remaining time.\n",
      "\t-0.1875\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 80.71s of the 121.22s of remaining time.\n",
      "\t-0.3126\t = Validation score   (r2)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 80.3s of the 120.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\t-0.0001\t = Validation score   (r2)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 77.24s of the 117.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.21%)\n",
      "\t0.0025\t = Validation score   (r2)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 73.99s of the 114.5s of remaining time.\n",
      "\t-0.2165\t = Validation score   (r2)\n",
      "\t6.41s\t = Training   runtime\n",
      "\t1.95s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 65.34s of the 105.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.25%)\n",
      "\t0.0074\t = Validation score   (r2)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 58.8s of the 99.31s of remaining time.\n",
      "\t-0.2358\t = Validation score   (r2)\n",
      "\t5.1s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 51.51s of the 92.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.26%)\n",
      "\t0.0113\t = Validation score   (r2)\n",
      "\t37.94s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 12.1s of the 52.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.33%)\n",
      "\t0.0089\t = Validation score   (r2)\n",
      "\t2.68s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 7.8s of the 48.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.15%)\n",
      "\t-0.0485\t = Validation score   (r2)\n",
      "\t7.98s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 121.55s of the 38.28s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.39, 'CatBoost_BAG_L1': 0.366, 'XGBoost_BAG_L1': 0.11, 'NeuralNetTorch_BAG_L1': 0.085, 'ExtraTreesMSE_BAG_L1': 0.037, 'KNeighborsDist_BAG_L1': 0.012}\n",
      "\t0.0154\t = Validation score   (r2)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 36.86s of the 36.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.28%)\n",
      "\t-0.0004\t = Validation score   (r2)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 33.7s of the 33.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\t0.007\t = Validation score   (r2)\n",
      "\t1.96s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 30.01s of the 29.87s of remaining time.\n",
      "\t-0.0158\t = Validation score   (r2)\n",
      "\t20.26s\t = Training   runtime\n",
      "\t1.97s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 7.55s of the 7.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.34%)\n",
      "\t0.0107\t = Validation score   (r2)\n",
      "\t4.3s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1.85s of the 1.71s of remaining time.\n",
      "\t-0.0323\t = Validation score   (r2)\n",
      "\t6.81s\t = Training   runtime\n",
      "\t1.98s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 121.55s of the -8.15s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.31, 'ExtraTreesMSE_BAG_L2': 0.197, 'NeuralNetFastAI_BAG_L1': 0.183, 'NeuralNetTorch_BAG_L1': 0.127, 'LightGBM_BAG_L2': 0.099, 'RandomForestMSE_BAG_L2': 0.07, 'CatBoost_BAG_L1': 0.014}\n",
      "\t0.0224\t = Validation score   (r2)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 132.98s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_215226\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240226_215556\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 180 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240226_215556/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 56 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 124 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 124s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_215556\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       14.79 GB / 31.70 GB (46.6%)\n",
      "Disk Space Avail:   90.81 GB / 953.85 GB (9.5%)\n",
      "===================================================\n",
      "Train Data Rows:    54241\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    15146.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.4s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 122.58s of the 122.56s of remaining time.\n",
      "\t-0.1876\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 122.2s of the 122.17s of remaining time.\n",
      "\t-0.3083\t = Validation score   (r2)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 121.8s of the 121.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t-0.001\t = Validation score   (r2)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 118.75s of the 118.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\t-0.0002\t = Validation score   (r2)\n",
      "\t1.66s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 115.57s of the 115.54s of remaining time.\n",
      "\t-0.2173\t = Validation score   (r2)\n",
      "\t6.45s\t = Training   runtime\n",
      "\t1.81s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 106.98s of the 106.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.28%)\n",
      "\t0.0075\t = Validation score   (r2)\n",
      "\t6.45s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 99.21s of the 99.18s of remaining time.\n",
      "\t-0.2403\t = Validation score   (r2)\n",
      "\t5.15s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 91.85s of the 91.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\t0.0105\t = Validation score   (r2)\n",
      "\t59.52s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 30.7s of the 30.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.36%)\n",
      "\t0.0093\t = Validation score   (r2)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 26.21s of the 26.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t-0.0487\t = Validation score   (r2)\n",
      "\t23.51s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.25s of the 1.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.40%)\n",
      "\t0.0007\t = Validation score   (r2)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 122.58s of the -2.46s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.338, 'CatBoost_BAG_L1': 0.312, 'XGBoost_BAG_L1': 0.2, 'NeuralNetTorch_BAG_L1': 0.1, 'KNeighborsUnif_BAG_L1': 0.025, 'RandomForestMSE_BAG_L1': 0.012, 'ExtraTreesMSE_BAG_L1': 0.012}\n",
      "\t0.0156\t = Validation score   (r2)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 127.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_215556\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240226_215916\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 180 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240226_215916/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 63 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 117 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 117s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_215916\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       14.49 GB / 31.70 GB (45.7%)\n",
      "Disk Space Avail:   90.38 GB / 953.85 GB (9.5%)\n",
      "===================================================\n",
      "Train Data Rows:    54241\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    14845.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.1s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 115.83s of the 115.8s of remaining time.\n",
      "\t-0.1908\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 115.45s of the 115.42s of remaining time.\n",
      "\t-0.3225\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 115.05s of the 115.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t0.001\t = Validation score   (r2)\n",
      "\t0.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 111.87s of the 111.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\t0.0012\t = Validation score   (r2)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 108.65s of the 108.63s of remaining time.\n",
      "\t-0.2157\t = Validation score   (r2)\n",
      "\t6.38s\t = Training   runtime\n",
      "\t1.83s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 100.13s of the 100.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.28%)\n",
      "\t0.0077\t = Validation score   (r2)\n",
      "\t8.39s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 90.47s of the 90.45s of remaining time.\n",
      "\t-0.2353\t = Validation score   (r2)\n",
      "\t5.08s\t = Training   runtime\n",
      "\t1.93s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 83.17s of the 83.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\t0.0103\t = Validation score   (r2)\n",
      "\t57.83s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 23.87s of the 23.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.34%)\n",
      "\t0.0086\t = Validation score   (r2)\n",
      "\t2.84s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 19.6s of the 19.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t-0.0483\t = Validation score   (r2)\n",
      "\t18.26s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 115.83s of the -0.6s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.407, 'NeuralNetFastAI_BAG_L1': 0.333, 'XGBoost_BAG_L1': 0.136, 'NeuralNetTorch_BAG_L1': 0.099, 'ExtraTreesMSE_BAG_L1': 0.025}\n",
      "\t0.0149\t = Validation score   (r2)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 119.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_215916\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240226_220233\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 180 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240226_220233/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 62 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 118 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 118s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_220233\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.21 GB / 31.70 GB (48.0%)\n",
      "Disk Space Avail:   89.96 GB / 953.85 GB (9.4%)\n",
      "===================================================\n",
      "Train Data Rows:    54241\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    15563.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.2s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 116.74s of the 116.71s of remaining time.\n",
      "\t-0.1868\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 116.35s of the 116.33s of remaining time.\n",
      "\t-0.3178\t = Validation score   (r2)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 115.97s of the 115.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.21%)\n",
      "\t-0.0006\t = Validation score   (r2)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 112.92s of the 112.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.0024\t = Validation score   (r2)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 109.59s of the 109.57s of remaining time.\n",
      "\t-0.216\t = Validation score   (r2)\n",
      "\t6.63s\t = Training   runtime\n",
      "\t1.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 100.72s of the 100.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.28%)\n",
      "\t0.0086\t = Validation score   (r2)\n",
      "\t5.4s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 93.97s of the 93.94s of remaining time.\n",
      "\t-0.236\t = Validation score   (r2)\n",
      "\t5.23s\t = Training   runtime\n",
      "\t1.99s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86.46s of the 86.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\t0.0095\t = Validation score   (r2)\n",
      "\t60.6s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 24.29s of the 24.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.35%)\n",
      "\t0.0094\t = Validation score   (r2)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 19.78s of the 19.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t-0.0481\t = Validation score   (r2)\n",
      "\t18.61s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 116.74s of the -0.83s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.387, 'NeuralNetFastAI_BAG_L1': 0.267, 'XGBoost_BAG_L1': 0.2, 'NeuralNetTorch_BAG_L1': 0.107, 'ExtraTreesMSE_BAG_L1': 0.027, 'KNeighborsUnif_BAG_L1': 0.013}\n",
      "\t0.0152\t = Validation score   (r2)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 120.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_220233\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240226_220551\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 180 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240226_220551/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 65 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 115 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Values in column 'Exposure' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 115s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240226_220551\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       14.49 GB / 31.70 GB (45.7%)\n",
      "Disk Space Avail:   89.54 GB / 953.85 GB (9.4%)\n",
      "===================================================\n",
      "Train Data Rows:    54241\n",
      "Train Data Columns: 59\n",
      "Label Column:       ClaimNb\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tAvailable Memory:                    14840.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype boolean is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'BooleanDtype' as a data type\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "Warning: dtype Int64 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'Int64Dtype()' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 12): ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', 'VehAge5', 'VehAge6', 'DrivAge2', 'DrivAge3', 'DrivAge4', 'DrivAge5', 'DrivAge6']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('Int64', []) : 12 | ['VehAge', 'DrivAge', 'VehAge2', 'VehAge3', 'VehAge4', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])    : 43 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t\t('boolean', []) :  1 | ['B12RN']\n",
      "\t\t('float', [])   :  1 | ['Density']\n",
      "\t\t('int', [])     :  1 | ['BonusMalus']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['Density']\n",
      "\t\t('int', [])       :  1 | ['BonusMalus']\n",
      "\t\t('int', ['bool']) : 44 | ['VehBrand_B10', 'VehBrand_B11', 'VehBrand_B12', 'VehBrand_B13', 'VehBrand_B14', ...]\n",
      "\t1.2s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 113.75s of the 113.73s of remaining time.\n",
      "\t-0.2054\t = Validation score   (r2)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 113.43s of the 113.4s of remaining time.\n",
      "\t-0.3328\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 113.03s of the 113.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t-0.0008\t = Validation score   (r2)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 109.86s of the 109.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.24%)\n",
      "\t-0.0005\t = Validation score   (r2)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 106.46s of the 106.44s of remaining time.\n",
      "\t-0.2108\t = Validation score   (r2)\n",
      "\t6.86s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 97.35s of the 97.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.29%)\n",
      "\t0.0086\t = Validation score   (r2)\n",
      "\t4.76s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 90.9s of the 90.88s of remaining time.\n",
      "\t-0.2351\t = Validation score   (r2)\n",
      "\t5.84s\t = Training   runtime\n",
      "\t2.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 82.73s of the 82.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\t0.009\t = Validation score   (r2)\n",
      "\t61.64s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 19.12s of the 19.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.36%)\n",
      "\t0.0082\t = Validation score   (r2)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 14.61s of the 14.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\t-0.0483\t = Validation score   (r2)\n",
      "\t13.48s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 113.75s of the -0.88s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.379, 'NeuralNetFastAI_BAG_L1': 0.273, 'XGBoost_BAG_L1': 0.182, 'NeuralNetTorch_BAG_L1': 0.106, 'ExtraTreesMSE_BAG_L1': 0.045, 'KNeighborsUnif_BAG_L1': 0.015}\n",
      "\t0.0146\t = Validation score   (r2)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 117.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240226_220551\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Model   Type               Metric     Mean    Std\n1   AutoML   test                  MAE   0.0981 0.0005\n5   AutoML   test             MaxError   2.9428 0.6154\n7   AutoML   test  MeanPoissonDeviance   0.3034 0.0045\n9   AutoML   test                  PDE   0.0320 0.0046\n11  AutoML   test                   R2   0.0140 0.0014\n3   AutoML   test                 RMSE   0.2324 0.0035\n15  AutoML   test               memory   0.3251 0.0472\n13  AutoML   test                 time  15.7934 2.1139\n0   AutoML  train                  MAE   0.0954 0.0007\n4   AutoML  train             MaxError   3.6574 0.3784\n6   AutoML  train  MeanPoissonDeviance   0.2669 0.0050\n8   AutoML  train                  PDE   0.1485 0.0149\n10  AutoML  train                   R2   0.0661 0.0068\n2   AutoML  train                 RMSE   0.2262 0.0013\n14  AutoML  train               memory   0.2586 0.0297\n12  AutoML  train                 time 184.5989 2.8514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0981</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9428</td>\n      <td>0.6154</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3034</td>\n      <td>0.0045</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0320</td>\n      <td>0.0046</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0140</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2324</td>\n      <td>0.0035</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.3251</td>\n      <td>0.0472</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>AutoML</td>\n      <td>test</td>\n      <td>time</td>\n      <td>15.7934</td>\n      <td>2.1139</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0954</td>\n      <td>0.0007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.6574</td>\n      <td>0.3784</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.2669</td>\n      <td>0.0050</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.1485</td>\n      <td>0.0149</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0661</td>\n      <td>0.0068</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2262</td>\n      <td>0.0013</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.2586</td>\n      <td>0.0297</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>AutoML</td>\n      <td>train</td>\n      <td>time</td>\n      <td>184.5989</td>\n      <td>2.8514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "df[\"VehAge4\"] = df[\"VehAge\"] ** 4\n",
    "df[\"VehAge5\"] = df[\"VehAge\"] ** 5\n",
    "df[\"VehAge6\"] = df[\"VehAge\"] ** 6\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "df[\"DrivAge4\"] = df[\"DrivAge\"] ** 4\n",
    "df[\"DrivAge5\"] = df[\"DrivAge\"] ** 5\n",
    "df[\"DrivAge6\"] = df[\"DrivAge\"] ** 6\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"AutoML\")\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    predictor = TabularPredictor(label='ClaimNb',\n",
    "                                 problem_type=\"regression\",\n",
    "                                 eval_metric=\"r2\",\n",
    "                                 sample_weight=\"Exposure\").fit(train_data, time_limit=180, presets='best_quality')\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_test = predictor.predict(test_data)\n",
    "    y_pred_train = predictor.predict(train_data)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:09:08.932572600Z",
     "start_time": "2024-02-26T21:52:25.512366100Z"
    }
   },
   "id": "38ae79d7f452930e",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combined actuarial neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e120b25da91117"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ad63cfd79574de1a6606dfc96832acb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From D:\\workspace\\GraduateGrade2Term2LSEWT\\PGPC2024\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "1695/1695 [==============================] - 4s 2ms/step - loss: 0.0928 - val_loss: 0.0585\n",
      "Epoch 2/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0560\n",
      "Epoch 3/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0546 - val_loss: 0.0560\n",
      "Epoch 4/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0546 - val_loss: 0.0561\n",
      "Epoch 5/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 6/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 7/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 8/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 9/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 10/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 11/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 12/100\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 13/100\n",
      "1663/1695 [============================>.] - ETA: 0s - loss: 0.0543Restoring model weights from the end of the best epoch: 3.\n",
      "1695/1695 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 13: early stopping\n",
      "424/424 [==============================] - 1s 1ms/step\n",
      "1695/1695 [==============================] - 2s 1ms/step\n",
      "Epoch 1/100\n",
      "1696/1696 [==============================] - 4s 2ms/step - loss: 0.0551 - val_loss: 0.0539\n",
      "Epoch 2/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0537\n",
      "Epoch 3/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0538\n",
      "Epoch 4/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0550 - val_loss: 0.0536\n",
      "Epoch 5/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0536\n",
      "Epoch 6/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0536\n",
      "Epoch 7/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0537\n",
      "Epoch 8/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0536\n",
      "Epoch 9/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0537\n",
      "Epoch 10/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0538\n",
      "Epoch 11/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0537\n",
      "Epoch 12/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0537\n",
      "Epoch 13/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0540\n",
      "Epoch 14/100\n",
      "1667/1696 [============================>.] - ETA: 0s - loss: 0.0551Restoring model weights from the end of the best epoch: 4.\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0536\n",
      "Epoch 14: early stopping\n",
      "424/424 [==============================] - 0s 1ms/step\n",
      "1696/1696 [==============================] - 2s 1ms/step\n",
      "Epoch 1/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 2/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 3/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 4/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 5/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0563\n",
      "Epoch 6/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0561\n",
      "Epoch 7/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0563\n",
      "Epoch 8/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 9/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0563\n",
      "Epoch 10/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 11/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 12/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 13/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 14/100\n",
      "1670/1696 [============================>.] - ETA: 0s - loss: 0.0546Restoring model weights from the end of the best epoch: 4.\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 14: early stopping\n",
      "424/424 [==============================] - 0s 1ms/step\n",
      "1696/1696 [==============================] - 2s 1ms/step\n",
      "Epoch 1/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0556 - val_loss: 0.0520\n",
      "Epoch 2/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0519\n",
      "Epoch 3/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0520\n",
      "Epoch 4/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0556 - val_loss: 0.0521\n",
      "Epoch 5/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0519\n",
      "Epoch 6/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0521\n",
      "Epoch 7/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0521\n",
      "Epoch 8/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0520\n",
      "Epoch 9/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0520\n",
      "Epoch 10/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0519\n",
      "Epoch 11/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0519\n",
      "Epoch 12/100\n",
      "1674/1696 [============================>.] - ETA: 0s - loss: 0.0554Restoring model weights from the end of the best epoch: 2.\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0556 - val_loss: 0.0521\n",
      "Epoch 12: early stopping\n",
      "424/424 [==============================] - 0s 1ms/step\n",
      "1696/1696 [==============================] - 2s 1ms/step\n",
      "Epoch 1/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0562\n",
      "Epoch 2/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 3/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 4/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 5/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 6/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 7/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 8/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 9/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 10/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 11/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0558\n",
      "Epoch 12/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 13/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 14/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 15/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 16/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0558\n",
      "Epoch 17/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0563\n",
      "Epoch 18/100\n",
      "1696/1696 [==============================] - 4s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 19/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 20/100\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0560\n",
      "Epoch 21/100\n",
      "1692/1696 [============================>.] - ETA: 0s - loss: 0.0546Restoring model weights from the end of the best epoch: 11.\n",
      "1696/1696 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0561\n",
      "Epoch 21: early stopping\n",
      "424/424 [==============================] - 0s 1ms/step\n",
      "1696/1696 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Model   Type               Metric    Mean     Std\n1   CANN   test                  MAE  0.0980  0.0030\n5   CANN   test             MaxError  2.9498  0.6278\n7   CANN   test  MeanPoissonDeviance  0.3083  0.0041\n9   CANN   test                  PDE  0.0163  0.0020\n11  CANN   test                   R2  0.0054  0.0008\n3   CANN   test                 RMSE  0.2335  0.0036\n15  CANN   test               memory  0.0351  0.0002\n13  CANN   test                 time  4.9040  2.4076\n0   CANN  train                  MAE  0.0978  0.0031\n4   CANN  train             MaxError  3.7497  0.3970\n6   CANN  train  MeanPoissonDeviance  0.3082  0.0014\n8   CANN  train                  PDE  0.0167  0.0033\n10  CANN  train                   R2  0.0055  0.0013\n2   CANN  train                 RMSE  0.2335  0.0009\n14  CANN  train               memory  0.0322  0.0001\n12  CANN  train                 time 44.6305 10.5835",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Type</th>\n      <th>Metric</th>\n      <th>Mean</th>\n      <th>Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>MAE</td>\n      <td>0.0980</td>\n      <td>0.0030</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>MaxError</td>\n      <td>2.9498</td>\n      <td>0.6278</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3083</td>\n      <td>0.0041</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>PDE</td>\n      <td>0.0163</td>\n      <td>0.0020</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>R2</td>\n      <td>0.0054</td>\n      <td>0.0008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>RMSE</td>\n      <td>0.2335</td>\n      <td>0.0036</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>memory</td>\n      <td>0.0351</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CANN</td>\n      <td>test</td>\n      <td>time</td>\n      <td>4.9040</td>\n      <td>2.4076</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>MAE</td>\n      <td>0.0978</td>\n      <td>0.0031</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>MaxError</td>\n      <td>3.7497</td>\n      <td>0.3970</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>MeanPoissonDeviance</td>\n      <td>0.3082</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>PDE</td>\n      <td>0.0167</td>\n      <td>0.0033</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>R2</td>\n      <td>0.0055</td>\n      <td>0.0013</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>RMSE</td>\n      <td>0.2335</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>memory</td>\n      <td>0.0322</td>\n      <td>0.0001</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>CANN</td>\n      <td>train</td>\n      <td>time</td>\n      <td>44.6305</td>\n      <td>10.5835</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_data()\n",
    "\n",
    "# drop Area AS it can be inferred from Region\n",
    "# drop ClaimAmount as we can not use it to predict the number of claims\n",
    "df.drop(columns=[\"Area\", \"ClaimAmount\"], inplace=True)\n",
    "\n",
    "# clip as kaggler's notebook\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the data preprocessing here\n",
    "\"\"\"\n",
    "df = pd.get_dummies(df, columns=[\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\"], drop_first=True)\n",
    "\n",
    "df[\"VehAge\"] = df[\"VehAge\"].clip(upper=np.percentile(df[\"VehAge\"], 97.5))\n",
    "df[\"DrivAge\"] = df[\"DrivAge\"].clip(upper=np.percentile(df[\"DrivAge\"], 97.5))\n",
    "df[\"Density\"] = np.log(df[\"Density\"])\n",
    "\n",
    "df[\"Density\"] = np.log(df[\"Density\"] + 1)\n",
    "df[\"VehAge2\"] = df[\"VehAge\"] ** 2\n",
    "df[\"VehAge3\"] = df[\"VehAge\"] ** 3\n",
    "\n",
    "df[\"DrivAge2\"] = df[\"DrivAge\"] ** 2\n",
    "df[\"DrivAge3\"] = df[\"DrivAge\"] ** 3\n",
    "\n",
    "# VehBrand=='B12' , VehGas =='Regular', VehAge == 0.0,has a higher claim frequency as kaggler's notebook\n",
    "df[\"B12RN\"] = df[\"VehBrand_B12\"] * df[\"VehGas_Regular\"] * (df[\"VehAge\"] == 0.0)\n",
    "# df.loc[:, [\"VehBrand_B12\", \"VehGas_Regular\", \"VehAge\", \"B12RN\"]].head(10) \n",
    "# # End of data preprocessing\n",
    "\n",
    "\n",
    "# do not change the fellowing code\n",
    "X = df.drop(columns=['ClaimNb'])\n",
    "y = df['ClaimNb']\n",
    "\n",
    "# data integrity check\n",
    "# make sure we do not drop some rows\n",
    "assert X.shape[0] == load_data().shape[0]\n",
    "# assert ClaimAmount is not in X, as ClaimAmount can not be used to predict the number of claims\n",
    "assert \"ClaimAmount\" not in X.columns\n",
    "# assert Frequency is not in X, as Frequency can not be used to predict the number of claims\n",
    "assert \"Frequency\" not in X.columns\n",
    "\n",
    "\"\"\"\n",
    "TODO: please specify the model here\n",
    "\"\"\"\n",
    "scores = get_scores(model_name=\"CANN\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create a neural network model for regression on preprocessed data,\n",
    "    with Dropout and L2 regularization to reduce overfitting.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=(input_shape,))\n",
    "\n",
    "    # Neural network layers with Dropout and L2 regularization\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "    x = layers.Dropout(0.5)(x)  # Dropout layer\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Another Dropout layer\n",
    "    output = layers.Dense(1)(x)  # Output layer for regression\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Adding EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model = create_model(X.shape[1])\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in tqdm(kf5.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # MinMaxScaler the X_train and X_test\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    X_train = MinMaxScaler().fit_transform(X_train)\n",
    "    X_test = MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    tracemalloc.start()\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: enter your model training here\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              epochs=100,\n",
    "              batch_size=32,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"train_time\"].append(end - start)\n",
    "    scores[\"train_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    start = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # TODO: enter your model testing here\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    end = time.time()\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    scores[\"test_time\"].append(end - start)\n",
    "    scores[\"test_memory\"].append(peak / (1024 * 1024 * 1024))\n",
    "\n",
    "    \"\"\"Calculate the Metrics\"\"\"\n",
    "    scores = calculate_metrics(scores, y_train, y_pred_train, y_test, y_pred_test)\n",
    "results.append(scores)\n",
    "print_scores(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:26.572663900Z",
     "start_time": "2024-02-26T22:09:08.937577Z"
    }
   },
   "id": "4c6e33a5e81e10",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Two technique\n",
    "## XGBoost Poisson Regression + REBAG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5fe95cfdedfd53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacking Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445a20914eb889a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502bf47cacf5f47"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "time_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "with open(f\"results_{time_str}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:26.581186700Z",
     "start_time": "2024-02-26T22:13:26.567663500Z"
    }
   },
   "id": "a437d4104d3b0ad5",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          train_MAE  test_MAE  train_RMSE  test_RMSE  train_MaxError  \\\nmodel                                                                  \nZero         0.0515    0.0515      0.2397     0.2397          3.8000   \nXGB          0.0871    0.0939      0.2128     0.2317          3.5343   \nCGB-N        0.0895    0.0940      0.2181     0.2311          3.6866   \nLGB          0.0923    0.0957      0.2216     0.2309          3.7177   \nXGB-H        0.0947    0.0961      0.2265     0.2305          3.7288   \nLGB-H        0.0953    0.0965      0.2275     0.2305          3.7293   \nGLM-Z        0.0964    0.0965      0.2324     0.2326          3.7140   \nCANN         0.0978    0.0980      0.2335     0.2335          3.7497   \nINT          0.0980    0.0980      0.2341     0.2341          3.7485   \nAutoML       0.0954    0.0981      0.2262     0.2324          3.6574   \nGLM-N        0.1077    0.1077      0.2344     0.2343          3.7378   \nGLM-N-FH     0.1078    0.1078      0.2343     0.2343          3.7388   \nGLM-H        0.1229    0.1230      0.2356     0.2360          3.6730   \n\n          test_MaxError  train_MeanPoissonDeviance  test_MeanPoissonDeviance  \\\nmodel                                                                          \nZero             3.0000                     1.3280                    1.3280   \nXGB              2.9513                     0.2340                    0.2956   \nCGB-N            2.9465                     0.2544                    0.2933   \nLGB              2.9495                     0.2603                    0.2926   \nXGB-H            2.9477                     0.2763                    0.2906   \nLGB-H            2.9484                     0.2794                    0.2910   \nGLM-Z            2.9539                     0.3012                    0.3030   \nCANN             2.9498                     0.3082                    0.3083   \nINT              2.9485                     0.3135                    0.3134   \nAutoML           2.9428                     0.2669                    0.3034   \nGLM-N            2.9378                     0.3154                    0.3154   \nGLM-N-FH         2.9419                     0.3149                    0.3149   \nGLM-H            2.9290                     0.3144                    0.3162   \n\n          train_PDE  test_PDE  train_R2  test_R2  train_time  test_time  \\\nmodel                                                                     \nZero        -3.2367   -3.2368   -0.0484  -0.0484      0.0000     0.0000   \nXGB          0.2534    0.0570    0.1740   0.0205      1.1210     0.7925   \nCGB-N        0.1885    0.0644    0.1324   0.0257     22.3055     4.8856   \nLGB          0.1695    0.0666    0.1042   0.0271      0.2225     0.0680   \nXGB-H        0.1184    0.0727    0.0642   0.0308      1.7754     0.8289   \nLGB-H        0.1087    0.0716    0.0553   0.0301      0.2381     0.0790   \nGLM-Z        0.0390    0.0335    0.0150   0.0128     12.8646     0.0432   \nCANN         0.0167    0.0163    0.0055   0.0054     44.6305     4.9040   \nINT          0.0000    0.0000    0.0000   0.0000      0.0000     0.0002   \nAutoML       0.1485    0.0320    0.0661   0.0140    184.5989    15.7934   \nGLM-N       -0.0063   -0.0064   -0.0021  -0.0021      0.0543     0.0256   \nGLM-N-FH    -0.0045   -0.0048   -0.0016  -0.0017      0.1441     0.0521   \nGLM-H       -0.0031   -0.0088   -0.0128  -0.0161     22.3274     0.0180   \n\n          train_memory  test_memory  \nmodel                                \nZero            0.0000       0.0008  \nXGB             0.0540       0.0540  \nCGB-N           0.0039       0.0038  \nLGB             0.0477       0.0481  \nXGB-H           0.0540       0.0540  \nLGB-H           0.0477       0.0481  \nGLM-Z           0.2142       0.0857  \nCANN            0.0322       0.0351  \nINT             0.0000       0.0009  \nAutoML          0.2586       0.3251  \nGLM-N           0.0388       0.0388  \nGLM-N-FH        0.0446       0.0428  \nGLM-H           0.1057       0.0445  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train_MAE</th>\n      <th>test_MAE</th>\n      <th>train_RMSE</th>\n      <th>test_RMSE</th>\n      <th>train_MaxError</th>\n      <th>test_MaxError</th>\n      <th>train_MeanPoissonDeviance</th>\n      <th>test_MeanPoissonDeviance</th>\n      <th>train_PDE</th>\n      <th>test_PDE</th>\n      <th>train_R2</th>\n      <th>test_R2</th>\n      <th>train_time</th>\n      <th>test_time</th>\n      <th>train_memory</th>\n      <th>test_memory</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Zero</th>\n      <td>0.0515</td>\n      <td>0.0515</td>\n      <td>0.2397</td>\n      <td>0.2397</td>\n      <td>3.8000</td>\n      <td>3.0000</td>\n      <td>1.3280</td>\n      <td>1.3280</td>\n      <td>-3.2367</td>\n      <td>-3.2368</td>\n      <td>-0.0484</td>\n      <td>-0.0484</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0008</td>\n    </tr>\n    <tr>\n      <th>XGB</th>\n      <td>0.0871</td>\n      <td>0.0939</td>\n      <td>0.2128</td>\n      <td>0.2317</td>\n      <td>3.5343</td>\n      <td>2.9513</td>\n      <td>0.2340</td>\n      <td>0.2956</td>\n      <td>0.2534</td>\n      <td>0.0570</td>\n      <td>0.1740</td>\n      <td>0.0205</td>\n      <td>1.1210</td>\n      <td>0.7925</td>\n      <td>0.0540</td>\n      <td>0.0540</td>\n    </tr>\n    <tr>\n      <th>CGB-N</th>\n      <td>0.0895</td>\n      <td>0.0940</td>\n      <td>0.2181</td>\n      <td>0.2311</td>\n      <td>3.6866</td>\n      <td>2.9465</td>\n      <td>0.2544</td>\n      <td>0.2933</td>\n      <td>0.1885</td>\n      <td>0.0644</td>\n      <td>0.1324</td>\n      <td>0.0257</td>\n      <td>22.3055</td>\n      <td>4.8856</td>\n      <td>0.0039</td>\n      <td>0.0038</td>\n    </tr>\n    <tr>\n      <th>LGB</th>\n      <td>0.0923</td>\n      <td>0.0957</td>\n      <td>0.2216</td>\n      <td>0.2309</td>\n      <td>3.7177</td>\n      <td>2.9495</td>\n      <td>0.2603</td>\n      <td>0.2926</td>\n      <td>0.1695</td>\n      <td>0.0666</td>\n      <td>0.1042</td>\n      <td>0.0271</td>\n      <td>0.2225</td>\n      <td>0.0680</td>\n      <td>0.0477</td>\n      <td>0.0481</td>\n    </tr>\n    <tr>\n      <th>XGB-H</th>\n      <td>0.0947</td>\n      <td>0.0961</td>\n      <td>0.2265</td>\n      <td>0.2305</td>\n      <td>3.7288</td>\n      <td>2.9477</td>\n      <td>0.2763</td>\n      <td>0.2906</td>\n      <td>0.1184</td>\n      <td>0.0727</td>\n      <td>0.0642</td>\n      <td>0.0308</td>\n      <td>1.7754</td>\n      <td>0.8289</td>\n      <td>0.0540</td>\n      <td>0.0540</td>\n    </tr>\n    <tr>\n      <th>LGB-H</th>\n      <td>0.0953</td>\n      <td>0.0965</td>\n      <td>0.2275</td>\n      <td>0.2305</td>\n      <td>3.7293</td>\n      <td>2.9484</td>\n      <td>0.2794</td>\n      <td>0.2910</td>\n      <td>0.1087</td>\n      <td>0.0716</td>\n      <td>0.0553</td>\n      <td>0.0301</td>\n      <td>0.2381</td>\n      <td>0.0790</td>\n      <td>0.0477</td>\n      <td>0.0481</td>\n    </tr>\n    <tr>\n      <th>GLM-Z</th>\n      <td>0.0964</td>\n      <td>0.0965</td>\n      <td>0.2324</td>\n      <td>0.2326</td>\n      <td>3.7140</td>\n      <td>2.9539</td>\n      <td>0.3012</td>\n      <td>0.3030</td>\n      <td>0.0390</td>\n      <td>0.0335</td>\n      <td>0.0150</td>\n      <td>0.0128</td>\n      <td>12.8646</td>\n      <td>0.0432</td>\n      <td>0.2142</td>\n      <td>0.0857</td>\n    </tr>\n    <tr>\n      <th>CANN</th>\n      <td>0.0978</td>\n      <td>0.0980</td>\n      <td>0.2335</td>\n      <td>0.2335</td>\n      <td>3.7497</td>\n      <td>2.9498</td>\n      <td>0.3082</td>\n      <td>0.3083</td>\n      <td>0.0167</td>\n      <td>0.0163</td>\n      <td>0.0055</td>\n      <td>0.0054</td>\n      <td>44.6305</td>\n      <td>4.9040</td>\n      <td>0.0322</td>\n      <td>0.0351</td>\n    </tr>\n    <tr>\n      <th>INT</th>\n      <td>0.0980</td>\n      <td>0.0980</td>\n      <td>0.2341</td>\n      <td>0.2341</td>\n      <td>3.7485</td>\n      <td>2.9485</td>\n      <td>0.3135</td>\n      <td>0.3134</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0002</td>\n      <td>0.0000</td>\n      <td>0.0009</td>\n    </tr>\n    <tr>\n      <th>AutoML</th>\n      <td>0.0954</td>\n      <td>0.0981</td>\n      <td>0.2262</td>\n      <td>0.2324</td>\n      <td>3.6574</td>\n      <td>2.9428</td>\n      <td>0.2669</td>\n      <td>0.3034</td>\n      <td>0.1485</td>\n      <td>0.0320</td>\n      <td>0.0661</td>\n      <td>0.0140</td>\n      <td>184.5989</td>\n      <td>15.7934</td>\n      <td>0.2586</td>\n      <td>0.3251</td>\n    </tr>\n    <tr>\n      <th>GLM-N</th>\n      <td>0.1077</td>\n      <td>0.1077</td>\n      <td>0.2344</td>\n      <td>0.2343</td>\n      <td>3.7378</td>\n      <td>2.9378</td>\n      <td>0.3154</td>\n      <td>0.3154</td>\n      <td>-0.0063</td>\n      <td>-0.0064</td>\n      <td>-0.0021</td>\n      <td>-0.0021</td>\n      <td>0.0543</td>\n      <td>0.0256</td>\n      <td>0.0388</td>\n      <td>0.0388</td>\n    </tr>\n    <tr>\n      <th>GLM-N-FH</th>\n      <td>0.1078</td>\n      <td>0.1078</td>\n      <td>0.2343</td>\n      <td>0.2343</td>\n      <td>3.7388</td>\n      <td>2.9419</td>\n      <td>0.3149</td>\n      <td>0.3149</td>\n      <td>-0.0045</td>\n      <td>-0.0048</td>\n      <td>-0.0016</td>\n      <td>-0.0017</td>\n      <td>0.1441</td>\n      <td>0.0521</td>\n      <td>0.0446</td>\n      <td>0.0428</td>\n    </tr>\n    <tr>\n      <th>GLM-H</th>\n      <td>0.1229</td>\n      <td>0.1230</td>\n      <td>0.2356</td>\n      <td>0.2360</td>\n      <td>3.6730</td>\n      <td>2.9290</td>\n      <td>0.3144</td>\n      <td>0.3162</td>\n      <td>-0.0031</td>\n      <td>-0.0088</td>\n      <td>-0.0128</td>\n      <td>-0.0161</td>\n      <td>22.3274</td>\n      <td>0.0180</td>\n      <td>0.1057</td>\n      <td>0.0445</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame()\n",
    "for result in results:\n",
    "    df_results = pd.concat([df_results, pd.DataFrame(result)])\n",
    "\n",
    "df_results.groupby(\"model\").mean().sort_values(by=\"test_MAE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:26.643183700Z",
     "start_time": "2024-02-26T22:13:26.581186700Z"
    }
   },
   "id": "b5f92a8b00a8fc79",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x466.667 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x466.667 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x466.667 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x466.667 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x466.667 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for metric in results[0].keys():\n",
    "    if metric in [\"test_MAE\", \"test_MeanPoissonDeviance\", \"test_PDE\", \"train_time\", \"train_memory\"]:\n",
    "        plt.figure(figsize=(10, len(results) / 3))\n",
    "        sns.boxplot(data=df_results, y=\"model\", x=metric)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:27.486148500Z",
     "start_time": "2024-02-26T22:13:26.627181500Z"
    }
   },
   "id": "8c57acde57a8e470",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:27.508147500Z",
     "start_time": "2024-02-26T22:13:27.486148500Z"
    }
   },
   "id": "f9ee8ad973e22bef",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:27.521171700Z",
     "start_time": "2024-02-26T22:13:27.501148Z"
    }
   },
   "id": "b2bab109c38b469",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'test_PDE')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = (df_results.groupby(\"model\").mean() * 1000).astype(int).rank()\n",
    "# summary = df_results.groupby(\"model\").mean()\n",
    "\n",
    "# droped_index = [\"XGB-H-REBAGG\"]\n",
    "\n",
    "# summary = summary.drop(index=droped_index)\n",
    "\n",
    "x_label = \"test_MAE\"\n",
    "y_label = \"test_PDE\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = summary[x_label]\n",
    "y = summary[y_label]\n",
    "n = summary.index\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for i, txt in enumerate(n):\n",
    "    ax.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:27.689590500Z",
     "start_time": "2024-02-26T22:13:27.518215700Z"
    }
   },
   "id": "be4e2705c0edd695",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 3-D scatter plot using Plotly\n",
    "# \n",
    "# import plotly.express as px\n",
    "# \n",
    "# fig = px.scatter_3d(summary, x=x_label, y=y_label, z=\"train_time\", text=summary.index)\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T22:13:27.710483800Z",
     "start_time": "2024-02-26T22:13:27.688564700Z"
    }
   },
   "id": "929f5844a2349845",
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
